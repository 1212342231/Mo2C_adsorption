{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aad8b71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#设置全局字体为新罗马\n",
    "plt.rc('font',family='Times New Roman') \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "#计时\n",
    "from time import time\n",
    "\n",
    "#线性模型\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "#集成树\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "#支持向量机\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "#评估指标\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# 忽略警告\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5d87b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pri_feature = pd.read_excel(r\"C:\\Users\\Administrator\\Desktop\\Mo2C_data_eassy\\pri_feature.xlsx\",index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d904a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "atom_feature = pri_feature.iloc[[2,4,5,8,11,13,16,20,23,25],0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5ef8ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gas_feature =  pri_feature.iloc[0:4,13:24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83184af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gas_feature.index = [\"CO\",\"H2S\",\"CH4\",\"C2H6\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c71587e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#合并成数据\n",
    "\n",
    "_ = gas_feature\n",
    "for i in range(1,10):\n",
    "    gas_feature = pd.concat([gas_feature,_])\n",
    "\n",
    "_ = pd.DataFrame()\n",
    "\n",
    "for i in range(10):\n",
    "    temp1 = atom_feature.iloc[[i]]    \n",
    "    temp2 = pd.concat([temp1,temp1])\n",
    "    temp3 = pd.concat([temp2,temp2])\n",
    "    _ = pd.concat([_,temp3])\n",
    "atom_feature = _\n",
    "\n",
    "#更新索引\n",
    "gas_feature.index = atom_feature.index+\"+\"+gas_feature.index\n",
    "atom_feature.index = gas_feature.index\n",
    "df = pd.concat([atom_feature,gas_feature],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecbec9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ade = pri_feature[[\"ade\"]] \n",
    "ade.index = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c60f314",
   "metadata": {},
   "outputs": [],
   "source": [
    "#到这里已经处理好数据了，df为X ade为y\n",
    "#接下来就开始处理df的特征\n",
    "#由于ade与df分开，这里使用临时变量\n",
    "_ = pd.concat([df,ade],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88e2548b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_.to_excel(r\"C:\\Users\\Administrator\\Desktop\\Mo2C_data_eassy\\df.xlsx\")\n",
    "_.corr().to_excel(r\"C:\\Users\\Administrator\\Desktop\\Mo2C_data_eassy\\corr.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a93dad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#根据相关系数删除特征\n",
    "df.drop([\"ed\",\"M\",\"rg\",\"Ng\",\"eg\",\"Ag\",\"Nmg\"], axis=1, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92311cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b9c39fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#过滤特征之后，开始对整个特征进行归一化处理\n",
    "atom_feature = pri_feature.iloc[[i for i in range(0,26)],0:11]\n",
    "gas_feature =  pri_feature.iloc[0:4,13:24]\n",
    "gas_feature.index = [\"CO\",\"H2S\",\"CH4\",\"C2H6\"]\n",
    "#合并成数据\n",
    "\n",
    "_ = gas_feature\n",
    "for i in range(1,26):\n",
    "    gas_feature = pd.concat([gas_feature,_])\n",
    "\n",
    "_ = pd.DataFrame()\n",
    "\n",
    "for i in range(26):\n",
    "    temp1 = atom_feature.iloc[[i]]    \n",
    "    temp2 = pd.concat([temp1,temp1])\n",
    "    temp3 = pd.concat([temp2,temp2])\n",
    "    _ = pd.concat([_,temp3])\n",
    "atom_feature = _\n",
    "\n",
    "#更新索引\n",
    "gas_feature.index = atom_feature.index+\"+\"+gas_feature.index\n",
    "atom_feature.index = gas_feature.index\n",
    "df = pd.concat([atom_feature,gas_feature],axis=1)\n",
    "df.drop([\"ed\",\"M\",\"rg\",\"Ng\",\"eg\",\"Ag\",\"Nmg\"], axis=1, inplace=True)\n",
    "ade = pri_feature[[\"ade\"]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d321249",
   "metadata": {},
   "outputs": [],
   "source": [
    "#得到X和y，分别进行归一化处理\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#先保存用来逆转归一化的值\n",
    "import copy\n",
    "X =  copy.deepcopy(df).values\n",
    "y =  copy.deepcopy(ade).values\n",
    "#归一化\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler() \n",
    "df[list(df.columns)] = scaler.fit_transform(df[list(df.columns)])\n",
    "ade[list(ade.columns)] = scaler.fit_transform(ade[list(ade.columns)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e278f8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义预测函数，主要用来逆转归一化\n",
    "def model_predict(model,y,X_data,y_data):\n",
    "    result = model.predict(X_data)\n",
    "    y_pred = result*(y.max()-y.min())+y.min()\n",
    "    y_pred = y_pred.reshape(-1)\n",
    "    y_real = y_data*(y.max()-y.min())+y.min()\n",
    "    return y_real,y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2527b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#根据逆转后的真实值，可以定义模型评价函数\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "def compute_r2_and_rmse(y_true, y_pred):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return r2,rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5830f013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7179be1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#现在df为104个特征，需要将df划分为40+64个\n",
    "#先给出40个的索引，[2,4,5,8,11,13,16,20,23,25]转化40个索引应该为\n",
    "_ = [2,4,5,8,11,13,16,20,23,25]\n",
    "\n",
    "#__为四十个df的索引\n",
    "__ = [4*i+j for i in _ for j in range(4) ]\n",
    "df1 = df.iloc[__,:]\n",
    "\n",
    "#___为64个的索引\n",
    "___ = [i for i in range(104) if i not in __]\n",
    "df2 = df.iloc[___,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ee1bffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#然后df1和ade形成机器学习训练数据\n",
    "ade.index = df1.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0dcb3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rM</th>\n",
       "      <th>Nm</th>\n",
       "      <th>Im</th>\n",
       "      <th>dc</th>\n",
       "      <th>Am</th>\n",
       "      <th>Im3</th>\n",
       "      <th>bp</th>\n",
       "      <th>Lmin</th>\n",
       "      <th>de</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Ig</th>\n",
       "      <th>rNg</th>\n",
       "      <th>MNg</th>\n",
       "      <th>ve</th>\n",
       "      <th>ge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>V+CO</th>\n",
       "      <td>0.414286</td>\n",
       "      <td>0.359649</td>\n",
       "      <td>0.165721</td>\n",
       "      <td>0.846743</td>\n",
       "      <td>0.246241</td>\n",
       "      <td>0.738233</td>\n",
       "      <td>0.533173</td>\n",
       "      <td>0.168831</td>\n",
       "      <td>0.261649</td>\n",
       "      <td>0.354309</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.663617</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.889344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V+H2S</th>\n",
       "      <td>0.414286</td>\n",
       "      <td>0.359649</td>\n",
       "      <td>0.165721</td>\n",
       "      <td>0.846743</td>\n",
       "      <td>0.246241</td>\n",
       "      <td>0.738233</td>\n",
       "      <td>0.533173</td>\n",
       "      <td>0.168831</td>\n",
       "      <td>0.261649</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914327</td>\n",
       "      <td>0.455667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V+CH4</th>\n",
       "      <td>0.414286</td>\n",
       "      <td>0.359649</td>\n",
       "      <td>0.165721</td>\n",
       "      <td>0.846743</td>\n",
       "      <td>0.246241</td>\n",
       "      <td>0.738233</td>\n",
       "      <td>0.533173</td>\n",
       "      <td>0.168831</td>\n",
       "      <td>0.261649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.564549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V+C2H6</th>\n",
       "      <td>0.414286</td>\n",
       "      <td>0.359649</td>\n",
       "      <td>0.165721</td>\n",
       "      <td>0.846743</td>\n",
       "      <td>0.246241</td>\n",
       "      <td>0.738233</td>\n",
       "      <td>0.533173</td>\n",
       "      <td>0.168831</td>\n",
       "      <td>0.261649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177667</td>\n",
       "      <td>0.666057</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mn+CO</th>\n",
       "      <td>0.271429</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.382483</td>\n",
       "      <td>0.542146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.847219</td>\n",
       "      <td>0.246124</td>\n",
       "      <td>0.064935</td>\n",
       "      <td>0.362903</td>\n",
       "      <td>0.354309</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.663617</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.889344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mn+H2S</th>\n",
       "      <td>0.271429</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.382483</td>\n",
       "      <td>0.542146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.847219</td>\n",
       "      <td>0.246124</td>\n",
       "      <td>0.064935</td>\n",
       "      <td>0.362903</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914327</td>\n",
       "      <td>0.455667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mn+CH4</th>\n",
       "      <td>0.271429</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.382483</td>\n",
       "      <td>0.542146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.847219</td>\n",
       "      <td>0.246124</td>\n",
       "      <td>0.064935</td>\n",
       "      <td>0.362903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.564549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mn+C2H6</th>\n",
       "      <td>0.271429</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.382483</td>\n",
       "      <td>0.542146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.847219</td>\n",
       "      <td>0.246124</td>\n",
       "      <td>0.064935</td>\n",
       "      <td>0.362903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177667</td>\n",
       "      <td>0.666057</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fe+CO</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.535088</td>\n",
       "      <td>0.530246</td>\n",
       "      <td>0.613027</td>\n",
       "      <td>0.076598</td>\n",
       "      <td>0.771457</td>\n",
       "      <td>0.416732</td>\n",
       "      <td>0.129870</td>\n",
       "      <td>0.405914</td>\n",
       "      <td>0.354309</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.663617</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.889344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fe+H2S</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.535088</td>\n",
       "      <td>0.530246</td>\n",
       "      <td>0.613027</td>\n",
       "      <td>0.076598</td>\n",
       "      <td>0.771457</td>\n",
       "      <td>0.416732</td>\n",
       "      <td>0.129870</td>\n",
       "      <td>0.405914</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914327</td>\n",
       "      <td>0.455667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fe+CH4</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.535088</td>\n",
       "      <td>0.530246</td>\n",
       "      <td>0.613027</td>\n",
       "      <td>0.076598</td>\n",
       "      <td>0.771457</td>\n",
       "      <td>0.416732</td>\n",
       "      <td>0.129870</td>\n",
       "      <td>0.405914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.564549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fe+C2H6</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.535088</td>\n",
       "      <td>0.530246</td>\n",
       "      <td>0.613027</td>\n",
       "      <td>0.076598</td>\n",
       "      <td>0.771457</td>\n",
       "      <td>0.416732</td>\n",
       "      <td>0.129870</td>\n",
       "      <td>0.405914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177667</td>\n",
       "      <td>0.666057</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cu+CO</th>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.596491</td>\n",
       "      <td>0.474795</td>\n",
       "      <td>0.302682</td>\n",
       "      <td>0.576598</td>\n",
       "      <td>0.927259</td>\n",
       "      <td>0.352968</td>\n",
       "      <td>0.311688</td>\n",
       "      <td>0.815412</td>\n",
       "      <td>0.354309</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.663617</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.889344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cu+H2S</th>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.596491</td>\n",
       "      <td>0.474795</td>\n",
       "      <td>0.302682</td>\n",
       "      <td>0.576598</td>\n",
       "      <td>0.927259</td>\n",
       "      <td>0.352968</td>\n",
       "      <td>0.311688</td>\n",
       "      <td>0.815412</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914327</td>\n",
       "      <td>0.455667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cu+CH4</th>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.596491</td>\n",
       "      <td>0.474795</td>\n",
       "      <td>0.302682</td>\n",
       "      <td>0.576598</td>\n",
       "      <td>0.927259</td>\n",
       "      <td>0.352968</td>\n",
       "      <td>0.311688</td>\n",
       "      <td>0.815412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.564549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cu+C2H6</th>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.596491</td>\n",
       "      <td>0.474795</td>\n",
       "      <td>0.302682</td>\n",
       "      <td>0.576598</td>\n",
       "      <td>0.927259</td>\n",
       "      <td>0.352968</td>\n",
       "      <td>0.311688</td>\n",
       "      <td>0.815412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177667</td>\n",
       "      <td>0.666057</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zr+CO</th>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.096491</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.911877</td>\n",
       "      <td>0.200188</td>\n",
       "      <td>0.578656</td>\n",
       "      <td>0.746860</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.175627</td>\n",
       "      <td>0.354309</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.663617</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.889344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zr+H2S</th>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.096491</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.911877</td>\n",
       "      <td>0.200188</td>\n",
       "      <td>0.578656</td>\n",
       "      <td>0.746860</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.175627</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914327</td>\n",
       "      <td>0.455667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zr+CH4</th>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.096491</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.911877</td>\n",
       "      <td>0.200188</td>\n",
       "      <td>0.578656</td>\n",
       "      <td>0.746860</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.175627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.564549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zr+C2H6</th>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.096491</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.911877</td>\n",
       "      <td>0.200188</td>\n",
       "      <td>0.578656</td>\n",
       "      <td>0.746860</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.175627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177667</td>\n",
       "      <td>0.666057</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mo+CO</th>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.824561</td>\n",
       "      <td>0.274732</td>\n",
       "      <td>0.601533</td>\n",
       "      <td>0.350094</td>\n",
       "      <td>0.682859</td>\n",
       "      <td>0.795910</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.179211</td>\n",
       "      <td>0.354309</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.663617</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.889344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mo+H2S</th>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.824561</td>\n",
       "      <td>0.274732</td>\n",
       "      <td>0.601533</td>\n",
       "      <td>0.350094</td>\n",
       "      <td>0.682859</td>\n",
       "      <td>0.795910</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.179211</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914327</td>\n",
       "      <td>0.455667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mo+CH4</th>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.824561</td>\n",
       "      <td>0.274732</td>\n",
       "      <td>0.601533</td>\n",
       "      <td>0.350094</td>\n",
       "      <td>0.682859</td>\n",
       "      <td>0.795910</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.179211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.564549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mo+C2H6</th>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.824561</td>\n",
       "      <td>0.274732</td>\n",
       "      <td>0.601533</td>\n",
       "      <td>0.350094</td>\n",
       "      <td>0.682859</td>\n",
       "      <td>0.795910</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.179211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177667</td>\n",
       "      <td>0.666057</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rh+CO</th>\n",
       "      <td>0.442857</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>0.390359</td>\n",
       "      <td>0.371648</td>\n",
       "      <td>0.534305</td>\n",
       "      <td>0.781777</td>\n",
       "      <td>0.594592</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.525090</td>\n",
       "      <td>0.354309</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.663617</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.889344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rh+H2S</th>\n",
       "      <td>0.442857</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>0.390359</td>\n",
       "      <td>0.371648</td>\n",
       "      <td>0.534305</td>\n",
       "      <td>0.781777</td>\n",
       "      <td>0.594592</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.525090</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914327</td>\n",
       "      <td>0.455667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rh+CH4</th>\n",
       "      <td>0.442857</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>0.390359</td>\n",
       "      <td>0.371648</td>\n",
       "      <td>0.534305</td>\n",
       "      <td>0.781777</td>\n",
       "      <td>0.594592</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.525090</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.564549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rh+C2H6</th>\n",
       "      <td>0.442857</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>0.390359</td>\n",
       "      <td>0.371648</td>\n",
       "      <td>0.534305</td>\n",
       "      <td>0.781777</td>\n",
       "      <td>0.594592</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.525090</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177667</td>\n",
       "      <td>0.666057</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ta+CO</th>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.245614</td>\n",
       "      <td>0.526150</td>\n",
       "      <td>0.829502</td>\n",
       "      <td>0.150376</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.970570</td>\n",
       "      <td>0.324675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.354309</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.663617</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.889344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ta+H2S</th>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.245614</td>\n",
       "      <td>0.526150</td>\n",
       "      <td>0.829502</td>\n",
       "      <td>0.150376</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.970570</td>\n",
       "      <td>0.324675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914327</td>\n",
       "      <td>0.455667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ta+CH4</th>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.245614</td>\n",
       "      <td>0.526150</td>\n",
       "      <td>0.829502</td>\n",
       "      <td>0.150376</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.970570</td>\n",
       "      <td>0.324675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.564549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ta+C2H6</th>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.245614</td>\n",
       "      <td>0.526150</td>\n",
       "      <td>0.829502</td>\n",
       "      <td>0.150376</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.970570</td>\n",
       "      <td>0.324675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177667</td>\n",
       "      <td>0.666057</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Os+CO</th>\n",
       "      <td>0.614286</td>\n",
       "      <td>0.859649</td>\n",
       "      <td>0.784499</td>\n",
       "      <td>0.346743</td>\n",
       "      <td>0.516917</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.875456</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.241935</td>\n",
       "      <td>0.354309</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.663617</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.889344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Os+H2S</th>\n",
       "      <td>0.614286</td>\n",
       "      <td>0.859649</td>\n",
       "      <td>0.784499</td>\n",
       "      <td>0.346743</td>\n",
       "      <td>0.516917</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.875456</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.241935</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914327</td>\n",
       "      <td>0.455667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Os+CH4</th>\n",
       "      <td>0.614286</td>\n",
       "      <td>0.859649</td>\n",
       "      <td>0.784499</td>\n",
       "      <td>0.346743</td>\n",
       "      <td>0.516917</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.875456</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.241935</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.564549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Os+C2H6</th>\n",
       "      <td>0.614286</td>\n",
       "      <td>0.859649</td>\n",
       "      <td>0.784499</td>\n",
       "      <td>0.346743</td>\n",
       "      <td>0.516917</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.875456</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.241935</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177667</td>\n",
       "      <td>0.666057</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pt+CO</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>0.882168</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.622316</td>\n",
       "      <td>0.402597</td>\n",
       "      <td>0.635305</td>\n",
       "      <td>0.354309</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.663617</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.889344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pt+H2S</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>0.882168</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.622316</td>\n",
       "      <td>0.402597</td>\n",
       "      <td>0.635305</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914327</td>\n",
       "      <td>0.455667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pt+CH4</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>0.882168</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.622316</td>\n",
       "      <td>0.402597</td>\n",
       "      <td>0.635305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.564549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pt+C2H6</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>0.882168</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.622316</td>\n",
       "      <td>0.402597</td>\n",
       "      <td>0.635305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177667</td>\n",
       "      <td>0.666057</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               rM        Nm        Im        dc        Am       Im3        bp  \\\n",
       "V+CO     0.414286  0.359649  0.165721  0.846743  0.246241  0.738233  0.533173   \n",
       "V+H2S    0.414286  0.359649  0.165721  0.846743  0.246241  0.738233  0.533173   \n",
       "V+CH4    0.414286  0.359649  0.165721  0.846743  0.246241  0.738233  0.533173   \n",
       "V+C2H6   0.414286  0.359649  0.165721  0.846743  0.246241  0.738233  0.533173   \n",
       "Mn+CO    0.271429  0.289474  0.382483  0.542146  0.000000  0.847219  0.246124   \n",
       "Mn+H2S   0.271429  0.289474  0.382483  0.542146  0.000000  0.847219  0.246124   \n",
       "Mn+CH4   0.271429  0.289474  0.382483  0.542146  0.000000  0.847219  0.246124   \n",
       "Mn+C2H6  0.271429  0.289474  0.382483  0.542146  0.000000  0.847219  0.246124   \n",
       "Fe+CO    0.200000  0.535088  0.530246  0.613027  0.076598  0.771457  0.416732   \n",
       "Fe+H2S   0.200000  0.535088  0.530246  0.613027  0.076598  0.771457  0.416732   \n",
       "Fe+CH4   0.200000  0.535088  0.530246  0.613027  0.076598  0.771457  0.416732   \n",
       "Fe+C2H6  0.200000  0.535088  0.530246  0.613027  0.076598  0.771457  0.416732   \n",
       "Cu+CO    0.042857  0.596491  0.474795  0.302682  0.576598  0.927259  0.352968   \n",
       "Cu+H2S   0.042857  0.596491  0.474795  0.302682  0.576598  0.927259  0.352968   \n",
       "Cu+CH4   0.042857  0.596491  0.474795  0.302682  0.576598  0.927259  0.352968   \n",
       "Cu+C2H6  0.042857  0.596491  0.474795  0.302682  0.576598  0.927259  0.352968   \n",
       "Zr+CO    0.914286  0.096491  0.130435  0.911877  0.200188  0.578656  0.746860   \n",
       "Zr+H2S   0.914286  0.096491  0.130435  0.911877  0.200188  0.578656  0.746860   \n",
       "Zr+CH4   0.914286  0.096491  0.130435  0.911877  0.200188  0.578656  0.746860   \n",
       "Zr+C2H6  0.914286  0.096491  0.130435  0.911877  0.200188  0.578656  0.746860   \n",
       "Mo+CO    0.685714  0.824561  0.274732  0.601533  0.350094  0.682859  0.795910   \n",
       "Mo+H2S   0.685714  0.824561  0.274732  0.601533  0.350094  0.682859  0.795910   \n",
       "Mo+CH4   0.685714  0.824561  0.274732  0.601533  0.350094  0.682859  0.795910   \n",
       "Mo+C2H6  0.685714  0.824561  0.274732  0.601533  0.350094  0.682859  0.795910   \n",
       "Rh+CO    0.442857  0.929825  0.390359  0.371648  0.534305  0.781777  0.594592   \n",
       "Rh+H2S   0.442857  0.929825  0.390359  0.371648  0.534305  0.781777  0.594592   \n",
       "Rh+CH4   0.442857  0.929825  0.390359  0.371648  0.534305  0.781777  0.594592   \n",
       "Rh+C2H6  0.442857  0.929825  0.390359  0.371648  0.534305  0.781777  0.594592   \n",
       "Ta+CO    0.828571  0.245614  0.526150  0.829502  0.150376  0.000000  0.970570   \n",
       "Ta+H2S   0.828571  0.245614  0.526150  0.829502  0.150376  0.000000  0.970570   \n",
       "Ta+CH4   0.828571  0.245614  0.526150  0.829502  0.150376  0.000000  0.970570   \n",
       "Ta+C2H6  0.828571  0.245614  0.526150  0.829502  0.150376  0.000000  0.970570   \n",
       "Os+CO    0.614286  0.859649  0.784499  0.346743  0.516917  0.000000  0.875456   \n",
       "Os+H2S   0.614286  0.859649  0.784499  0.346743  0.516917  0.000000  0.875456   \n",
       "Os+CH4   0.614286  0.859649  0.784499  0.346743  0.516917  0.000000  0.875456   \n",
       "Os+C2H6  0.614286  0.859649  0.784499  0.346743  0.516917  0.000000  0.875456   \n",
       "Pt+CO    0.500000  0.929825  0.882168  0.310345  1.000000  0.000000  0.622316   \n",
       "Pt+H2S   0.500000  0.929825  0.882168  0.310345  1.000000  0.000000  0.622316   \n",
       "Pt+CH4   0.500000  0.929825  0.882168  0.310345  1.000000  0.000000  0.622316   \n",
       "Pt+C2H6  0.500000  0.929825  0.882168  0.310345  1.000000  0.000000  0.622316   \n",
       "\n",
       "             Lmin        de        Mg        Ig       rNg       MNg        ve  \\\n",
       "V+CO     0.168831  0.261649  0.354309  1.000000  1.000000  0.663617  0.333333   \n",
       "V+H2S    0.168831  0.261649  1.000000  0.914327  0.455667  1.000000  0.000000   \n",
       "V+CH4    0.168831  0.261649  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "V+C2H6   0.168831  0.261649  0.000000  0.000000  0.177667  0.666057  1.000000   \n",
       "Mn+CO    0.064935  0.362903  0.354309  1.000000  1.000000  0.663617  0.333333   \n",
       "Mn+H2S   0.064935  0.362903  1.000000  0.914327  0.455667  1.000000  0.000000   \n",
       "Mn+CH4   0.064935  0.362903  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "Mn+C2H6  0.064935  0.362903  0.000000  0.000000  0.177667  0.666057  1.000000   \n",
       "Fe+CO    0.129870  0.405914  0.354309  1.000000  1.000000  0.663617  0.333333   \n",
       "Fe+H2S   0.129870  0.405914  1.000000  0.914327  0.455667  1.000000  0.000000   \n",
       "Fe+CH4   0.129870  0.405914  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "Fe+C2H6  0.129870  0.405914  0.000000  0.000000  0.177667  0.666057  1.000000   \n",
       "Cu+CO    0.311688  0.815412  0.354309  1.000000  1.000000  0.663617  0.333333   \n",
       "Cu+H2S   0.311688  0.815412  1.000000  0.914327  0.455667  1.000000  0.000000   \n",
       "Cu+CH4   0.311688  0.815412  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "Cu+C2H6  0.311688  0.815412  0.000000  0.000000  0.177667  0.666057  1.000000   \n",
       "Zr+CO    0.454545  0.175627  0.354309  1.000000  1.000000  0.663617  0.333333   \n",
       "Zr+H2S   0.454545  0.175627  1.000000  0.914327  0.455667  1.000000  0.000000   \n",
       "Zr+CH4   0.454545  0.175627  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "Zr+C2H6  0.454545  0.175627  0.000000  0.000000  0.177667  0.666057  1.000000   \n",
       "Mo+CO    0.285714  0.179211  0.354309  1.000000  1.000000  0.663617  0.333333   \n",
       "Mo+H2S   0.285714  0.179211  1.000000  0.914327  0.455667  1.000000  0.000000   \n",
       "Mo+CH4   0.285714  0.179211  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "Mo+C2H6  0.285714  0.179211  0.000000  0.000000  0.177667  0.666057  1.000000   \n",
       "Rh+CO    0.272727  0.525090  0.354309  1.000000  1.000000  0.663617  0.333333   \n",
       "Rh+H2S   0.272727  0.525090  1.000000  0.914327  0.455667  1.000000  0.000000   \n",
       "Rh+CH4   0.272727  0.525090  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "Rh+C2H6  0.272727  0.525090  0.000000  0.000000  0.177667  0.666057  1.000000   \n",
       "Ta+CO    0.324675  0.000000  0.354309  1.000000  1.000000  0.663617  0.333333   \n",
       "Ta+H2S   0.324675  0.000000  1.000000  0.914327  0.455667  1.000000  0.000000   \n",
       "Ta+CH4   0.324675  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "Ta+C2H6  0.324675  0.000000  0.000000  0.000000  0.177667  0.666057  1.000000   \n",
       "Os+CO    0.272727  0.241935  0.354309  1.000000  1.000000  0.663617  0.333333   \n",
       "Os+H2S   0.272727  0.241935  1.000000  0.914327  0.455667  1.000000  0.000000   \n",
       "Os+CH4   0.272727  0.241935  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "Os+C2H6  0.272727  0.241935  0.000000  0.000000  0.177667  0.666057  1.000000   \n",
       "Pt+CO    0.402597  0.635305  0.354309  1.000000  1.000000  0.663617  0.333333   \n",
       "Pt+H2S   0.402597  0.635305  1.000000  0.914327  0.455667  1.000000  0.000000   \n",
       "Pt+CH4   0.402597  0.635305  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "Pt+C2H6  0.402597  0.635305  0.000000  0.000000  0.177667  0.666057  1.000000   \n",
       "\n",
       "               ge  \n",
       "V+CO     0.889344  \n",
       "V+H2S    1.000000  \n",
       "V+CH4    0.564549  \n",
       "V+C2H6   0.000000  \n",
       "Mn+CO    0.889344  \n",
       "Mn+H2S   1.000000  \n",
       "Mn+CH4   0.564549  \n",
       "Mn+C2H6  0.000000  \n",
       "Fe+CO    0.889344  \n",
       "Fe+H2S   1.000000  \n",
       "Fe+CH4   0.564549  \n",
       "Fe+C2H6  0.000000  \n",
       "Cu+CO    0.889344  \n",
       "Cu+H2S   1.000000  \n",
       "Cu+CH4   0.564549  \n",
       "Cu+C2H6  0.000000  \n",
       "Zr+CO    0.889344  \n",
       "Zr+H2S   1.000000  \n",
       "Zr+CH4   0.564549  \n",
       "Zr+C2H6  0.000000  \n",
       "Mo+CO    0.889344  \n",
       "Mo+H2S   1.000000  \n",
       "Mo+CH4   0.564549  \n",
       "Mo+C2H6  0.000000  \n",
       "Rh+CO    0.889344  \n",
       "Rh+H2S   1.000000  \n",
       "Rh+CH4   0.564549  \n",
       "Rh+C2H6  0.000000  \n",
       "Ta+CO    0.889344  \n",
       "Ta+H2S   1.000000  \n",
       "Ta+CH4   0.564549  \n",
       "Ta+C2H6  0.000000  \n",
       "Os+CO    0.889344  \n",
       "Os+H2S   1.000000  \n",
       "Os+CH4   0.564549  \n",
       "Os+C2H6  0.000000  \n",
       "Pt+CO    0.889344  \n",
       "Pt+H2S   1.000000  \n",
       "Pt+CH4   0.564549  \n",
       "Pt+C2H6  0.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#处理后用于ML训练的特征如下\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea723171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>V+CO</th>\n",
       "      <td>0.245509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V+H2S</th>\n",
       "      <td>0.607784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V+CH4</th>\n",
       "      <td>0.841317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V+C2H6</th>\n",
       "      <td>0.805389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mn+CO</th>\n",
       "      <td>0.077844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mn+H2S</th>\n",
       "      <td>0.458084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mn+CH4</th>\n",
       "      <td>0.739521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mn+C2H6</th>\n",
       "      <td>0.733533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fe+CO</th>\n",
       "      <td>0.212575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fe+H2S</th>\n",
       "      <td>0.691617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fe+CH4</th>\n",
       "      <td>0.910180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fe+C2H6</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cu+CO</th>\n",
       "      <td>0.616766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cu+H2S</th>\n",
       "      <td>0.817365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cu+CH4</th>\n",
       "      <td>0.916168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cu+C2H6</th>\n",
       "      <td>0.913174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zr+CO</th>\n",
       "      <td>0.482036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zr+H2S</th>\n",
       "      <td>0.676647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zr+CH4</th>\n",
       "      <td>0.829341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zr+C2H6</th>\n",
       "      <td>0.808383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mo+CO</th>\n",
       "      <td>0.137725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mo+H2S</th>\n",
       "      <td>0.440120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mo+CH4</th>\n",
       "      <td>0.841317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mo+C2H6</th>\n",
       "      <td>0.805389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rh+CO</th>\n",
       "      <td>0.173653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rh+H2S</th>\n",
       "      <td>0.619760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rh+CH4</th>\n",
       "      <td>0.922156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rh+C2H6</th>\n",
       "      <td>0.916168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ta+CO</th>\n",
       "      <td>0.251497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ta+H2S</th>\n",
       "      <td>0.601796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ta+CH4</th>\n",
       "      <td>0.841317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ta+C2H6</th>\n",
       "      <td>0.805389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Os+CO</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Os+H2S</th>\n",
       "      <td>0.544910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Os+CH4</th>\n",
       "      <td>0.922156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Os+C2H6</th>\n",
       "      <td>0.901198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pt+CO</th>\n",
       "      <td>0.302395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pt+H2S</th>\n",
       "      <td>0.724551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pt+CH4</th>\n",
       "      <td>0.925150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pt+C2H6</th>\n",
       "      <td>0.925150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ade\n",
       "V+CO     0.245509\n",
       "V+H2S    0.607784\n",
       "V+CH4    0.841317\n",
       "V+C2H6   0.805389\n",
       "Mn+CO    0.077844\n",
       "Mn+H2S   0.458084\n",
       "Mn+CH4   0.739521\n",
       "Mn+C2H6  0.733533\n",
       "Fe+CO    0.212575\n",
       "Fe+H2S   0.691617\n",
       "Fe+CH4   0.910180\n",
       "Fe+C2H6  1.000000\n",
       "Cu+CO    0.616766\n",
       "Cu+H2S   0.817365\n",
       "Cu+CH4   0.916168\n",
       "Cu+C2H6  0.913174\n",
       "Zr+CO    0.482036\n",
       "Zr+H2S   0.676647\n",
       "Zr+CH4   0.829341\n",
       "Zr+C2H6  0.808383\n",
       "Mo+CO    0.137725\n",
       "Mo+H2S   0.440120\n",
       "Mo+CH4   0.841317\n",
       "Mo+C2H6  0.805389\n",
       "Rh+CO    0.173653\n",
       "Rh+H2S   0.619760\n",
       "Rh+CH4   0.922156\n",
       "Rh+C2H6  0.916168\n",
       "Ta+CO    0.251497\n",
       "Ta+H2S   0.601796\n",
       "Ta+CH4   0.841317\n",
       "Ta+C2H6  0.805389\n",
       "Os+CO    0.000000\n",
       "Os+H2S   0.544910\n",
       "Os+CH4   0.922156\n",
       "Os+C2H6  0.901198\n",
       "Pt+CO    0.302395\n",
       "Pt+H2S   0.724551\n",
       "Pt+CH4   0.925150\n",
       "Pt+C2H6  0.925150"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#处理后用于ML训练的目标值\n",
    "ade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5da0b7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成模型\n",
    "svr = SVR(kernel = \"rbf\",C = 1)\n",
    "\n",
    "\n",
    "RFR = RandomForestRegressor(n_estimators=300 \n",
    "                            ,max_depth = 10\n",
    "                            ,max_features = 11\n",
    "                            )\n",
    "from sklearn.linear_model import Ridge\n",
    "Rge = Ridge(alpha = 0.11)\n",
    "\n",
    "import xgboost as xgb\n",
    "# 定义XGBoost模型\n",
    "xgb = xgb.XGBRegressor(\n",
    "    n_estimators=300\n",
    "    ,eta =0.5\n",
    "    ,max_depth = 3\n",
    "    )\n",
    "GBR = GradientBoostingRegressor(n_estimators=300\n",
    "                                ,max_depth = 3\n",
    "                                ,max_features = 4\n",
    "                                \n",
    "                               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f75752bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9151310456566589 0.2680077867747234 0.7611707594777444 0.4219836805910623\n"
     ]
    }
   ],
   "source": [
    "#单次支持向量5折交叉验证 \n",
    "kf = KFold(n_splits=5\n",
    "              ,shuffle=True\n",
    "              ,random_state = 42\n",
    "          )  \n",
    "#保存R2和RMSE\n",
    "train_ = []\n",
    "train__ = []\n",
    "test_ = []\n",
    "test__ = []\n",
    "# 执行交叉验证并获取每次划分的索引\n",
    "for i,(train_index, test_index) in enumerate(kf.split(df1)):\n",
    "        #print(f\"Fold {i+1}:\")\n",
    "        #print(\"Train Index:\", train_index)\n",
    "        #print(\"Test Index:\", test_index)\n",
    "        X_train= df1.iloc[train_index,:]\n",
    "        X_test =  df1.iloc[test_index,:]\n",
    "        y_train = ade.iloc[train_index,:]\n",
    "        y_test = ade.iloc[test_index,:]\n",
    "        # 训练模型\n",
    "        svr.fit(X_train,y_train)\n",
    "        #输出真实值和预测值，这里是已经逆转归一化的预测值\n",
    "        y_real_train,y_pred_train = model_predict(svr,y,X_train,y_train)\n",
    "        y_real_test,y_pred_test = model_predict(svr,y,X_test,y_test) \n",
    "        #输出模型评价\n",
    "        evalue_train = compute_r2_and_rmse(y_real_train,y_pred_train)\n",
    "        evalue_test = compute_r2_and_rmse(y_real_test,y_pred_test)\n",
    "        #print(\"training_r2:%.2f,rmse:%.2f\"%(evalue_train[0],evalue_train[1]))\n",
    "        #print(\"testing_r2:%.2f,rmse:%.2f\"%(evalue_test[0],evalue_test[1]))\n",
    "        train_.append(evalue_train[0])\n",
    "        train__.append(evalue_train[1])\n",
    "        test_.append(evalue_test[0])\n",
    "        test__.append(evalue_test[1])\n",
    "print(np.mean(train_),np.mean(train__),np.mean(test_),np.mean(test__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a696fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9746710782403929 0.1455880489427847 0.7998116063444896 0.3513530960416099\n"
     ]
    }
   ],
   "source": [
    "#单次RFR5折交叉验证 \n",
    "kf = KFold(n_splits=5\n",
    "              ,shuffle=True\n",
    "              ,random_state = 42\n",
    "          )  \n",
    "#保存R2和RMSE\n",
    "train_ = []\n",
    "train__ = []\n",
    "test_ = []\n",
    "test__ = []\n",
    "# 执行交叉验证并获取每次划分的索引\n",
    "for i,(train_index, test_index) in enumerate(kf.split(df1)):\n",
    "        #print(f\"Fold {i+1}:\")\n",
    "        #print(\"Train Index:\", train_index)\n",
    "        #print(\"Test Index:\", test_index)\n",
    "        X_train= df1.iloc[train_index,:]\n",
    "        X_test =  df1.iloc[test_index,:]\n",
    "        y_train = ade.iloc[train_index,:]\n",
    "        y_test = ade.iloc[test_index,:]\n",
    "        # 训练模型\n",
    "        RFR.fit(X_train,y_train)\n",
    "        #输出真实值和预测值，这里是已经逆转归一化的预测值\n",
    "        y_real_train,y_pred_train = model_predict(RFR,y,X_train,y_train)\n",
    "        y_real_test,y_pred_test = model_predict(RFR,y,X_test,y_test) \n",
    "        #输出模型评价\n",
    "        evalue_train = compute_r2_and_rmse(y_real_train,y_pred_train)\n",
    "        evalue_test = compute_r2_and_rmse(y_real_test,y_pred_test)\n",
    "        #print(\"training_r2:%.2f,rmse:%.2f\"%(evalue_train[0],evalue_train[1]))\n",
    "        #print(\"testing_r2:%.2f,rmse:%.2f\"%(evalue_test[0],evalue_test[1]))\n",
    "        train_.append(evalue_train[0])\n",
    "        train__.append(evalue_train[1])\n",
    "        test_.append(evalue_test[0])\n",
    "        test__.append(evalue_test[1])\n",
    "print(np.mean(train_),np.mean(train__),np.mean(test_),np.mean(test__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b08f477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9184228676174356 0.26130267446311833 0.8533614377741923 0.32692886287836465\n"
     ]
    }
   ],
   "source": [
    "#单次Ridge5折交叉验证 \n",
    "kf = KFold(n_splits=5\n",
    "              ,shuffle=True\n",
    "              ,random_state = 42\n",
    "          )  \n",
    "#保存R2和RMSE\n",
    "train_ = []\n",
    "train__ = []\n",
    "test_ = []\n",
    "test__ = []\n",
    "# 执行交叉验证并获取每次划分的索引\n",
    "for i,(train_index, test_index) in enumerate(kf.split(df1)):\n",
    "        #print(f\"Fold {i+1}:\")\n",
    "        #print(\"Train Index:\", train_index)\n",
    "        #print(\"Test Index:\", test_index)\n",
    "        X_train= df1.iloc[train_index,:]\n",
    "        X_test =  df1.iloc[test_index,:]\n",
    "        y_train = ade.iloc[train_index,:]\n",
    "        y_test = ade.iloc[test_index,:]\n",
    "        # 训练模型\n",
    "        Rge.fit(X_train,y_train)\n",
    "        #输出真实值和预测值，这里是已经逆转归一化的预测值\n",
    "        y_real_train,y_pred_train = model_predict(Rge,y,X_train,y_train)\n",
    "        y_real_test,y_pred_test = model_predict(Rge,y,X_test,y_test) \n",
    "        #输出模型评价\n",
    "        evalue_train = compute_r2_and_rmse(y_real_train,y_pred_train)\n",
    "        evalue_test = compute_r2_and_rmse(y_real_test,y_pred_test)\n",
    "        #print(\"training_r2:%.2f,rmse:%.2f\"%(evalue_train[0],evalue_train[1]))\n",
    "        #print(\"testing_r2:%.2f,rmse:%.2f\"%(evalue_test[0],evalue_test[1]))\n",
    "        train_.append(evalue_train[0])\n",
    "        train__.append(evalue_train[1])\n",
    "        test_.append(evalue_test[0])\n",
    "        test__.append(evalue_test[1])\n",
    "print(np.mean(train_),np.mean(train__),np.mean(test_),np.mean(test__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dca151b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9998878317500802 0.009728790691578722 0.8230421090516243 0.35865312922259684\n"
     ]
    }
   ],
   "source": [
    "#单次XGB5折交叉验证 \n",
    "kf = KFold(n_splits=5\n",
    "              ,shuffle=True\n",
    "              ,random_state = 42\n",
    "          )  \n",
    "#保存R2和RMSE\n",
    "train_ = []\n",
    "train__ = []\n",
    "test_ = []\n",
    "test__ = []\n",
    "# 执行交叉验证并获取每次划分的索引\n",
    "for i,(train_index, test_index) in enumerate(kf.split(df1)):\n",
    "        #print(f\"Fold {i+1}:\")\n",
    "        #print(\"Train Index:\", train_index)\n",
    "        #print(\"Test Index:\", test_index)\n",
    "        X_train= df1.iloc[train_index,:]\n",
    "        X_test =  df1.iloc[test_index,:]\n",
    "        y_train = ade.iloc[train_index,:]\n",
    "        y_test = ade.iloc[test_index,:]\n",
    "        # 训练模型\n",
    "        xgb.fit(X_train,y_train)\n",
    "        #输出真实值和预测值，这里是已经逆转归一化的预测值\n",
    "        y_real_train,y_pred_train = model_predict(xgb,y,X_train,y_train)\n",
    "        y_real_test,y_pred_test = model_predict(xgb,y,X_test,y_test) \n",
    "        #输出模型评价\n",
    "        evalue_train = compute_r2_and_rmse(y_real_train,y_pred_train)\n",
    "        evalue_test = compute_r2_and_rmse(y_real_test,y_pred_test)\n",
    "        #print(\"training_r2:%.2f,rmse:%.2f\"%(evalue_train[0],evalue_train[1]))\n",
    "        #print(\"testing_r2:%.2f,rmse:%.2f\"%(evalue_test[0],evalue_test[1]))\n",
    "        train_.append(evalue_train[0])\n",
    "        train__.append(evalue_train[1])\n",
    "        test_.append(evalue_test[0])\n",
    "        test__.append(evalue_test[1])\n",
    "print(np.mean(train_),np.mean(train__),np.mean(test_),np.mean(test__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d4b1365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999496933043 0.0001728958468503777 0.8716981357405839 0.29196947406738927\n"
     ]
    }
   ],
   "source": [
    "#单次GBR5折交叉验证 \n",
    "kf = KFold(n_splits=5\n",
    "              ,shuffle=True\n",
    "              ,random_state = 42\n",
    "          )  \n",
    "#保存R2和RMSE\n",
    "train_ = []\n",
    "train__ = []\n",
    "test_ = []\n",
    "test__ = []\n",
    "# 执行交叉验证并获取每次划分的索引\n",
    "for i,(train_index, test_index) in enumerate(kf.split(df1)):\n",
    "        #print(f\"Fold {i+1}:\")\n",
    "        #print(\"Train Index:\", train_index)\n",
    "        #print(\"Test Index:\", test_index)\n",
    "        X_train= df1.iloc[train_index,:]\n",
    "        X_test =  df1.iloc[test_index,:]\n",
    "        y_train = ade.iloc[train_index,:]\n",
    "        y_test = ade.iloc[test_index,:]\n",
    "        # 训练模型\n",
    "        GBR.fit(X_train,y_train)\n",
    "        #输出真实值和预测值，这里是已经逆转归一化的预测值\n",
    "        y_real_train,y_pred_train = model_predict(GBR,y,X_train,y_train)\n",
    "        y_real_test,y_pred_test = model_predict(GBR,y,X_test,y_test) \n",
    "        #输出模型评价\n",
    "        evalue_train = compute_r2_and_rmse(y_real_train,y_pred_train)\n",
    "        evalue_test = compute_r2_and_rmse(y_real_test,y_pred_test)\n",
    "        #print(\"training_r2:%.2f,rmse:%.2f\"%(evalue_train[0],evalue_train[1]))\n",
    "        #print(\"testing_r2:%.2f,rmse:%.2f\"%(evalue_test[0],evalue_test[1]))\n",
    "        train_.append(evalue_train[0])\n",
    "        train__.append(evalue_train[1])\n",
    "        test_.append(evalue_test[0])\n",
    "        test__.append(evalue_test[1])\n",
    "print(np.mean(train_),np.mean(train__),np.mean(test_),np.mean(test__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82af44c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.initializers import RandomNormal\n",
    "bpnn = keras.Sequential([\n",
    "    keras.Input(shape = (16))\n",
    "    ,keras.layers.Dense(300\n",
    "                        ,activation = \"relu\"\n",
    "                        ,kernel_regularizer = keras.regularizers.l2(0.001)\n",
    "                        ,kernel_initializer = RandomNormal(seed = 123)\n",
    "                       )\n",
    "    ,keras.layers.Dropout(0.1)\n",
    "    ,keras.layers.Dense(300\n",
    "                        ,activation = \"relu\"\n",
    "                        ,kernel_regularizer = keras.regularizers.l2(0.001)\n",
    "                       )\n",
    "    ,keras.layers.Dense(1)\n",
    "    ,keras.layers.Dense(300\n",
    "                        ,activation = \"relu\"\n",
    "                        ,kernel_regularizer = keras.regularizers.l2(0.001)\n",
    "                       )\n",
    "    ,keras.layers.Dense(1)\n",
    "])\n",
    "adam = keras.optimizers.Adam(learning_rate = 0.001)\n",
    "bpnn.compile(loss = \"mse\",optimizer = adam)\n",
    "history = keras.callbacks.History()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5dad0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "WARNING:tensorflow:5 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000014B340D4D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "WARNING:tensorflow:6 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000014B340D4D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "0.9968721107001025 0.05123580095488793 0.921088878952062 0.23438128602674269\n"
     ]
    }
   ],
   "source": [
    "#然后拟合神经网络，对神经网络执行一次交叉验证\n",
    "kf = KFold(n_splits=5\n",
    "              ,shuffle=True\n",
    "              ,random_state = 42\n",
    "              )  \n",
    "#保存R2和RMSE\n",
    "train_ = []\n",
    "train__ = []\n",
    "test_ = []\n",
    "test__ = []\n",
    "#保存交叉验证的测试集的结果\n",
    "#这个fold_test在后续并没有被使用，本来打算作图，但后续放弃了这个图的制作\n",
    "fold_test= []\n",
    "# 执行交叉验证并获取每次划分的索引\n",
    "for i,(train_index, test_index) in enumerate(kf.split(df1)):\n",
    "        #print(f\"Fold {i+1}:\")\n",
    "        #print(\"Train Index:\", train_index)\n",
    "        #print(\"Test Index:\", test_index)\n",
    "        X_train= df1.iloc[train_index,:]\n",
    "        X_test =  df1.iloc[test_index,:]\n",
    "        y_train = ade.iloc[train_index,:]\n",
    "        y_test = ade.iloc[test_index,:]\n",
    "        \n",
    "        \n",
    "        #神经网络这里需要重新加载一下模型\n",
    "        bpnn = keras.Sequential([\n",
    "            keras.Input(shape = (15))\n",
    "            ,keras.layers.Dense(300\n",
    "                        ,activation = \"relu\"\n",
    "                        ,kernel_regularizer = keras.regularizers.l2(0.001)\n",
    "                        ,kernel_initializer = RandomNormal(seed = 123)\n",
    "                       )\n",
    "            ,keras.layers.Dropout(0.01)\n",
    "            ,keras.layers.Dense(300\n",
    "                        ,activation = \"relu\"\n",
    "                        ,kernel_regularizer = keras.regularizers.l2(0.001)\n",
    "                       )\n",
    "            ,keras.layers.Dropout(0.01)\n",
    "            ,keras.layers.Dense(300\n",
    "                        ,activation = \"relu\"\n",
    "                        ,kernel_regularizer = keras.regularizers.l2(0.001)\n",
    "                       )\n",
    "            ,keras.layers.Dropout(0.01)\n",
    "            ,keras.layers.Dense(1)\n",
    "            ])\n",
    "        adam = keras.optimizers.Adam(learning_rate = 0.001)\n",
    "        bpnn.compile(loss = \"mse\",optimizer = adam)\n",
    "        history = keras.callbacks.History()\n",
    "        # 训练模型\n",
    "        bpnn.fit(X_train,y_train,validation_data = (X_test,y_test),epochs = 2000,callbacks = [history],verbose = 0)\n",
    "        #输出真实值和预测值，这里是已经逆转归一化的预测值\n",
    "        y_real_train,y_pred_train = model_predict(bpnn,y,X_train,y_train)\n",
    "        y_real_test,y_pred_test = model_predict(bpnn,y,X_test,y_test)\n",
    "        fold_i = model_predict(bpnn,y,X_test,y_test)\n",
    "        fold_test.append(fold_i)\n",
    "        #输出模型评价\n",
    "        evalue_train = compute_r2_and_rmse(y_real_train,y_pred_train)\n",
    "        evalue_test = compute_r2_and_rmse(y_real_test,y_pred_test)\n",
    "        #print(\"training_r2:%.2f,rmse:%.2f\"%(evalue_train[0],evalue_train[1]))\n",
    "        #print(\"testing_r2:%.2f,rmse:%.2f\"%(evalue_test[0],evalue_test[1]))\n",
    "        train_.append(evalue_train[0])\n",
    "        train__.append(evalue_train[1])\n",
    "        test_.append(evalue_test[0])\n",
    "        test__.append(evalue_test[1])\n",
    "print(np.mean(train_),np.mean(train__),np.mean(test_),np.mean(test__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ac098b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.initializers import RandomNormal\n",
    "bpnn = keras.Sequential([\n",
    "    keras.Input(shape = (15))\n",
    "    ,keras.layers.Dense(300\n",
    "                        ,activation = \"relu\"\n",
    "                        ,kernel_regularizer = keras.regularizers.l2(0.001)\n",
    "\n",
    "                       )\n",
    "    ,keras.layers.Dropout(0.01)\n",
    "    ,keras.layers.Dense(300\n",
    "                        ,activation = \"relu\"\n",
    "                        ,kernel_regularizer = keras.regularizers.l2(0.001)\n",
    "           \n",
    "                       )\n",
    "    ,keras.layers.Dropout(0.01)\n",
    "    ,keras.layers.Dense(300\n",
    "                        ,activation = \"relu\"\n",
    "                        ,kernel_regularizer = keras.regularizers.l2(0.001)\n",
    "            \n",
    "                       )\n",
    "    ,keras.layers.Dropout(0.01)\n",
    "\n",
    "    ,keras.layers.Dense(1)\n",
    "])\n",
    "adam = keras.optimizers.Adam(learning_rate = 0.001)\n",
    "bpnn.compile(loss = \"mse\",optimizer = adam)\n",
    "history = keras.callbacks.History()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c31d52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(df1,ade,train_size=0.75,random_state = 521)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ddf75570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "1/1 [==============================] - 1s 956ms/step - loss: 1.0290 - val_loss: 0.7228\n",
      "Epoch 2/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.7706 - val_loss: 0.7249\n",
      "Epoch 3/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.7701 - val_loss: 0.7048\n",
      "Epoch 4/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.7426 - val_loss: 0.6501\n",
      "Epoch 5/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.6878 - val_loss: 0.6159\n",
      "Epoch 6/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.6361 - val_loss: 0.6143\n",
      "Epoch 7/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.6134 - val_loss: 0.6234\n",
      "Epoch 8/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.6093 - val_loss: 0.6212\n",
      "Epoch 9/2000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.5967 - val_loss: 0.6023\n",
      "Epoch 10/2000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.5809 - val_loss: 0.5787\n",
      "Epoch 11/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.5691 - val_loss: 0.5621\n",
      "Epoch 12/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.5646 - val_loss: 0.5503\n",
      "Epoch 13/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.5584 - val_loss: 0.5395\n",
      "Epoch 14/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5421 - val_loss: 0.5313\n",
      "Epoch 15/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.5274 - val_loss: 0.5253\n",
      "Epoch 16/2000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.5199 - val_loss: 0.5174\n",
      "Epoch 17/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.5140 - val_loss: 0.5061\n",
      "Epoch 18/2000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.5050 - val_loss: 0.4948\n",
      "Epoch 19/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.4977 - val_loss: 0.4869\n",
      "Epoch 20/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4896 - val_loss: 0.4814\n",
      "Epoch 21/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.4852 - val_loss: 0.4733\n",
      "Epoch 22/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.4780 - val_loss: 0.4641\n",
      "Epoch 23/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.4649 - val_loss: 0.4575\n",
      "Epoch 24/2000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.4576 - val_loss: 0.4534\n",
      "Epoch 25/2000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.4512 - val_loss: 0.4490\n",
      "Epoch 26/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.4443 - val_loss: 0.4423\n",
      "Epoch 27/2000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4372 - val_loss: 0.4338\n",
      "Epoch 28/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.4305 - val_loss: 0.4252\n",
      "Epoch 29/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.4250 - val_loss: 0.4172\n",
      "Epoch 30/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.4180 - val_loss: 0.4101\n",
      "Epoch 31/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.4107 - val_loss: 0.4039\n",
      "Epoch 32/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.4039 - val_loss: 0.3986\n",
      "Epoch 33/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3982 - val_loss: 0.3934\n",
      "Epoch 34/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3920 - val_loss: 0.3884\n",
      "Epoch 35/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.3876 - val_loss: 0.3832\n",
      "Epoch 36/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3816 - val_loss: 0.3774\n",
      "Epoch 37/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3754 - val_loss: 0.3715\n",
      "Epoch 38/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.3686 - val_loss: 0.3658\n",
      "Epoch 39/2000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.3647 - val_loss: 0.3604\n",
      "Epoch 40/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.3587 - val_loss: 0.3553\n",
      "Epoch 41/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3535 - val_loss: 0.3503\n",
      "Epoch 42/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.3502 - val_loss: 0.3453\n",
      "Epoch 43/2000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.3440 - val_loss: 0.3405\n",
      "Epoch 44/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3387 - val_loss: 0.3359\n",
      "Epoch 45/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3343 - val_loss: 0.3313\n",
      "Epoch 46/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.3296 - val_loss: 0.3266\n",
      "Epoch 47/2000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.3247 - val_loss: 0.3219\n",
      "Epoch 48/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.3218 - val_loss: 0.3174\n",
      "Epoch 49/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3166 - val_loss: 0.3131\n",
      "Epoch 50/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.3116 - val_loss: 0.3090\n",
      "Epoch 51/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.3082 - val_loss: 0.3053\n",
      "Epoch 52/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3037 - val_loss: 0.3018\n",
      "Epoch 53/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.3002 - val_loss: 0.2979\n",
      "Epoch 54/2000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.2960 - val_loss: 0.2943\n",
      "Epoch 55/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2924 - val_loss: 0.2903\n",
      "Epoch 56/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2884 - val_loss: 0.2862\n",
      "Epoch 57/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2843 - val_loss: 0.2824\n",
      "Epoch 58/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2812 - val_loss: 0.2790\n",
      "Epoch 59/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2776 - val_loss: 0.2760\n",
      "Epoch 60/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2734 - val_loss: 0.2731\n",
      "Epoch 61/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2711 - val_loss: 0.2696\n",
      "Epoch 62/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2686 - val_loss: 0.2656\n",
      "Epoch 63/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2643 - val_loss: 0.2620\n",
      "Epoch 64/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2608 - val_loss: 0.2589\n",
      "Epoch 65/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.2578 - val_loss: 0.2567\n",
      "Epoch 66/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2548 - val_loss: 0.2549\n",
      "Epoch 67/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2525 - val_loss: 0.2523\n",
      "Epoch 68/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2491 - val_loss: 0.2480\n",
      "Epoch 69/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2453 - val_loss: 0.2441\n",
      "Epoch 70/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2434 - val_loss: 0.2411\n",
      "Epoch 71/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2404 - val_loss: 0.2385\n",
      "Epoch 72/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2374 - val_loss: 0.2372\n",
      "Epoch 73/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2347 - val_loss: 0.2362\n",
      "Epoch 74/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2332 - val_loss: 0.2325\n",
      "Epoch 75/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2298 - val_loss: 0.2284\n",
      "Epoch 76/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2282 - val_loss: 0.2255\n",
      "Epoch 77/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2255 - val_loss: 0.2232\n",
      "Epoch 78/2000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.2231 - val_loss: 0.2218\n",
      "Epoch 79/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2204 - val_loss: 0.2203\n",
      "Epoch 80/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2175 - val_loss: 0.2178\n",
      "Epoch 81/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2157 - val_loss: 0.2148\n",
      "Epoch 82/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2132 - val_loss: 0.2121\n",
      "Epoch 83/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2115 - val_loss: 0.2100\n",
      "Epoch 84/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2088 - val_loss: 0.2086\n",
      "Epoch 85/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2066 - val_loss: 0.2069\n",
      "Epoch 86/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2050 - val_loss: 0.2044\n",
      "Epoch 87/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2024 - val_loss: 0.2017\n",
      "Epoch 88/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2002 - val_loss: 0.1993\n",
      "Epoch 89/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1985 - val_loss: 0.1974\n",
      "Epoch 90/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1970 - val_loss: 0.1961\n",
      "Epoch 91/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1942 - val_loss: 0.1946\n",
      "Epoch 92/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1927 - val_loss: 0.1927\n",
      "Epoch 93/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1910 - val_loss: 0.1904\n",
      "Epoch 94/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1888 - val_loss: 0.1883\n",
      "Epoch 95/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1879 - val_loss: 0.1865\n",
      "Epoch 96/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1859 - val_loss: 0.1854\n",
      "Epoch 97/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1834 - val_loss: 0.1842\n",
      "Epoch 98/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1824 - val_loss: 0.1819\n",
      "Epoch 99/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1807 - val_loss: 0.1797\n",
      "Epoch 100/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1786 - val_loss: 0.1780\n",
      "Epoch 101/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1771 - val_loss: 0.1764\n",
      "Epoch 102/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1756 - val_loss: 0.1755\n",
      "Epoch 103/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1737 - val_loss: 0.1742\n",
      "Epoch 104/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1728 - val_loss: 0.1719\n",
      "Epoch 105/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1707 - val_loss: 0.1700\n",
      "Epoch 106/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1692 - val_loss: 0.1689\n",
      "Epoch 107/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1680 - val_loss: 0.1682\n",
      "Epoch 108/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1663 - val_loss: 0.1668\n",
      "Epoch 109/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1651 - val_loss: 0.1649\n",
      "Epoch 110/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1635 - val_loss: 0.1631\n",
      "Epoch 111/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1617 - val_loss: 0.1618\n",
      "Epoch 112/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1605 - val_loss: 0.1607\n",
      "Epoch 113/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1594 - val_loss: 0.1598\n",
      "Epoch 114/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1581 - val_loss: 0.1583\n",
      "Epoch 115/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1575 - val_loss: 0.1564\n",
      "Epoch 116/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1566 - val_loss: 0.1550\n",
      "Epoch 117/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1549 - val_loss: 0.1540\n",
      "Epoch 118/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1528 - val_loss: 0.1534\n",
      "Epoch 119/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1522 - val_loss: 0.1522\n",
      "Epoch 120/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1505 - val_loss: 0.1500\n",
      "Epoch 121/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1489 - val_loss: 0.1486\n",
      "Epoch 122/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1479 - val_loss: 0.1474\n",
      "Epoch 123/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1467 - val_loss: 0.1468\n",
      "Epoch 124/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1454 - val_loss: 0.1465\n",
      "Epoch 125/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1445 - val_loss: 0.1451\n",
      "Epoch 126/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1433 - val_loss: 0.1431\n",
      "Epoch 127/2000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.1421 - val_loss: 0.1417\n",
      "Epoch 128/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1413 - val_loss: 0.1407\n",
      "Epoch 129/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1402 - val_loss: 0.1406\n",
      "Epoch 130/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1392 - val_loss: 0.1403\n",
      "Epoch 131/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1382 - val_loss: 0.1383\n",
      "Epoch 132/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1365 - val_loss: 0.1369\n",
      "Epoch 133/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1359 - val_loss: 0.1357\n",
      "Epoch 134/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1346 - val_loss: 0.1350\n",
      "Epoch 135/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1338 - val_loss: 0.1344\n",
      "Epoch 136/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1328 - val_loss: 0.1330\n",
      "Epoch 137/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1318 - val_loss: 0.1315\n",
      "Epoch 138/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1307 - val_loss: 0.1303\n",
      "Epoch 139/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1299 - val_loss: 0.1296\n",
      "Epoch 140/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1288 - val_loss: 0.1296\n",
      "Epoch 141/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1285 - val_loss: 0.1285\n",
      "Epoch 142/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1270 - val_loss: 0.1266\n",
      "Epoch 143/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1262 - val_loss: 0.1256\n",
      "Epoch 144/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1250 - val_loss: 0.1250\n",
      "Epoch 145/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1245 - val_loss: 0.1241\n",
      "Epoch 146/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1230 - val_loss: 0.1234\n",
      "Epoch 147/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1221 - val_loss: 0.1226\n",
      "Epoch 148/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1211 - val_loss: 0.1218\n",
      "Epoch 149/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1206 - val_loss: 0.1212\n",
      "Epoch 150/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1199 - val_loss: 0.1202\n",
      "Epoch 151/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1186 - val_loss: 0.1192\n",
      "Epoch 152/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1182 - val_loss: 0.1181\n",
      "Epoch 153/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1171 - val_loss: 0.1172\n",
      "Epoch 154/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1165 - val_loss: 0.1167\n",
      "Epoch 155/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1156 - val_loss: 0.1157\n",
      "Epoch 156/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1143 - val_loss: 0.1146\n",
      "Epoch 157/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1136 - val_loss: 0.1138\n",
      "Epoch 158/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1133 - val_loss: 0.1130\n",
      "Epoch 159/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1122 - val_loss: 0.1123\n",
      "Epoch 160/2000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.1116 - val_loss: 0.1115\n",
      "Epoch 161/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1108 - val_loss: 0.1108\n",
      "Epoch 162/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1100 - val_loss: 0.1101\n",
      "Epoch 163/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1090 - val_loss: 0.1095\n",
      "Epoch 164/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1083 - val_loss: 0.1088\n",
      "Epoch 165/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1076 - val_loss: 0.1082\n",
      "Epoch 166/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1066 - val_loss: 0.1073\n",
      "Epoch 167/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1069 - val_loss: 0.1063\n",
      "Epoch 168/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1053 - val_loss: 0.1056\n",
      "Epoch 169/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1052 - val_loss: 0.1053\n",
      "Epoch 170/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1041 - val_loss: 0.1057\n",
      "Epoch 171/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1035 - val_loss: 0.1042\n",
      "Epoch 172/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1027 - val_loss: 0.1030\n",
      "Epoch 173/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1025 - val_loss: 0.1024\n",
      "Epoch 174/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1011 - val_loss: 0.1022\n",
      "Epoch 175/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1008 - val_loss: 0.1014\n",
      "Epoch 176/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1003 - val_loss: 0.1001\n",
      "Epoch 177/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0993 - val_loss: 0.0994\n",
      "Epoch 178/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0987 - val_loss: 0.0995\n",
      "Epoch 179/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0982 - val_loss: 0.0987\n",
      "Epoch 180/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0976 - val_loss: 0.0976\n",
      "Epoch 181/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0968 - val_loss: 0.0968\n",
      "Epoch 182/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0958 - val_loss: 0.0962\n",
      "Epoch 183/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0957 - val_loss: 0.0957\n",
      "Epoch 184/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0950 - val_loss: 0.0954\n",
      "Epoch 185/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0939 - val_loss: 0.0950\n",
      "Epoch 186/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0938 - val_loss: 0.0939\n",
      "Epoch 187/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0933 - val_loss: 0.0932\n",
      "Epoch 188/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0924 - val_loss: 0.0928\n",
      "Epoch 189/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0915 - val_loss: 0.0925\n",
      "Epoch 190/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0914 - val_loss: 0.0917\n",
      "Epoch 191/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0911 - val_loss: 0.0908\n",
      "Epoch 192/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0903 - val_loss: 0.0902\n",
      "Epoch 193/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0896 - val_loss: 0.0898\n",
      "Epoch 194/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0892 - val_loss: 0.0893\n",
      "Epoch 195/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0886 - val_loss: 0.0895\n",
      "Epoch 196/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0879 - val_loss: 0.0884\n",
      "Epoch 197/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0874 - val_loss: 0.0875\n",
      "Epoch 198/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0868 - val_loss: 0.0870\n",
      "Epoch 199/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0865 - val_loss: 0.0867\n",
      "Epoch 200/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0854 - val_loss: 0.0863\n",
      "Epoch 201/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0849 - val_loss: 0.0858\n",
      "Epoch 202/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0846 - val_loss: 0.0850\n",
      "Epoch 203/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0839 - val_loss: 0.0845\n",
      "Epoch 204/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0838 - val_loss: 0.0839\n",
      "Epoch 205/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0831 - val_loss: 0.0836\n",
      "Epoch 206/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0823 - val_loss: 0.0832\n",
      "Epoch 207/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0819 - val_loss: 0.0823\n",
      "Epoch 208/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0812 - val_loss: 0.0817\n",
      "Epoch 209/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0809 - val_loss: 0.0813\n",
      "Epoch 210/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0804 - val_loss: 0.0810\n",
      "Epoch 211/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0800 - val_loss: 0.0804\n",
      "Epoch 212/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0791 - val_loss: 0.0796\n",
      "Epoch 213/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0793 - val_loss: 0.0792\n",
      "Epoch 214/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0782 - val_loss: 0.0789\n",
      "Epoch 215/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0783 - val_loss: 0.0784\n",
      "Epoch 216/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0777 - val_loss: 0.0781\n",
      "Epoch 217/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0769 - val_loss: 0.0779\n",
      "Epoch 218/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0765 - val_loss: 0.0772\n",
      "Epoch 219/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0760 - val_loss: 0.0767\n",
      "Epoch 220/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0754 - val_loss: 0.0763\n",
      "Epoch 221/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0750 - val_loss: 0.0758\n",
      "Epoch 222/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0745 - val_loss: 0.0752\n",
      "Epoch 223/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0742 - val_loss: 0.0745\n",
      "Epoch 224/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0742 - val_loss: 0.0742\n",
      "Epoch 225/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0731 - val_loss: 0.0746\n",
      "Epoch 226/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0731 - val_loss: 0.0738\n",
      "Epoch 227/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0732 - val_loss: 0.0728\n",
      "Epoch 228/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0719 - val_loss: 0.0724\n",
      "Epoch 229/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0719 - val_loss: 0.0723\n",
      "Epoch 230/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0712 - val_loss: 0.0723\n",
      "Epoch 231/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0709 - val_loss: 0.0712\n",
      "Epoch 232/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0705 - val_loss: 0.0707\n",
      "Epoch 233/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0699 - val_loss: 0.0703\n",
      "Epoch 234/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0692 - val_loss: 0.0703\n",
      "Epoch 235/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0691 - val_loss: 0.0695\n",
      "Epoch 236/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0687 - val_loss: 0.0689\n",
      "Epoch 237/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0685 - val_loss: 0.0686\n",
      "Epoch 238/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0678 - val_loss: 0.0687\n",
      "Epoch 239/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0674 - val_loss: 0.0681\n",
      "Epoch 240/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0673 - val_loss: 0.0675\n",
      "Epoch 241/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0666 - val_loss: 0.0670\n",
      "Epoch 242/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0661 - val_loss: 0.0666\n",
      "Epoch 243/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0661 - val_loss: 0.0663\n",
      "Epoch 244/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0654 - val_loss: 0.0660\n",
      "Epoch 245/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0651 - val_loss: 0.0654\n",
      "Epoch 246/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0648 - val_loss: 0.0649\n",
      "Epoch 247/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0644 - val_loss: 0.0646\n",
      "Epoch 248/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0641 - val_loss: 0.0645\n",
      "Epoch 249/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0640 - val_loss: 0.0647\n",
      "Epoch 250/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0635 - val_loss: 0.0642\n",
      "Epoch 251/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0630 - val_loss: 0.0632\n",
      "Epoch 252/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0629 - val_loss: 0.0629\n",
      "Epoch 253/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0625 - val_loss: 0.0628\n",
      "Epoch 254/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0626 - val_loss: 0.0625\n",
      "Epoch 255/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0617 - val_loss: 0.0619\n",
      "Epoch 256/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0614 - val_loss: 0.0618\n",
      "Epoch 257/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0617 - val_loss: 0.0615\n",
      "Epoch 258/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0603 - val_loss: 0.0620\n",
      "Epoch 259/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0605 - val_loss: 0.0606\n",
      "Epoch 260/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0593 - val_loss: 0.0604\n",
      "Epoch 261/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0595 - val_loss: 0.0600\n",
      "Epoch 262/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0597 - val_loss: 0.0598\n",
      "Epoch 263/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0587 - val_loss: 0.0594\n",
      "Epoch 264/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0585 - val_loss: 0.0591\n",
      "Epoch 265/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0581 - val_loss: 0.0586\n",
      "Epoch 266/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0582 - val_loss: 0.0581\n",
      "Epoch 267/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0576 - val_loss: 0.0577\n",
      "Epoch 268/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0573 - val_loss: 0.0574\n",
      "Epoch 269/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0571 - val_loss: 0.0573\n",
      "Epoch 270/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0563 - val_loss: 0.0571\n",
      "Epoch 271/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0565 - val_loss: 0.0566\n",
      "Epoch 272/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0560 - val_loss: 0.0563\n",
      "Epoch 273/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0554 - val_loss: 0.0561\n",
      "Epoch 274/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0551 - val_loss: 0.0562\n",
      "Epoch 275/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0550 - val_loss: 0.0556\n",
      "Epoch 276/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0547 - val_loss: 0.0552\n",
      "Epoch 277/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0543 - val_loss: 0.0549\n",
      "Epoch 278/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0540 - val_loss: 0.0552\n",
      "Epoch 279/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0541 - val_loss: 0.0546\n",
      "Epoch 280/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0533 - val_loss: 0.0541\n",
      "Epoch 281/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0534 - val_loss: 0.0537\n",
      "Epoch 282/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0528 - val_loss: 0.0537\n",
      "Epoch 283/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0525 - val_loss: 0.0534\n",
      "Epoch 284/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0522 - val_loss: 0.0528\n",
      "Epoch 285/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0521 - val_loss: 0.0524\n",
      "Epoch 286/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0518 - val_loss: 0.0522\n",
      "Epoch 287/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0515 - val_loss: 0.0523\n",
      "Epoch 288/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0513 - val_loss: 0.0516\n",
      "Epoch 289/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0511 - val_loss: 0.0513\n",
      "Epoch 290/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0509 - val_loss: 0.0510\n",
      "Epoch 291/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0502 - val_loss: 0.0508\n",
      "Epoch 292/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0503 - val_loss: 0.0505\n",
      "Epoch 293/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0496 - val_loss: 0.0504\n",
      "Epoch 294/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0495 - val_loss: 0.0502\n",
      "Epoch 295/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0493 - val_loss: 0.0498\n",
      "Epoch 296/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0489 - val_loss: 0.0495\n",
      "Epoch 297/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0485 - val_loss: 0.0493\n",
      "Epoch 298/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0483 - val_loss: 0.0491\n",
      "Epoch 299/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0483 - val_loss: 0.0487\n",
      "Epoch 300/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0481 - val_loss: 0.0486\n",
      "Epoch 301/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0479 - val_loss: 0.0482\n",
      "Epoch 302/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0473 - val_loss: 0.0485\n",
      "Epoch 303/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0471 - val_loss: 0.0478\n",
      "Epoch 304/2000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0468 - val_loss: 0.0473\n",
      "Epoch 305/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0469 - val_loss: 0.0471\n",
      "Epoch 306/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0461 - val_loss: 0.0467\n",
      "Epoch 307/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0461 - val_loss: 0.0465\n",
      "Epoch 308/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0458 - val_loss: 0.0463\n",
      "Epoch 309/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0453 - val_loss: 0.0459\n",
      "Epoch 310/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0454 - val_loss: 0.0457\n",
      "Epoch 311/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0457 - val_loss: 0.0457\n",
      "Epoch 312/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0450 - val_loss: 0.0456\n",
      "Epoch 313/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0448 - val_loss: 0.0451\n",
      "Epoch 314/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0446 - val_loss: 0.0449\n",
      "Epoch 315/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0445 - val_loss: 0.0448\n",
      "Epoch 316/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0439 - val_loss: 0.0447\n",
      "Epoch 317/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0438 - val_loss: 0.0443\n",
      "Epoch 318/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0435 - val_loss: 0.0441\n",
      "Epoch 319/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0434 - val_loss: 0.0438\n",
      "Epoch 320/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0431 - val_loss: 0.0437\n",
      "Epoch 321/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0429 - val_loss: 0.0434\n",
      "Epoch 322/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0427 - val_loss: 0.0431\n",
      "Epoch 323/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0429 - val_loss: 0.0428\n",
      "Epoch 324/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0423 - val_loss: 0.0425\n",
      "Epoch 325/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0427 - val_loss: 0.0425\n",
      "Epoch 326/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0419 - val_loss: 0.0423\n",
      "Epoch 327/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0414 - val_loss: 0.0419\n",
      "Epoch 328/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0413 - val_loss: 0.0417\n",
      "Epoch 329/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0409 - val_loss: 0.0416\n",
      "Epoch 330/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0412 - val_loss: 0.0415\n",
      "Epoch 331/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0408 - val_loss: 0.0412\n",
      "Epoch 332/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0408 - val_loss: 0.0410\n",
      "Epoch 333/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0404 - val_loss: 0.0409\n",
      "Epoch 334/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0405 - val_loss: 0.0417\n",
      "Epoch 335/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0409 - val_loss: 0.0406\n",
      "Epoch 336/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0398 - val_loss: 0.0402\n",
      "Epoch 337/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0396 - val_loss: 0.0400\n",
      "Epoch 338/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0395 - val_loss: 0.0403\n",
      "Epoch 339/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0395 - val_loss: 0.0398\n",
      "Epoch 340/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0392 - val_loss: 0.0396\n",
      "Epoch 341/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0387 - val_loss: 0.0393\n",
      "Epoch 342/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0387 - val_loss: 0.0400\n",
      "Epoch 343/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0385 - val_loss: 0.0396\n",
      "Epoch 344/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0383 - val_loss: 0.0386\n",
      "Epoch 345/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0382 - val_loss: 0.0386\n",
      "Epoch 346/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0380 - val_loss: 0.0382\n",
      "Epoch 347/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0377 - val_loss: 0.0383\n",
      "Epoch 348/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0378 - val_loss: 0.0378\n",
      "Epoch 349/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0373 - val_loss: 0.0379\n",
      "Epoch 350/2000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0371 - val_loss: 0.0374\n",
      "Epoch 351/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0368 - val_loss: 0.0375\n",
      "Epoch 352/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0367 - val_loss: 0.0371\n",
      "Epoch 353/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0362 - val_loss: 0.0369\n",
      "Epoch 354/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0362 - val_loss: 0.0367\n",
      "Epoch 355/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0360 - val_loss: 0.0367\n",
      "Epoch 356/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0359 - val_loss: 0.0363\n",
      "Epoch 357/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0357 - val_loss: 0.0361\n",
      "Epoch 358/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0353 - val_loss: 0.0360\n",
      "Epoch 359/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0353 - val_loss: 0.0359\n",
      "Epoch 360/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0350 - val_loss: 0.0356\n",
      "Epoch 361/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0349 - val_loss: 0.0354\n",
      "Epoch 362/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0350 - val_loss: 0.0352\n",
      "Epoch 363/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0352 - val_loss: 0.0355\n",
      "Epoch 364/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0345 - val_loss: 0.0356\n",
      "Epoch 365/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0346 - val_loss: 0.0348\n",
      "Epoch 366/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0342 - val_loss: 0.0347\n",
      "Epoch 367/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0344 - val_loss: 0.0345\n",
      "Epoch 368/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0338 - val_loss: 0.0346\n",
      "Epoch 369/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0340 - val_loss: 0.0341\n",
      "Epoch 370/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0336 - val_loss: 0.0342\n",
      "Epoch 371/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0341 - val_loss: 0.0342\n",
      "Epoch 372/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0335 - val_loss: 0.0341\n",
      "Epoch 373/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0333 - val_loss: 0.0335\n",
      "Epoch 374/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0328 - val_loss: 0.0335\n",
      "Epoch 375/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0328 - val_loss: 0.0332\n",
      "Epoch 376/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0323 - val_loss: 0.0341\n",
      "Epoch 377/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0333 - val_loss: 0.0330\n",
      "Epoch 378/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0324 - val_loss: 0.0335\n",
      "Epoch 379/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0333 - val_loss: 0.0326\n",
      "Epoch 380/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0322 - val_loss: 0.0337\n",
      "Epoch 381/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0323 - val_loss: 0.0330\n",
      "Epoch 382/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0319 - val_loss: 0.0324\n",
      "Epoch 383/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0317 - val_loss: 0.0322\n",
      "Epoch 384/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0313 - val_loss: 0.0324\n",
      "Epoch 385/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0320 - val_loss: 0.0320\n",
      "Epoch 386/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0312 - val_loss: 0.0317\n",
      "Epoch 387/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0309 - val_loss: 0.0314\n",
      "Epoch 388/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0307 - val_loss: 0.0316\n",
      "Epoch 389/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0307 - val_loss: 0.0314\n",
      "Epoch 390/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0303 - val_loss: 0.0309\n",
      "Epoch 391/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0303 - val_loss: 0.0307\n",
      "Epoch 392/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0303 - val_loss: 0.0307\n",
      "Epoch 393/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0301 - val_loss: 0.0306\n",
      "Epoch 394/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0306 - val_loss: 0.0303\n",
      "Epoch 395/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0297 - val_loss: 0.0303\n",
      "Epoch 396/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0299 - val_loss: 0.0301\n",
      "Epoch 397/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0294 - val_loss: 0.0303\n",
      "Epoch 398/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0294 - val_loss: 0.0299\n",
      "Epoch 399/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0290 - val_loss: 0.0301\n",
      "Epoch 400/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0293 - val_loss: 0.0297\n",
      "Epoch 401/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0288 - val_loss: 0.0302\n",
      "Epoch 402/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0288 - val_loss: 0.0299\n",
      "Epoch 403/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0294 - val_loss: 0.0293\n",
      "Epoch 404/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0286 - val_loss: 0.0292\n",
      "Epoch 405/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0289 - val_loss: 0.0292\n",
      "Epoch 406/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0283 - val_loss: 0.0292\n",
      "Epoch 407/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0283 - val_loss: 0.0287\n",
      "Epoch 408/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0278 - val_loss: 0.0286\n",
      "Epoch 409/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0280 - val_loss: 0.0285\n",
      "Epoch 410/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0276 - val_loss: 0.0288\n",
      "Epoch 411/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0279 - val_loss: 0.0284\n",
      "Epoch 412/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0274 - val_loss: 0.0281\n",
      "Epoch 413/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0277 - val_loss: 0.0280\n",
      "Epoch 414/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0272 - val_loss: 0.0281\n",
      "Epoch 415/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0271 - val_loss: 0.0282\n",
      "Epoch 416/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0271 - val_loss: 0.0277\n",
      "Epoch 417/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0269 - val_loss: 0.0275\n",
      "Epoch 418/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0266 - val_loss: 0.0274\n",
      "Epoch 419/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0265 - val_loss: 0.0272\n",
      "Epoch 420/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0266 - val_loss: 0.0272\n",
      "Epoch 421/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0263 - val_loss: 0.0271\n",
      "Epoch 422/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0265 - val_loss: 0.0269\n",
      "Epoch 423/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0260 - val_loss: 0.0271\n",
      "Epoch 424/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0261 - val_loss: 0.0266\n",
      "Epoch 425/2000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0256 - val_loss: 0.0265\n",
      "Epoch 426/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0259 - val_loss: 0.0264\n",
      "Epoch 427/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0255 - val_loss: 0.0263\n",
      "Epoch 428/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0257 - val_loss: 0.0265\n",
      "Epoch 429/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0256 - val_loss: 0.0260\n",
      "Epoch 430/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0258 - val_loss: 0.0262\n",
      "Epoch 431/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0257 - val_loss: 0.0258\n",
      "Epoch 432/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0253 - val_loss: 0.0262\n",
      "Epoch 433/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0252 - val_loss: 0.0258\n",
      "Epoch 434/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0253 - val_loss: 0.0258\n",
      "Epoch 435/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0249 - val_loss: 0.0255\n",
      "Epoch 436/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0246 - val_loss: 0.0257\n",
      "Epoch 437/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0249 - val_loss: 0.0254\n",
      "Epoch 438/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0254 - val_loss: 0.0252\n",
      "Epoch 439/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0246 - val_loss: 0.0251\n",
      "Epoch 440/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0251 - val_loss: 0.0252\n",
      "Epoch 441/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0247 - val_loss: 0.0252\n",
      "Epoch 442/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0242 - val_loss: 0.0246\n",
      "Epoch 443/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0243 - val_loss: 0.0247\n",
      "Epoch 444/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0239 - val_loss: 0.0244\n",
      "Epoch 445/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0236 - val_loss: 0.0244\n",
      "Epoch 446/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0234 - val_loss: 0.0242\n",
      "Epoch 447/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0234 - val_loss: 0.0240\n",
      "Epoch 448/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0233 - val_loss: 0.0240\n",
      "Epoch 449/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0237 - val_loss: 0.0240\n",
      "Epoch 450/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0232 - val_loss: 0.0239\n",
      "Epoch 451/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0234 - val_loss: 0.0236\n",
      "Epoch 452/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0231 - val_loss: 0.0236\n",
      "Epoch 453/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0231 - val_loss: 0.0237\n",
      "Epoch 454/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0233 - val_loss: 0.0235\n",
      "Epoch 455/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0225 - val_loss: 0.0233\n",
      "Epoch 456/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0225 - val_loss: 0.0232\n",
      "Epoch 457/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0225 - val_loss: 0.0233\n",
      "Epoch 458/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0223 - val_loss: 0.0237\n",
      "Epoch 459/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0229 - val_loss: 0.0229\n",
      "Epoch 460/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0221 - val_loss: 0.0230\n",
      "Epoch 461/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0223 - val_loss: 0.0227\n",
      "Epoch 462/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0219 - val_loss: 0.0227\n",
      "Epoch 463/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0221 - val_loss: 0.0226\n",
      "Epoch 464/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0218 - val_loss: 0.0225\n",
      "Epoch 465/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0217 - val_loss: 0.0224\n",
      "Epoch 466/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0216 - val_loss: 0.0223\n",
      "Epoch 467/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0214 - val_loss: 0.0221\n",
      "Epoch 468/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0215 - val_loss: 0.0220\n",
      "Epoch 469/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0214 - val_loss: 0.0219\n",
      "Epoch 470/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0210 - val_loss: 0.0218\n",
      "Epoch 471/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0211 - val_loss: 0.0216\n",
      "Epoch 472/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0213 - val_loss: 0.0216\n",
      "Epoch 473/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0209 - val_loss: 0.0215\n",
      "Epoch 474/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0207 - val_loss: 0.0216\n",
      "Epoch 475/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0208 - val_loss: 0.0214\n",
      "Epoch 476/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0209 - val_loss: 0.0214\n",
      "Epoch 477/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0206 - val_loss: 0.0213\n",
      "Epoch 478/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0204 - val_loss: 0.0212\n",
      "Epoch 479/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0206 - val_loss: 0.0211\n",
      "Epoch 480/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0205 - val_loss: 0.0210\n",
      "Epoch 481/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0203 - val_loss: 0.0209\n",
      "Epoch 482/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0202 - val_loss: 0.0207\n",
      "Epoch 483/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0200 - val_loss: 0.0210\n",
      "Epoch 484/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0200 - val_loss: 0.0207\n",
      "Epoch 485/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0201 - val_loss: 0.0205\n",
      "Epoch 486/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0200 - val_loss: 0.0204\n",
      "Epoch 487/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0197 - val_loss: 0.0206\n",
      "Epoch 488/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0196 - val_loss: 0.0203\n",
      "Epoch 489/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0198 - val_loss: 0.0202\n",
      "Epoch 490/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0197 - val_loss: 0.0201\n",
      "Epoch 491/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0193 - val_loss: 0.0202\n",
      "Epoch 492/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0194 - val_loss: 0.0200\n",
      "Epoch 493/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0196 - val_loss: 0.0203\n",
      "Epoch 494/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0198 - val_loss: 0.0201\n",
      "Epoch 495/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0193 - val_loss: 0.0202\n",
      "Epoch 496/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0192 - val_loss: 0.0199\n",
      "Epoch 497/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0191 - val_loss: 0.0197\n",
      "Epoch 498/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0188 - val_loss: 0.0197\n",
      "Epoch 499/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0192 - val_loss: 0.0195\n",
      "Epoch 500/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0191 - val_loss: 0.0194\n",
      "Epoch 501/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0186 - val_loss: 0.0192\n",
      "Epoch 502/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0185 - val_loss: 0.0192\n",
      "Epoch 503/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0186 - val_loss: 0.0191\n",
      "Epoch 504/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0187 - val_loss: 0.0190\n",
      "Epoch 505/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0184 - val_loss: 0.0191\n",
      "Epoch 506/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0183 - val_loss: 0.0189\n",
      "Epoch 507/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0182 - val_loss: 0.0189\n",
      "Epoch 508/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0180 - val_loss: 0.0188\n",
      "Epoch 509/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0180 - val_loss: 0.0187\n",
      "Epoch 510/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0179 - val_loss: 0.0187\n",
      "Epoch 511/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0179 - val_loss: 0.0186\n",
      "Epoch 512/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0181 - val_loss: 0.0186\n",
      "Epoch 513/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0179 - val_loss: 0.0186\n",
      "Epoch 514/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0178 - val_loss: 0.0186\n",
      "Epoch 515/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0178 - val_loss: 0.0183\n",
      "Epoch 516/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0179 - val_loss: 0.0184\n",
      "Epoch 517/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0177 - val_loss: 0.0182\n",
      "Epoch 518/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0173 - val_loss: 0.0182\n",
      "Epoch 519/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0175 - val_loss: 0.0182\n",
      "Epoch 520/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0174 - val_loss: 0.0180\n",
      "Epoch 521/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0172 - val_loss: 0.0181\n",
      "Epoch 522/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0177 - val_loss: 0.0178\n",
      "Epoch 523/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0173 - val_loss: 0.0180\n",
      "Epoch 524/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0175 - val_loss: 0.0176\n",
      "Epoch 525/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0170 - val_loss: 0.0179\n",
      "Epoch 526/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0171 - val_loss: 0.0176\n",
      "Epoch 527/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0170 - val_loss: 0.0177\n",
      "Epoch 528/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0170 - val_loss: 0.0175\n",
      "Epoch 529/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0168 - val_loss: 0.0178\n",
      "Epoch 530/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0171 - val_loss: 0.0176\n",
      "Epoch 531/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0168 - val_loss: 0.0177\n",
      "Epoch 532/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0168 - val_loss: 0.0173\n",
      "Epoch 533/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0165 - val_loss: 0.0173\n",
      "Epoch 534/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0164 - val_loss: 0.0172\n",
      "Epoch 535/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0166 - val_loss: 0.0170\n",
      "Epoch 536/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0163 - val_loss: 0.0170\n",
      "Epoch 537/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0164 - val_loss: 0.0170\n",
      "Epoch 538/2000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0160 - val_loss: 0.0169\n",
      "Epoch 539/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0161 - val_loss: 0.0169\n",
      "Epoch 540/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0163 - val_loss: 0.0167\n",
      "Epoch 541/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0160 - val_loss: 0.0166\n",
      "Epoch 542/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0159 - val_loss: 0.0165\n",
      "Epoch 543/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0162 - val_loss: 0.0167\n",
      "Epoch 544/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0162 - val_loss: 0.0164\n",
      "Epoch 545/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0159 - val_loss: 0.0164\n",
      "Epoch 546/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0157 - val_loss: 0.0164\n",
      "Epoch 547/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0156 - val_loss: 0.0163\n",
      "Epoch 548/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0157 - val_loss: 0.0162\n",
      "Epoch 549/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0156 - val_loss: 0.0161\n",
      "Epoch 550/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0154 - val_loss: 0.0161\n",
      "Epoch 551/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0155 - val_loss: 0.0160\n",
      "Epoch 552/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0153 - val_loss: 0.0160\n",
      "Epoch 553/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0155 - val_loss: 0.0159\n",
      "Epoch 554/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0155 - val_loss: 0.0159\n",
      "Epoch 555/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0153 - val_loss: 0.0159\n",
      "Epoch 556/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0150 - val_loss: 0.0159\n",
      "Epoch 557/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0150 - val_loss: 0.0157\n",
      "Epoch 558/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0149 - val_loss: 0.0157\n",
      "Epoch 559/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0151 - val_loss: 0.0155\n",
      "Epoch 560/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0149 - val_loss: 0.0155\n",
      "Epoch 561/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0148 - val_loss: 0.0156\n",
      "Epoch 562/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0148 - val_loss: 0.0155\n",
      "Epoch 563/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0152 - val_loss: 0.0155\n",
      "Epoch 564/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0149 - val_loss: 0.0155\n",
      "Epoch 565/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0145 - val_loss: 0.0153\n",
      "Epoch 566/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0146 - val_loss: 0.0152\n",
      "Epoch 567/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0146 - val_loss: 0.0151\n",
      "Epoch 568/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0143 - val_loss: 0.0151\n",
      "Epoch 569/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0146 - val_loss: 0.0150\n",
      "Epoch 570/2000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0145 - val_loss: 0.0150\n",
      "Epoch 571/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0145 - val_loss: 0.0151\n",
      "Epoch 572/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0143 - val_loss: 0.0151\n",
      "Epoch 573/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0143 - val_loss: 0.0149\n",
      "Epoch 574/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0140 - val_loss: 0.0149\n",
      "Epoch 575/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0139 - val_loss: 0.0148\n",
      "Epoch 576/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0138 - val_loss: 0.0148\n",
      "Epoch 577/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0142 - val_loss: 0.0149\n",
      "Epoch 578/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0140 - val_loss: 0.0148\n",
      "Epoch 579/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0140 - val_loss: 0.0148\n",
      "Epoch 580/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0139 - val_loss: 0.0149\n",
      "Epoch 581/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0137 - val_loss: 0.0145\n",
      "Epoch 582/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0137 - val_loss: 0.0144\n",
      "Epoch 583/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0138 - val_loss: 0.0143\n",
      "Epoch 584/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0136 - val_loss: 0.0144\n",
      "Epoch 585/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0136 - val_loss: 0.0142\n",
      "Epoch 586/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0140 - val_loss: 0.0144\n",
      "Epoch 587/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0138 - val_loss: 0.0141\n",
      "Epoch 588/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0134 - val_loss: 0.0145\n",
      "Epoch 589/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0134 - val_loss: 0.0142\n",
      "Epoch 590/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0133 - val_loss: 0.0141\n",
      "Epoch 591/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0132 - val_loss: 0.0141\n",
      "Epoch 592/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0132 - val_loss: 0.0139\n",
      "Epoch 593/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0133 - val_loss: 0.0141\n",
      "Epoch 594/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0133 - val_loss: 0.0139\n",
      "Epoch 595/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0132 - val_loss: 0.0140\n",
      "Epoch 596/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0130 - val_loss: 0.0138\n",
      "Epoch 597/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0131 - val_loss: 0.0140\n",
      "Epoch 598/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0131 - val_loss: 0.0137\n",
      "Epoch 599/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0130 - val_loss: 0.0139\n",
      "Epoch 600/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0133 - val_loss: 0.0135\n",
      "Epoch 601/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0128 - val_loss: 0.0139\n",
      "Epoch 602/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0131 - val_loss: 0.0134\n",
      "Epoch 603/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0131 - val_loss: 0.0138\n",
      "Epoch 604/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0133 - val_loss: 0.0135\n",
      "Epoch 605/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0128 - val_loss: 0.0139\n",
      "Epoch 606/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0132 - val_loss: 0.0136\n",
      "Epoch 607/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0126 - val_loss: 0.0135\n",
      "Epoch 608/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0125 - val_loss: 0.0137\n",
      "Epoch 609/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0127 - val_loss: 0.0133\n",
      "Epoch 610/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0124 - val_loss: 0.0136\n",
      "Epoch 611/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0130 - val_loss: 0.0132\n",
      "Epoch 612/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0125 - val_loss: 0.0142\n",
      "Epoch 613/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0134 - val_loss: 0.0131\n",
      "Epoch 614/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0125 - val_loss: 0.0141\n",
      "Epoch 615/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0134 - val_loss: 0.0132\n",
      "Epoch 616/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0122 - val_loss: 0.0138\n",
      "Epoch 617/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0129 - val_loss: 0.0131\n",
      "Epoch 618/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0124 - val_loss: 0.0140\n",
      "Epoch 619/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0128 - val_loss: 0.0135\n",
      "Epoch 620/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0130 - val_loss: 0.0134\n",
      "Epoch 621/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0124 - val_loss: 0.0139\n",
      "Epoch 622/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0133 - val_loss: 0.0129\n",
      "Epoch 623/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0121 - val_loss: 0.0144\n",
      "Epoch 624/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0139 - val_loss: 0.0130\n",
      "Epoch 625/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0118 - val_loss: 0.0144\n",
      "Epoch 626/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0133 - val_loss: 0.0129\n",
      "Epoch 627/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0118 - val_loss: 0.0143\n",
      "Epoch 628/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0130 - val_loss: 0.0131\n",
      "Epoch 629/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0124 - val_loss: 0.0137\n",
      "Epoch 630/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0125 - val_loss: 0.0136\n",
      "Epoch 631/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0123 - val_loss: 0.0127\n",
      "Epoch 632/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0116 - val_loss: 0.0141\n",
      "Epoch 633/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0133 - val_loss: 0.0126\n",
      "Epoch 634/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0114 - val_loss: 0.0136\n",
      "Epoch 635/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0121 - val_loss: 0.0132\n",
      "Epoch 636/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0121 - val_loss: 0.0129\n",
      "Epoch 637/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0117 - val_loss: 0.0135\n",
      "Epoch 638/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0125 - val_loss: 0.0124\n",
      "Epoch 639/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0114 - val_loss: 0.0135\n",
      "Epoch 640/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0122 - val_loss: 0.0126\n",
      "Epoch 641/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0115 - val_loss: 0.0125\n",
      "Epoch 642/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0119 - val_loss: 0.0126\n",
      "Epoch 643/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0118 - val_loss: 0.0121\n",
      "Epoch 644/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0112 - val_loss: 0.0126\n",
      "Epoch 645/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0114 - val_loss: 0.0120\n",
      "Epoch 646/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0112 - val_loss: 0.0120\n",
      "Epoch 647/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0113 - val_loss: 0.0120\n",
      "Epoch 648/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0110 - val_loss: 0.0118\n",
      "Epoch 649/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0109 - val_loss: 0.0122\n",
      "Epoch 650/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0114 - val_loss: 0.0117\n",
      "Epoch 651/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0109 - val_loss: 0.0119\n",
      "Epoch 652/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0113 - val_loss: 0.0117\n",
      "Epoch 653/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0108 - val_loss: 0.0120\n",
      "Epoch 654/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0109 - val_loss: 0.0120\n",
      "Epoch 655/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0111 - val_loss: 0.0116\n",
      "Epoch 656/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0108 - val_loss: 0.0117\n",
      "Epoch 657/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0107 - val_loss: 0.0116\n",
      "Epoch 658/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0110 - val_loss: 0.0117\n",
      "Epoch 659/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0107 - val_loss: 0.0116\n",
      "Epoch 660/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0106 - val_loss: 0.0116\n",
      "Epoch 661/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0106 - val_loss: 0.0114\n",
      "Epoch 662/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0106 - val_loss: 0.0113\n",
      "Epoch 663/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0105 - val_loss: 0.0112\n",
      "Epoch 664/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0105 - val_loss: 0.0112\n",
      "Epoch 665/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0105 - val_loss: 0.0114\n",
      "Epoch 666/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0105 - val_loss: 0.0112\n",
      "Epoch 667/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0104 - val_loss: 0.0112\n",
      "Epoch 668/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0102 - val_loss: 0.0112\n",
      "Epoch 669/2000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0105 - val_loss: 0.0112\n",
      "Epoch 670/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0107 - val_loss: 0.0111\n",
      "Epoch 671/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0104 - val_loss: 0.0112\n",
      "Epoch 672/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0102 - val_loss: 0.0110\n",
      "Epoch 673/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0103 - val_loss: 0.0111\n",
      "Epoch 674/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0102 - val_loss: 0.0110\n",
      "Epoch 675/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0101 - val_loss: 0.0110\n",
      "Epoch 676/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0103 - val_loss: 0.0111\n",
      "Epoch 677/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0102 - val_loss: 0.0109\n",
      "Epoch 678/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0099 - val_loss: 0.0109\n",
      "Epoch 679/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0100 - val_loss: 0.0108\n",
      "Epoch 680/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0100 - val_loss: 0.0108\n",
      "Epoch 681/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0099 - val_loss: 0.0108\n",
      "Epoch 682/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0100 - val_loss: 0.0107\n",
      "Epoch 683/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0100 - val_loss: 0.0106\n",
      "Epoch 684/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0099 - val_loss: 0.0106\n",
      "Epoch 685/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0097 - val_loss: 0.0106\n",
      "Epoch 686/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0098 - val_loss: 0.0106\n",
      "Epoch 687/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0099 - val_loss: 0.0106\n",
      "Epoch 688/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0098 - val_loss: 0.0106\n",
      "Epoch 689/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0097 - val_loss: 0.0106\n",
      "Epoch 690/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0097 - val_loss: 0.0105\n",
      "Epoch 691/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0097 - val_loss: 0.0106\n",
      "Epoch 692/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0097 - val_loss: 0.0105\n",
      "Epoch 693/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0095 - val_loss: 0.0104\n",
      "Epoch 694/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0100 - val_loss: 0.0104\n",
      "Epoch 695/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0104\n",
      "Epoch 696/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0094 - val_loss: 0.0103\n",
      "Epoch 697/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0096 - val_loss: 0.0103\n",
      "Epoch 698/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0099 - val_loss: 0.0103\n",
      "Epoch 699/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0095 - val_loss: 0.0102\n",
      "Epoch 700/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0094 - val_loss: 0.0102\n",
      "Epoch 701/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0102\n",
      "Epoch 702/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0092 - val_loss: 0.0101\n",
      "Epoch 703/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0094 - val_loss: 0.0101\n",
      "Epoch 704/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0093 - val_loss: 0.0101\n",
      "Epoch 705/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0091 - val_loss: 0.0101\n",
      "Epoch 706/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0093 - val_loss: 0.0100\n",
      "Epoch 707/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0090 - val_loss: 0.0100\n",
      "Epoch 708/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0094 - val_loss: 0.0100\n",
      "Epoch 709/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0093 - val_loss: 0.0101\n",
      "Epoch 710/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0092 - val_loss: 0.0100\n",
      "Epoch 711/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0090 - val_loss: 0.0100\n",
      "Epoch 712/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0095 - val_loss: 0.0100\n",
      "Epoch 713/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0090 - val_loss: 0.0099\n",
      "Epoch 714/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0091 - val_loss: 0.0099\n",
      "Epoch 715/2000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 716/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0088 - val_loss: 0.0098\n",
      "Epoch 717/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 718/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 719/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 720/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 721/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 722/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0088 - val_loss: 0.0097\n",
      "Epoch 723/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0089 - val_loss: 0.0096\n",
      "Epoch 724/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0089 - val_loss: 0.0096\n",
      "Epoch 725/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0087 - val_loss: 0.0097\n",
      "Epoch 726/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0088 - val_loss: 0.0096\n",
      "Epoch 727/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0088 - val_loss: 0.0098\n",
      "Epoch 728/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0089 - val_loss: 0.0095\n",
      "Epoch 729/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0087 - val_loss: 0.0095\n",
      "Epoch 730/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0087 - val_loss: 0.0095\n",
      "Epoch 731/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0086 - val_loss: 0.0094\n",
      "Epoch 732/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0087 - val_loss: 0.0093\n",
      "Epoch 733/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0084 - val_loss: 0.0093\n",
      "Epoch 734/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0085 - val_loss: 0.0093\n",
      "Epoch 735/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0086 - val_loss: 0.0092\n",
      "Epoch 736/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0087 - val_loss: 0.0092\n",
      "Epoch 737/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0085 - val_loss: 0.0093\n",
      "Epoch 738/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0086 - val_loss: 0.0093\n",
      "Epoch 739/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0084 - val_loss: 0.0093\n",
      "Epoch 740/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0084 - val_loss: 0.0092\n",
      "Epoch 741/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0083 - val_loss: 0.0092\n",
      "Epoch 742/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0083 - val_loss: 0.0092\n",
      "Epoch 743/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0084 - val_loss: 0.0092\n",
      "Epoch 744/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0082 - val_loss: 0.0091\n",
      "Epoch 745/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0081 - val_loss: 0.0091\n",
      "Epoch 746/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0082 - val_loss: 0.0091\n",
      "Epoch 747/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0083 - val_loss: 0.0091\n",
      "Epoch 748/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0082 - val_loss: 0.0092\n",
      "Epoch 749/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0084 - val_loss: 0.0090\n",
      "Epoch 750/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0084 - val_loss: 0.0090\n",
      "Epoch 751/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0083 - val_loss: 0.0089\n",
      "Epoch 752/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0082 - val_loss: 0.0089\n",
      "Epoch 753/2000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0082 - val_loss: 0.0088\n",
      "Epoch 754/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0081 - val_loss: 0.0088\n",
      "Epoch 755/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0082 - val_loss: 0.0088\n",
      "Epoch 756/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0080 - val_loss: 0.0089\n",
      "Epoch 757/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0080 - val_loss: 0.0089\n",
      "Epoch 758/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0080 - val_loss: 0.0091\n",
      "Epoch 759/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0082 - val_loss: 0.0090\n",
      "Epoch 760/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0082 - val_loss: 0.0091\n",
      "Epoch 761/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0079 - val_loss: 0.0090\n",
      "Epoch 762/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0081 - val_loss: 0.0088\n",
      "Epoch 763/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0079 - val_loss: 0.0088\n",
      "Epoch 764/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0079 - val_loss: 0.0087\n",
      "Epoch 765/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0079 - val_loss: 0.0088\n",
      "Epoch 766/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0080 - val_loss: 0.0087\n",
      "Epoch 767/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0077 - val_loss: 0.0087\n",
      "Epoch 768/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0081 - val_loss: 0.0088\n",
      "Epoch 769/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0079 - val_loss: 0.0087\n",
      "Epoch 770/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0080 - val_loss: 0.0087\n",
      "Epoch 771/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0081 - val_loss: 0.0085\n",
      "Epoch 772/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0078 - val_loss: 0.0085\n",
      "Epoch 773/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0082 - val_loss: 0.0085\n",
      "Epoch 774/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0078 - val_loss: 0.0086\n",
      "Epoch 775/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0079 - val_loss: 0.0084\n",
      "Epoch 776/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0077 - val_loss: 0.0089\n",
      "Epoch 777/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0079 - val_loss: 0.0086\n",
      "Epoch 778/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0076 - val_loss: 0.0088\n",
      "Epoch 779/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0078 - val_loss: 0.0086\n",
      "Epoch 780/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0076 - val_loss: 0.0085\n",
      "Epoch 781/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0076 - val_loss: 0.0084\n",
      "Epoch 782/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0077 - val_loss: 0.0084\n",
      "Epoch 783/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0075 - val_loss: 0.0083\n",
      "Epoch 784/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0077 - val_loss: 0.0082\n",
      "Epoch 785/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0076 - val_loss: 0.0086\n",
      "Epoch 786/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0076 - val_loss: 0.0084\n",
      "Epoch 787/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0075 - val_loss: 0.0084\n",
      "Epoch 788/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0076 - val_loss: 0.0085\n",
      "Epoch 789/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0078 - val_loss: 0.0083\n",
      "Epoch 790/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0073 - val_loss: 0.0085\n",
      "Epoch 791/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0075 - val_loss: 0.0082\n",
      "Epoch 792/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0072 - val_loss: 0.0084\n",
      "Epoch 793/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0074 - val_loss: 0.0082\n",
      "Epoch 794/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0073 - val_loss: 0.0082\n",
      "Epoch 795/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0076 - val_loss: 0.0082\n",
      "Epoch 796/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0073 - val_loss: 0.0082\n",
      "Epoch 797/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0073 - val_loss: 0.0082\n",
      "Epoch 798/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0072 - val_loss: 0.0081\n",
      "Epoch 799/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0071 - val_loss: 0.0080\n",
      "Epoch 800/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0073 - val_loss: 0.0081\n",
      "Epoch 801/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0074 - val_loss: 0.0079\n",
      "Epoch 802/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0071 - val_loss: 0.0080\n",
      "Epoch 803/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0072 - val_loss: 0.0080\n",
      "Epoch 804/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0071 - val_loss: 0.0080\n",
      "Epoch 805/2000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0071 - val_loss: 0.0079\n",
      "Epoch 806/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0071 - val_loss: 0.0079\n",
      "Epoch 807/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0071 - val_loss: 0.0080\n",
      "Epoch 808/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0071 - val_loss: 0.0082\n",
      "Epoch 809/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0070 - val_loss: 0.0080\n",
      "Epoch 810/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0070 - val_loss: 0.0080\n",
      "Epoch 811/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0072 - val_loss: 0.0080\n",
      "Epoch 812/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0069 - val_loss: 0.0081\n",
      "Epoch 813/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0070 - val_loss: 0.0081\n",
      "Epoch 814/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0071 - val_loss: 0.0080\n",
      "Epoch 815/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0071 - val_loss: 0.0079\n",
      "Epoch 816/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0071 - val_loss: 0.0082\n",
      "Epoch 817/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0073 - val_loss: 0.0078\n",
      "Epoch 818/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0070 - val_loss: 0.0078\n",
      "Epoch 819/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0070 - val_loss: 0.0079\n",
      "Epoch 820/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0069 - val_loss: 0.0079\n",
      "Epoch 821/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0069 - val_loss: 0.0078\n",
      "Epoch 822/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0069 - val_loss: 0.0077\n",
      "Epoch 823/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0067 - val_loss: 0.0077\n",
      "Epoch 824/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0069 - val_loss: 0.0076\n",
      "Epoch 825/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0068 - val_loss: 0.0077\n",
      "Epoch 826/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0068 - val_loss: 0.0076\n",
      "Epoch 827/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0067 - val_loss: 0.0077\n",
      "Epoch 828/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0067 - val_loss: 0.0077\n",
      "Epoch 829/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0066 - val_loss: 0.0078\n",
      "Epoch 830/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0068 - val_loss: 0.0078\n",
      "Epoch 831/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0069 - val_loss: 0.0077\n",
      "Epoch 832/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0067 - val_loss: 0.0077\n",
      "Epoch 833/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0067 - val_loss: 0.0075\n",
      "Epoch 834/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0066 - val_loss: 0.0075\n",
      "Epoch 835/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0066 - val_loss: 0.0075\n",
      "Epoch 836/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0067 - val_loss: 0.0076\n",
      "Epoch 837/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0066 - val_loss: 0.0076\n",
      "Epoch 838/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0066 - val_loss: 0.0078\n",
      "Epoch 839/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0069 - val_loss: 0.0077\n",
      "Epoch 840/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0068 - val_loss: 0.0077\n",
      "Epoch 841/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0067 - val_loss: 0.0077\n",
      "Epoch 842/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0070 - val_loss: 0.0075\n",
      "Epoch 843/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0067 - val_loss: 0.0078\n",
      "Epoch 844/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0069 - val_loss: 0.0075\n",
      "Epoch 845/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0068 - val_loss: 0.0078\n",
      "Epoch 846/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0069 - val_loss: 0.0076\n",
      "Epoch 847/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0065 - val_loss: 0.0078\n",
      "Epoch 848/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0067 - val_loss: 0.0078\n",
      "Epoch 849/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0067 - val_loss: 0.0076\n",
      "Epoch 850/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0067 - val_loss: 0.0076\n",
      "Epoch 851/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0067 - val_loss: 0.0077\n",
      "Epoch 852/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0065 - val_loss: 0.0076\n",
      "Epoch 853/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0064 - val_loss: 0.0075\n",
      "Epoch 854/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0064 - val_loss: 0.0074\n",
      "Epoch 855/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 856/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0064 - val_loss: 0.0075\n",
      "Epoch 857/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0062 - val_loss: 0.0074\n",
      "Epoch 858/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0063 - val_loss: 0.0073\n",
      "Epoch 859/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0063 - val_loss: 0.0072\n",
      "Epoch 860/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0062 - val_loss: 0.0071\n",
      "Epoch 861/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0064 - val_loss: 0.0073\n",
      "Epoch 862/2000\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0064 - val_loss: 0.0071\n",
      "Epoch 863/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0063 - val_loss: 0.0072\n",
      "Epoch 864/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0064 - val_loss: 0.0073\n",
      "Epoch 865/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0062 - val_loss: 0.0075\n",
      "Epoch 866/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 867/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0064 - val_loss: 0.0074\n",
      "Epoch 868/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0061 - val_loss: 0.0071\n",
      "Epoch 869/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0065 - val_loss: 0.0071\n",
      "Epoch 870/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0064 - val_loss: 0.0069\n",
      "Epoch 871/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0061 - val_loss: 0.0069\n",
      "Epoch 872/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0061 - val_loss: 0.0068\n",
      "Epoch 873/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0062 - val_loss: 0.0071\n",
      "Epoch 874/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0065 - val_loss: 0.0070\n",
      "Epoch 875/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0060 - val_loss: 0.0074\n",
      "Epoch 876/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0067 - val_loss: 0.0070\n",
      "Epoch 877/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0060 - val_loss: 0.0080\n",
      "Epoch 878/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0072 - val_loss: 0.0069\n",
      "Epoch 879/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0061 - val_loss: 0.0076\n",
      "Epoch 880/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0071 - val_loss: 0.0071\n",
      "Epoch 881/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0061 - val_loss: 0.0081\n",
      "Epoch 882/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0069 - val_loss: 0.0071\n",
      "Epoch 883/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0061 - val_loss: 0.0075\n",
      "Epoch 884/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0068 - val_loss: 0.0071\n",
      "Epoch 885/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0061 - val_loss: 0.0078\n",
      "Epoch 886/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0067 - val_loss: 0.0070\n",
      "Epoch 887/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0059 - val_loss: 0.0072\n",
      "Epoch 888/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0066 - val_loss: 0.0069\n",
      "Epoch 889/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0060 - val_loss: 0.0073\n",
      "Epoch 890/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0064 - val_loss: 0.0069\n",
      "Epoch 891/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0060 - val_loss: 0.0072\n",
      "Epoch 892/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0063 - val_loss: 0.0069\n",
      "Epoch 893/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0058 - val_loss: 0.0071\n",
      "Epoch 894/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0061 - val_loss: 0.0069\n",
      "Epoch 895/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0060 - val_loss: 0.0069\n",
      "Epoch 896/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0061 - val_loss: 0.0068\n",
      "Epoch 897/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0059 - val_loss: 0.0071\n",
      "Epoch 898/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0060 - val_loss: 0.0072\n",
      "Epoch 899/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0061 - val_loss: 0.0069\n",
      "Epoch 900/2000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0059 - val_loss: 0.0074\n",
      "Epoch 901/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0063 - val_loss: 0.0068\n",
      "Epoch 902/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0059 - val_loss: 0.0078\n",
      "Epoch 903/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0067 - val_loss: 0.0067\n",
      "Epoch 904/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0058 - val_loss: 0.0071\n",
      "Epoch 905/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0062 - val_loss: 0.0068\n",
      "Epoch 906/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0058 - val_loss: 0.0069\n",
      "Epoch 907/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0061 - val_loss: 0.0068\n",
      "Epoch 908/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0058 - val_loss: 0.0068\n",
      "Epoch 909/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0058 - val_loss: 0.0070\n",
      "Epoch 910/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0060 - val_loss: 0.0067\n",
      "Epoch 911/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0057 - val_loss: 0.0070\n",
      "Epoch 912/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0061 - val_loss: 0.0067\n",
      "Epoch 913/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0058 - val_loss: 0.0067\n",
      "Epoch 914/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0057 - val_loss: 0.0067\n",
      "Epoch 915/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0056 - val_loss: 0.0068\n",
      "Epoch 916/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0057 - val_loss: 0.0068\n",
      "Epoch 917/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0060 - val_loss: 0.0068\n",
      "Epoch 918/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0059 - val_loss: 0.0067\n",
      "Epoch 919/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0056 - val_loss: 0.0070\n",
      "Epoch 920/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0057 - val_loss: 0.0067\n",
      "Epoch 921/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0057 - val_loss: 0.0066\n",
      "Epoch 922/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0058 - val_loss: 0.0065\n",
      "Epoch 923/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0058 - val_loss: 0.0068\n",
      "Epoch 924/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0056 - val_loss: 0.0067\n",
      "Epoch 925/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0059 - val_loss: 0.0065\n",
      "Epoch 926/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0058 - val_loss: 0.0065\n",
      "Epoch 927/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0055 - val_loss: 0.0066\n",
      "Epoch 928/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0055 - val_loss: 0.0066\n",
      "Epoch 929/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0056 - val_loss: 0.0064\n",
      "Epoch 930/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0057 - val_loss: 0.0064\n",
      "Epoch 931/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0055 - val_loss: 0.0065\n",
      "Epoch 932/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0055 - val_loss: 0.0066\n",
      "Epoch 933/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0056 - val_loss: 0.0065\n",
      "Epoch 934/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0056 - val_loss: 0.0064\n",
      "Epoch 935/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0055 - val_loss: 0.0065\n",
      "Epoch 936/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0055 - val_loss: 0.0066\n",
      "Epoch 937/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0054 - val_loss: 0.0065\n",
      "Epoch 938/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0055 - val_loss: 0.0065\n",
      "Epoch 939/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0056 - val_loss: 0.0063\n",
      "Epoch 940/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0053 - val_loss: 0.0063\n",
      "Epoch 941/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0058 - val_loss: 0.0063\n",
      "Epoch 942/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0054 - val_loss: 0.0063\n",
      "Epoch 943/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0053 - val_loss: 0.0063\n",
      "Epoch 944/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0053 - val_loss: 0.0063\n",
      "Epoch 945/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0054 - val_loss: 0.0062\n",
      "Epoch 946/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0055 - val_loss: 0.0062\n",
      "Epoch 947/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0052 - val_loss: 0.0062\n",
      "Epoch 948/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0053 - val_loss: 0.0062\n",
      "Epoch 949/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0054 - val_loss: 0.0062\n",
      "Epoch 950/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0055 - val_loss: 0.0064\n",
      "Epoch 951/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0055 - val_loss: 0.0063\n",
      "Epoch 952/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0054 - val_loss: 0.0063\n",
      "Epoch 953/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0055 - val_loss: 0.0062\n",
      "Epoch 954/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0052 - val_loss: 0.0061\n",
      "Epoch 955/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0051 - val_loss: 0.0061\n",
      "Epoch 956/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0053 - val_loss: 0.0064\n",
      "Epoch 957/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0054 - val_loss: 0.0062\n",
      "Epoch 958/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0053 - val_loss: 0.0062\n",
      "Epoch 959/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0053 - val_loss: 0.0063\n",
      "Epoch 960/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0052 - val_loss: 0.0064\n",
      "Epoch 961/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0052 - val_loss: 0.0062\n",
      "Epoch 962/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0052 - val_loss: 0.0062\n",
      "Epoch 963/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0052 - val_loss: 0.0061\n",
      "Epoch 964/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0052 - val_loss: 0.0062\n",
      "Epoch 965/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0055 - val_loss: 0.0061\n",
      "Epoch 966/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0054 - val_loss: 0.0061\n",
      "Epoch 967/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0054 - val_loss: 0.0062\n",
      "Epoch 968/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0052 - val_loss: 0.0063\n",
      "Epoch 969/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0054 - val_loss: 0.0062\n",
      "Epoch 970/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0053 - val_loss: 0.0061\n",
      "Epoch 971/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0052 - val_loss: 0.0061\n",
      "Epoch 972/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0050 - val_loss: 0.0063\n",
      "Epoch 973/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0052 - val_loss: 0.0060\n",
      "Epoch 974/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0051 - val_loss: 0.0060\n",
      "Epoch 975/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0052 - val_loss: 0.0061\n",
      "Epoch 976/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0052 - val_loss: 0.0064\n",
      "Epoch 977/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0054 - val_loss: 0.0062\n",
      "Epoch 978/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0051 - val_loss: 0.0065\n",
      "Epoch 979/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0055 - val_loss: 0.0061\n",
      "Epoch 980/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0051 - val_loss: 0.0065\n",
      "Epoch 981/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0056 - val_loss: 0.0060\n",
      "Epoch 982/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0050 - val_loss: 0.0063\n",
      "Epoch 983/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0056 - val_loss: 0.0061\n",
      "Epoch 984/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0049 - val_loss: 0.0065\n",
      "Epoch 985/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0054 - val_loss: 0.0060\n",
      "Epoch 986/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0052 - val_loss: 0.0064\n",
      "Epoch 987/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0056 - val_loss: 0.0061\n",
      "Epoch 988/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0051 - val_loss: 0.0067\n",
      "Epoch 989/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0052 - val_loss: 0.0062\n",
      "Epoch 990/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0053 - val_loss: 0.0062\n",
      "Epoch 991/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0052 - val_loss: 0.0061\n",
      "Epoch 992/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0050 - val_loss: 0.0060\n",
      "Epoch 993/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0052 - val_loss: 0.0061\n",
      "Epoch 994/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0052 - val_loss: 0.0058\n",
      "Epoch 995/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0051 - val_loss: 0.0061\n",
      "Epoch 996/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0053 - val_loss: 0.0059\n",
      "Epoch 997/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0049 - val_loss: 0.0064\n",
      "Epoch 998/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0053 - val_loss: 0.0059\n",
      "Epoch 999/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0048 - val_loss: 0.0059\n",
      "Epoch 1000/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0050 - val_loss: 0.0058\n",
      "Epoch 1001/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0048 - val_loss: 0.0062\n",
      "Epoch 1002/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0051 - val_loss: 0.0058\n",
      "Epoch 1003/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0051 - val_loss: 0.0059\n",
      "Epoch 1004/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0050 - val_loss: 0.0059\n",
      "Epoch 1005/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0048 - val_loss: 0.0060\n",
      "Epoch 1006/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0049 - val_loss: 0.0059\n",
      "Epoch 1007/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0050 - val_loss: 0.0058\n",
      "Epoch 1008/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0050 - val_loss: 0.0057\n",
      "Epoch 1009/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0049 - val_loss: 0.0060\n",
      "Epoch 1010/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0050 - val_loss: 0.0057\n",
      "Epoch 1011/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0048 - val_loss: 0.0057\n",
      "Epoch 1012/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0049 - val_loss: 0.0056\n",
      "Epoch 1013/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0047 - val_loss: 0.0056\n",
      "Epoch 1014/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0049 - val_loss: 0.0057\n",
      "Epoch 1015/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0047 - val_loss: 0.0056\n",
      "Epoch 1016/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0050 - val_loss: 0.0057\n",
      "Epoch 1017/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0049 - val_loss: 0.0060\n",
      "Epoch 1018/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0050 - val_loss: 0.0057\n",
      "Epoch 1019/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0047 - val_loss: 0.0057\n",
      "Epoch 1020/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0048 - val_loss: 0.0057\n",
      "Epoch 1021/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0047 - val_loss: 0.0057\n",
      "Epoch 1022/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0047 - val_loss: 0.0057\n",
      "Epoch 1023/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0049 - val_loss: 0.0056\n",
      "Epoch 1024/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0047 - val_loss: 0.0058\n",
      "Epoch 1025/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0048 - val_loss: 0.0057\n",
      "Epoch 1026/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0047 - val_loss: 0.0058\n",
      "Epoch 1027/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0048 - val_loss: 0.0063\n",
      "Epoch 1028/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0051 - val_loss: 0.0055\n",
      "Epoch 1029/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0050 - val_loss: 0.0058\n",
      "Epoch 1030/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0048 - val_loss: 0.0056\n",
      "Epoch 1031/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0047 - val_loss: 0.0063\n",
      "Epoch 1032/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0053 - val_loss: 0.0058\n",
      "Epoch 1033/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0048 - val_loss: 0.0058\n",
      "Epoch 1034/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0047 - val_loss: 0.0058\n",
      "Epoch 1035/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0047 - val_loss: 0.0059\n",
      "Epoch 1036/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0047 - val_loss: 0.0062\n",
      "Epoch 1037/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0051 - val_loss: 0.0057\n",
      "Epoch 1038/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0048 - val_loss: 0.0057\n",
      "Epoch 1039/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0048 - val_loss: 0.0059\n",
      "Epoch 1040/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0048 - val_loss: 0.0058\n",
      "Epoch 1041/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0047 - val_loss: 0.0056\n",
      "Epoch 1042/2000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0047 - val_loss: 0.0055\n",
      "Epoch 1043/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0046 - val_loss: 0.0057\n",
      "Epoch 1044/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0047 - val_loss: 0.0058\n",
      "Epoch 1045/2000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0049 - val_loss: 0.0055\n",
      "Epoch 1046/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0048 - val_loss: 0.0056\n",
      "Epoch 1047/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0051 - val_loss: 0.0058\n",
      "Epoch 1048/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0046 - val_loss: 0.0059\n",
      "Epoch 1049/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0047 - val_loss: 0.0055\n",
      "Epoch 1050/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0045 - val_loss: 0.0056\n",
      "Epoch 1051/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0047 - val_loss: 0.0055\n",
      "Epoch 1052/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0046 - val_loss: 0.0059\n",
      "Epoch 1053/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0047 - val_loss: 0.0057\n",
      "Epoch 1054/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0046 - val_loss: 0.0056\n",
      "Epoch 1055/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0045 - val_loss: 0.0056\n",
      "Epoch 1056/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0044 - val_loss: 0.0055\n",
      "Epoch 1057/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0044 - val_loss: 0.0055\n",
      "Epoch 1058/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0045 - val_loss: 0.0054\n",
      "Epoch 1059/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0044 - val_loss: 0.0054\n",
      "Epoch 1060/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0045 - val_loss: 0.0054\n",
      "Epoch 1061/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0048 - val_loss: 0.0054\n",
      "Epoch 1062/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0046 - val_loss: 0.0055\n",
      "Epoch 1063/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0045 - val_loss: 0.0055\n",
      "Epoch 1064/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0046 - val_loss: 0.0054\n",
      "Epoch 1065/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0047 - val_loss: 0.0055\n",
      "Epoch 1066/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0046 - val_loss: 0.0054\n",
      "Epoch 1067/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0044 - val_loss: 0.0053\n",
      "Epoch 1068/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0045 - val_loss: 0.0053\n",
      "Epoch 1069/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0044 - val_loss: 0.0054\n",
      "Epoch 1070/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0045 - val_loss: 0.0053\n",
      "Epoch 1071/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0043 - val_loss: 0.0053\n",
      "Epoch 1072/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0044 - val_loss: 0.0053\n",
      "Epoch 1073/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0043 - val_loss: 0.0053\n",
      "Epoch 1074/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0046 - val_loss: 0.0053\n",
      "Epoch 1075/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0044 - val_loss: 0.0052\n",
      "Epoch 1076/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0044 - val_loss: 0.0052\n",
      "Epoch 1077/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0045 - val_loss: 0.0053\n",
      "Epoch 1078/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0044 - val_loss: 0.0052\n",
      "Epoch 1079/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0043 - val_loss: 0.0052\n",
      "Epoch 1080/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0045 - val_loss: 0.0052\n",
      "Epoch 1081/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0044 - val_loss: 0.0053\n",
      "Epoch 1082/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0045 - val_loss: 0.0053\n",
      "Epoch 1083/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0042 - val_loss: 0.0052\n",
      "Epoch 1084/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0045 - val_loss: 0.0052\n",
      "Epoch 1085/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0043 - val_loss: 0.0052\n",
      "Epoch 1086/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0042 - val_loss: 0.0053\n",
      "Epoch 1087/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0044 - val_loss: 0.0052\n",
      "Epoch 1088/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0042 - val_loss: 0.0052\n",
      "Epoch 1089/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0043 - val_loss: 0.0052\n",
      "Epoch 1090/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0043 - val_loss: 0.0053\n",
      "Epoch 1091/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0044 - val_loss: 0.0052\n",
      "Epoch 1092/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0045 - val_loss: 0.0052\n",
      "Epoch 1093/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0043 - val_loss: 0.0052\n",
      "Epoch 1094/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0044 - val_loss: 0.0052\n",
      "Epoch 1095/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0042 - val_loss: 0.0053\n",
      "Epoch 1096/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0042 - val_loss: 0.0052\n",
      "Epoch 1097/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0043 - val_loss: 0.0051\n",
      "Epoch 1098/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0042 - val_loss: 0.0052\n",
      "Epoch 1099/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0043 - val_loss: 0.0052\n",
      "Epoch 1100/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0043 - val_loss: 0.0052\n",
      "Epoch 1101/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0042 - val_loss: 0.0051\n",
      "Epoch 1102/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0041 - val_loss: 0.0051\n",
      "Epoch 1103/2000\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0042 - val_loss: 0.0051\n",
      "Epoch 1104/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0042 - val_loss: 0.0053\n",
      "Epoch 1105/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0043 - val_loss: 0.0051\n",
      "Epoch 1106/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0042 - val_loss: 0.0051\n",
      "Epoch 1107/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0043 - val_loss: 0.0052\n",
      "Epoch 1108/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0047 - val_loss: 0.0056\n",
      "Epoch 1109/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0049 - val_loss: 0.0054\n",
      "Epoch 1110/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0046 - val_loss: 0.0052\n",
      "Epoch 1111/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0045 - val_loss: 0.0065\n",
      "Epoch 1112/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0049 - val_loss: 0.0055\n",
      "Epoch 1113/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0044 - val_loss: 0.0054\n",
      "Epoch 1114/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0049 - val_loss: 0.0052\n",
      "Epoch 1115/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0043 - val_loss: 0.0057\n",
      "Epoch 1116/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0046 - val_loss: 0.0051\n",
      "Epoch 1117/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0041 - val_loss: 0.0052\n",
      "Epoch 1118/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0044 - val_loss: 0.0051\n",
      "Epoch 1119/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0041 - val_loss: 0.0053\n",
      "Epoch 1120/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0046 - val_loss: 0.0051\n",
      "Epoch 1121/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0043 - val_loss: 0.0051\n",
      "Epoch 1122/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0044 - val_loss: 0.0051\n",
      "Epoch 1123/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0045 - val_loss: 0.0054\n",
      "Epoch 1124/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0045 - val_loss: 0.0051\n",
      "Epoch 1125/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0041 - val_loss: 0.0051\n",
      "Epoch 1126/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0043 - val_loss: 0.0052\n",
      "Epoch 1127/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0042 - val_loss: 0.0052\n",
      "Epoch 1128/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0041 - val_loss: 0.0049\n",
      "Epoch 1129/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0042 - val_loss: 0.0049\n",
      "Epoch 1130/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0042 - val_loss: 0.0049\n",
      "Epoch 1131/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0040 - val_loss: 0.0049\n",
      "Epoch 1132/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0042 - val_loss: 0.0050\n",
      "Epoch 1133/2000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0041 - val_loss: 0.0050\n",
      "Epoch 1134/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0042 - val_loss: 0.0049\n",
      "Epoch 1135/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0043 - val_loss: 0.0049\n",
      "Epoch 1136/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0041 - val_loss: 0.0049\n",
      "Epoch 1137/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0040 - val_loss: 0.0049\n",
      "Epoch 1138/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0041 - val_loss: 0.0050\n",
      "Epoch 1139/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0041 - val_loss: 0.0051\n",
      "Epoch 1140/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0042 - val_loss: 0.0051\n",
      "Epoch 1141/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0042 - val_loss: 0.0052\n",
      "Epoch 1142/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0044 - val_loss: 0.0051\n",
      "Epoch 1143/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0042 - val_loss: 0.0057\n",
      "Epoch 1144/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0044 - val_loss: 0.0053\n",
      "Epoch 1145/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0043 - val_loss: 0.0051\n",
      "Epoch 1146/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0043 - val_loss: 0.0050\n",
      "Epoch 1147/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0042 - val_loss: 0.0057\n",
      "Epoch 1148/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0046 - val_loss: 0.0055\n",
      "Epoch 1149/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0042 - val_loss: 0.0052\n",
      "Epoch 1150/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0045 - val_loss: 0.0051\n",
      "Epoch 1151/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0041 - val_loss: 0.0051\n",
      "Epoch 1152/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0042 - val_loss: 0.0053\n",
      "Epoch 1153/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0045 - val_loss: 0.0051\n",
      "Epoch 1154/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0045 - val_loss: 0.0051\n",
      "Epoch 1155/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0045 - val_loss: 0.0057\n",
      "Epoch 1156/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0046 - val_loss: 0.0052\n",
      "Epoch 1157/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0042 - val_loss: 0.0054\n",
      "Epoch 1158/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0046 - val_loss: 0.0050\n",
      "Epoch 1159/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0040 - val_loss: 0.0053\n",
      "Epoch 1160/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0040 - val_loss: 0.0053\n",
      "Epoch 1161/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0043 - val_loss: 0.0050\n",
      "Epoch 1162/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0043 - val_loss: 0.0049\n",
      "Epoch 1163/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0039 - val_loss: 0.0050\n",
      "Epoch 1164/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0040 - val_loss: 0.0050\n",
      "Epoch 1165/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0040 - val_loss: 0.0049\n",
      "Epoch 1166/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0041 - val_loss: 0.0050\n",
      "Epoch 1167/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0040 - val_loss: 0.0051\n",
      "Epoch 1168/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0040 - val_loss: 0.0049\n",
      "Epoch 1169/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0040 - val_loss: 0.0048\n",
      "Epoch 1170/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0040 - val_loss: 0.0050\n",
      "Epoch 1171/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0041 - val_loss: 0.0048\n",
      "Epoch 1172/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0040 - val_loss: 0.0050\n",
      "Epoch 1173/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0041 - val_loss: 0.0049\n",
      "Epoch 1174/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0039 - val_loss: 0.0049\n",
      "Epoch 1175/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0039 - val_loss: 0.0047\n",
      "Epoch 1176/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0038 - val_loss: 0.0047\n",
      "Epoch 1177/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0040 - val_loss: 0.0047\n",
      "Epoch 1178/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0041 - val_loss: 0.0049\n",
      "Epoch 1179/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0039 - val_loss: 0.0047\n",
      "Epoch 1180/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0039 - val_loss: 0.0048\n",
      "Epoch 1181/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0039 - val_loss: 0.0049\n",
      "Epoch 1182/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0040 - val_loss: 0.0050\n",
      "Epoch 1183/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0039 - val_loss: 0.0049\n",
      "Epoch 1184/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0038 - val_loss: 0.0048\n",
      "Epoch 1185/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0038 - val_loss: 0.0048\n",
      "Epoch 1186/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0040 - val_loss: 0.0048\n",
      "Epoch 1187/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0040 - val_loss: 0.0048\n",
      "Epoch 1188/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0040 - val_loss: 0.0048\n",
      "Epoch 1189/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0039 - val_loss: 0.0049\n",
      "Epoch 1190/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0040 - val_loss: 0.0050\n",
      "Epoch 1191/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0040 - val_loss: 0.0047\n",
      "Epoch 1192/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0039 - val_loss: 0.0046\n",
      "Epoch 1193/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0039 - val_loss: 0.0048\n",
      "Epoch 1194/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0041 - val_loss: 0.0049\n",
      "Epoch 1195/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0038 - val_loss: 0.0048\n",
      "Epoch 1196/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0039 - val_loss: 0.0049\n",
      "Epoch 1197/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0042 - val_loss: 0.0053\n",
      "Epoch 1198/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0039 - val_loss: 0.0053\n",
      "Epoch 1199/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0041 - val_loss: 0.0050\n",
      "Epoch 1200/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0041 - val_loss: 0.0048\n",
      "Epoch 1201/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0040 - val_loss: 0.0049\n",
      "Epoch 1202/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0039 - val_loss: 0.0049\n",
      "Epoch 1203/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0040 - val_loss: 0.0048\n",
      "Epoch 1204/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0039 - val_loss: 0.0047\n",
      "Epoch 1205/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0039 - val_loss: 0.0047\n",
      "Epoch 1206/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0037 - val_loss: 0.0047\n",
      "Epoch 1207/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0039 - val_loss: 0.0047\n",
      "Epoch 1208/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0039 - val_loss: 0.0045\n",
      "Epoch 1209/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0038 - val_loss: 0.0045\n",
      "Epoch 1210/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0038 - val_loss: 0.0046\n",
      "Epoch 1211/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0040 - val_loss: 0.0049\n",
      "Epoch 1212/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0039 - val_loss: 0.0046\n",
      "Epoch 1213/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0038 - val_loss: 0.0047\n",
      "Epoch 1214/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0038 - val_loss: 0.0048\n",
      "Epoch 1215/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0038 - val_loss: 0.0048\n",
      "Epoch 1216/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0039 - val_loss: 0.0047\n",
      "Epoch 1217/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0042 - val_loss: 0.0047\n",
      "Epoch 1218/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0038 - val_loss: 0.0048\n",
      "Epoch 1219/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0038 - val_loss: 0.0048\n",
      "Epoch 1220/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0039 - val_loss: 0.0047\n",
      "Epoch 1221/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0038 - val_loss: 0.0047\n",
      "Epoch 1222/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0037 - val_loss: 0.0047\n",
      "Epoch 1223/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0037 - val_loss: 0.0045\n",
      "Epoch 1224/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0036 - val_loss: 0.0045\n",
      "Epoch 1225/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0037 - val_loss: 0.0051\n",
      "Epoch 1226/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0041 - val_loss: 0.0047\n",
      "Epoch 1227/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0038 - val_loss: 0.0049\n",
      "Epoch 1228/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0044 - val_loss: 0.0050\n",
      "Epoch 1229/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0039 - val_loss: 0.0056\n",
      "Epoch 1230/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0045 - val_loss: 0.0048\n",
      "Epoch 1231/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 1232/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0038 - val_loss: 0.0061\n",
      "Epoch 1233/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0048 - val_loss: 0.0046\n",
      "Epoch 1234/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0037 - val_loss: 0.0049\n",
      "Epoch 1235/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0043 - val_loss: 0.0047\n",
      "Epoch 1236/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0038 - val_loss: 0.0052\n",
      "Epoch 1237/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0041 - val_loss: 0.0045\n",
      "Epoch 1238/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0038 - val_loss: 0.0046\n",
      "Epoch 1239/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0038 - val_loss: 0.0047\n",
      "Epoch 1240/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0040 - val_loss: 0.0048\n",
      "Epoch 1241/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0037 - val_loss: 0.0048\n",
      "Epoch 1242/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0037 - val_loss: 0.0047\n",
      "Epoch 1243/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0038 - val_loss: 0.0048\n",
      "Epoch 1244/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0037 - val_loss: 0.0047\n",
      "Epoch 1245/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0038 - val_loss: 0.0046\n",
      "Epoch 1246/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0036 - val_loss: 0.0046\n",
      "Epoch 1247/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0038 - val_loss: 0.0047\n",
      "Epoch 1248/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0038 - val_loss: 0.0049\n",
      "Epoch 1249/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0038 - val_loss: 0.0048\n",
      "Epoch 1250/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0037 - val_loss: 0.0047\n",
      "Epoch 1251/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0038 - val_loss: 0.0052\n",
      "Epoch 1252/2000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0039 - val_loss: 0.0047\n",
      "Epoch 1253/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0038 - val_loss: 0.0045\n",
      "Epoch 1254/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0039 - val_loss: 0.0044\n",
      "Epoch 1255/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0038 - val_loss: 0.0051\n",
      "Epoch 1256/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0040 - val_loss: 0.0045\n",
      "Epoch 1257/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0037 - val_loss: 0.0046\n",
      "Epoch 1258/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0039 - val_loss: 0.0047\n",
      "Epoch 1259/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0037 - val_loss: 0.0052\n",
      "Epoch 1260/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0040 - val_loss: 0.0045\n",
      "Epoch 1261/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0040 - val_loss: 0.0046\n",
      "Epoch 1262/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0036 - val_loss: 0.0048\n",
      "Epoch 1263/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0037 - val_loss: 0.0045\n",
      "Epoch 1264/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0037 - val_loss: 0.0045\n",
      "Epoch 1265/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0036 - val_loss: 0.0046\n",
      "Epoch 1266/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0037 - val_loss: 0.0048\n",
      "Epoch 1267/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0037 - val_loss: 0.0045\n",
      "Epoch 1268/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0036 - val_loss: 0.0045\n",
      "Epoch 1269/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0037 - val_loss: 0.0048\n",
      "Epoch 1270/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0037 - val_loss: 0.0047\n",
      "Epoch 1271/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0035 - val_loss: 0.0045\n",
      "Epoch 1272/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0037 - val_loss: 0.0045\n",
      "Epoch 1273/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0038 - val_loss: 0.0045\n",
      "Epoch 1274/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0035 - val_loss: 0.0046\n",
      "Epoch 1275/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0037 - val_loss: 0.0045\n",
      "Epoch 1276/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0039 - val_loss: 0.0045\n",
      "Epoch 1277/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0036 - val_loss: 0.0044\n",
      "Epoch 1278/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0036 - val_loss: 0.0044\n",
      "Epoch 1279/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0035 - val_loss: 0.0044\n",
      "Epoch 1280/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0035 - val_loss: 0.0045\n",
      "Epoch 1281/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0036 - val_loss: 0.0044\n",
      "Epoch 1282/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0035 - val_loss: 0.0044\n",
      "Epoch 1283/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0035 - val_loss: 0.0045\n",
      "Epoch 1284/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0035 - val_loss: 0.0045\n",
      "Epoch 1285/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0037 - val_loss: 0.0044\n",
      "Epoch 1286/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0036 - val_loss: 0.0044\n",
      "Epoch 1287/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0036 - val_loss: 0.0045\n",
      "Epoch 1288/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0035 - val_loss: 0.0046\n",
      "Epoch 1289/2000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0038 - val_loss: 0.0044\n",
      "Epoch 1290/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0037 - val_loss: 0.0045\n",
      "Epoch 1291/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0039 - val_loss: 0.0047\n",
      "Epoch 1292/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0037 - val_loss: 0.0047\n",
      "Epoch 1293/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0037 - val_loss: 0.0045\n",
      "Epoch 1294/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0039 - val_loss: 0.0045\n",
      "Epoch 1295/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0036 - val_loss: 0.0049\n",
      "Epoch 1296/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0038 - val_loss: 0.0045\n",
      "Epoch 1297/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0035 - val_loss: 0.0044\n",
      "Epoch 1298/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0037 - val_loss: 0.0046\n",
      "Epoch 1299/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0036 - val_loss: 0.0044\n",
      "Epoch 1300/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0036 - val_loss: 0.0044\n",
      "Epoch 1301/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0039 - val_loss: 0.0046\n",
      "Epoch 1302/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0037 - val_loss: 0.0044\n",
      "Epoch 1303/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0035 - val_loss: 0.0044\n",
      "Epoch 1304/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0035 - val_loss: 0.0044\n",
      "Epoch 1305/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0035 - val_loss: 0.0044\n",
      "Epoch 1306/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0035 - val_loss: 0.0046\n",
      "Epoch 1307/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0035 - val_loss: 0.0045\n",
      "Epoch 1308/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0034 - val_loss: 0.0045\n",
      "Epoch 1309/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0036 - val_loss: 0.0046\n",
      "Epoch 1310/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0036 - val_loss: 0.0046\n",
      "Epoch 1311/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0036 - val_loss: 0.0047\n",
      "Epoch 1312/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0034 - val_loss: 0.0044\n",
      "Epoch 1313/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0035 - val_loss: 0.0043\n",
      "Epoch 1314/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0037 - val_loss: 0.0047\n",
      "Epoch 1315/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0038 - val_loss: 0.0046\n",
      "Epoch 1316/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0037 - val_loss: 0.0046\n",
      "Epoch 1317/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0036 - val_loss: 0.0046\n",
      "Epoch 1318/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0035 - val_loss: 0.0049\n",
      "Epoch 1319/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0036 - val_loss: 0.0045\n",
      "Epoch 1320/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0034 - val_loss: 0.0044\n",
      "Epoch 1321/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0035 - val_loss: 0.0044\n",
      "Epoch 1322/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0034 - val_loss: 0.0047\n",
      "Epoch 1323/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0036 - val_loss: 0.0043\n",
      "Epoch 1324/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0036 - val_loss: 0.0043\n",
      "Epoch 1325/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0035 - val_loss: 0.0044\n",
      "Epoch 1326/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0034 - val_loss: 0.0046\n",
      "Epoch 1327/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0036 - val_loss: 0.0043\n",
      "Epoch 1328/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0036 - val_loss: 0.0043\n",
      "Epoch 1329/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0034 - val_loss: 0.0049\n",
      "Epoch 1330/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0038 - val_loss: 0.0044\n",
      "Epoch 1331/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0035 - val_loss: 0.0044\n",
      "Epoch 1332/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0039 - val_loss: 0.0045\n",
      "Epoch 1333/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0035 - val_loss: 0.0049\n",
      "Epoch 1334/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0040 - val_loss: 0.0043\n",
      "Epoch 1335/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0035 - val_loss: 0.0043\n",
      "Epoch 1336/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0037 - val_loss: 0.0047\n",
      "Epoch 1337/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0036 - val_loss: 0.0045\n",
      "Epoch 1338/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0036 - val_loss: 0.0043\n",
      "Epoch 1339/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0035 - val_loss: 0.0044\n",
      "Epoch 1340/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0039 - val_loss: 0.0055\n",
      "Epoch 1341/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0045 - val_loss: 0.0044\n",
      "Epoch 1342/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0034 - val_loss: 0.0045\n",
      "Epoch 1343/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0040 - val_loss: 0.0045\n",
      "Epoch 1344/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0036 - val_loss: 0.0052\n",
      "Epoch 1345/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0039 - val_loss: 0.0042\n",
      "Epoch 1346/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0035 - val_loss: 0.0044\n",
      "Epoch 1347/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0038 - val_loss: 0.0046\n",
      "Epoch 1348/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0035 - val_loss: 0.0048\n",
      "Epoch 1349/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0037 - val_loss: 0.0043\n",
      "Epoch 1350/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0034 - val_loss: 0.0043\n",
      "Epoch 1351/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0035 - val_loss: 0.0045\n",
      "Epoch 1352/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0037 - val_loss: 0.0044\n",
      "Epoch 1353/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0034 - val_loss: 0.0043\n",
      "Epoch 1354/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0033 - val_loss: 0.0043\n",
      "Epoch 1355/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0035 - val_loss: 0.0044\n",
      "Epoch 1356/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0036 - val_loss: 0.0044\n",
      "Epoch 1357/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0034 - val_loss: 0.0043\n",
      "Epoch 1358/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 1359/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0037 - val_loss: 0.0043\n",
      "Epoch 1360/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0035 - val_loss: 0.0044\n",
      "Epoch 1361/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0034 - val_loss: 0.0043\n",
      "Epoch 1362/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0036 - val_loss: 0.0043\n",
      "Epoch 1363/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0035 - val_loss: 0.0043\n",
      "Epoch 1364/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 1365/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0034 - val_loss: 0.0043\n",
      "Epoch 1366/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0034 - val_loss: 0.0043\n",
      "Epoch 1367/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0035 - val_loss: 0.0042\n",
      "Epoch 1368/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0034 - val_loss: 0.0044\n",
      "Epoch 1369/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0035 - val_loss: 0.0045\n",
      "Epoch 1370/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0036 - val_loss: 0.0045\n",
      "Epoch 1371/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 1372/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0034 - val_loss: 0.0043\n",
      "Epoch 1373/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0033 - val_loss: 0.0047\n",
      "Epoch 1374/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0035 - val_loss: 0.0042\n",
      "Epoch 1375/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0035 - val_loss: 0.0042\n",
      "Epoch 1376/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0033 - val_loss: 0.0044\n",
      "Epoch 1377/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0035 - val_loss: 0.0043\n",
      "Epoch 1378/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0037 - val_loss: 0.0043\n",
      "Epoch 1379/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0035 - val_loss: 0.0043\n",
      "Epoch 1380/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0033 - val_loss: 0.0043\n",
      "Epoch 1381/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0033 - val_loss: 0.0043\n",
      "Epoch 1382/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 1383/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0034 - val_loss: 0.0044\n",
      "Epoch 1384/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 1385/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0034 - val_loss: 0.0044\n",
      "Epoch 1386/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0034 - val_loss: 0.0046\n",
      "Epoch 1387/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0034 - val_loss: 0.0044\n",
      "Epoch 1388/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0036 - val_loss: 0.0044\n",
      "Epoch 1389/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0034 - val_loss: 0.0043\n",
      "Epoch 1390/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 1391/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0036 - val_loss: 0.0045\n",
      "Epoch 1392/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0034 - val_loss: 0.0043\n",
      "Epoch 1393/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 1394/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 1395/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0035 - val_loss: 0.0051\n",
      "Epoch 1396/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0037 - val_loss: 0.0042\n",
      "Epoch 1397/2000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0033 - val_loss: 0.0043\n",
      "Epoch 1398/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0038 - val_loss: 0.0046\n",
      "Epoch 1399/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0034 - val_loss: 0.0044\n",
      "Epoch 1400/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 1401/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0035 - val_loss: 0.0044\n",
      "Epoch 1402/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0033 - val_loss: 0.0046\n",
      "Epoch 1403/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0035 - val_loss: 0.0044\n",
      "Epoch 1404/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 1405/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0033 - val_loss: 0.0043\n",
      "Epoch 1406/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0033 - val_loss: 0.0045\n",
      "Epoch 1407/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0033 - val_loss: 0.0043\n",
      "Epoch 1408/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 1409/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 1410/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 1411/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0033 - val_loss: 0.0041\n",
      "Epoch 1412/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0034 - val_loss: 0.0047\n",
      "Epoch 1413/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0037 - val_loss: 0.0040\n",
      "Epoch 1414/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 1415/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0040 - val_loss: 0.0047\n",
      "Epoch 1416/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0036 - val_loss: 0.0044\n",
      "Epoch 1417/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 1418/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0035 - val_loss: 0.0043\n",
      "Epoch 1419/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0033 - val_loss: 0.0044\n",
      "Epoch 1420/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0034 - val_loss: 0.0041\n",
      "Epoch 1421/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0034 - val_loss: 0.0041\n",
      "Epoch 1422/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0033 - val_loss: 0.0045\n",
      "Epoch 1423/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 1424/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 1425/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 1426/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0032 - val_loss: 0.0044\n",
      "Epoch 1427/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0032 - val_loss: 0.0045\n",
      "Epoch 1428/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0034 - val_loss: 0.0040\n",
      "Epoch 1429/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0033 - val_loss: 0.0040\n",
      "Epoch 1430/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0035 - val_loss: 0.0044\n",
      "Epoch 1431/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 1432/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0031 - val_loss: 0.0041\n",
      "Epoch 1433/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0034 - val_loss: 0.0043\n",
      "Epoch 1434/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0033 - val_loss: 0.0045\n",
      "Epoch 1435/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 1436/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 1437/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0031 - val_loss: 0.0042\n",
      "Epoch 1438/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0033 - val_loss: 0.0046\n",
      "Epoch 1439/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 1440/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0033 - val_loss: 0.0043\n",
      "Epoch 1441/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0032 - val_loss: 0.0047\n",
      "Epoch 1442/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0035 - val_loss: 0.0042\n",
      "Epoch 1443/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0034 - val_loss: 0.0041\n",
      "Epoch 1444/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0032 - val_loss: 0.0047\n",
      "Epoch 1445/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0036 - val_loss: 0.0041\n",
      "Epoch 1446/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0032 - val_loss: 0.0041\n",
      "Epoch 1447/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0035 - val_loss: 0.0045\n",
      "Epoch 1448/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0034 - val_loss: 0.0045\n",
      "Epoch 1449/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 1450/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0033 - val_loss: 0.0041\n",
      "Epoch 1451/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0031 - val_loss: 0.0043\n",
      "Epoch 1452/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 1453/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0031 - val_loss: 0.0041\n",
      "Epoch 1454/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0032 - val_loss: 0.0041\n",
      "Epoch 1455/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0033 - val_loss: 0.0044\n",
      "Epoch 1456/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0033 - val_loss: 0.0043\n",
      "Epoch 1457/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 1458/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0032 - val_loss: 0.0041\n",
      "Epoch 1459/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0033 - val_loss: 0.0043\n",
      "Epoch 1460/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 1461/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 1462/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0031 - val_loss: 0.0043\n",
      "Epoch 1463/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0031 - val_loss: 0.0042\n",
      "Epoch 1464/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0032 - val_loss: 0.0043\n",
      "Epoch 1465/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0033 - val_loss: 0.0043\n",
      "Epoch 1466/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0035 - val_loss: 0.0041\n",
      "Epoch 1467/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 1468/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0033 - val_loss: 0.0040\n",
      "Epoch 1469/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0033 - val_loss: 0.0041\n",
      "Epoch 1470/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0032 - val_loss: 0.0041\n",
      "Epoch 1471/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0031 - val_loss: 0.0041\n",
      "Epoch 1472/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 1473/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0033 - val_loss: 0.0043\n",
      "Epoch 1474/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0032 - val_loss: 0.0043\n",
      "Epoch 1475/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0032 - val_loss: 0.0041\n",
      "Epoch 1476/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0035 - val_loss: 0.0045\n",
      "Epoch 1477/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0033 - val_loss: 0.0041\n",
      "Epoch 1478/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0032 - val_loss: 0.0041\n",
      "Epoch 1479/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0032 - val_loss: 0.0045\n",
      "Epoch 1480/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0032 - val_loss: 0.0046\n",
      "Epoch 1481/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0035 - val_loss: 0.0041\n",
      "Epoch 1482/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0034 - val_loss: 0.0040\n",
      "Epoch 1483/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0031 - val_loss: 0.0046\n",
      "Epoch 1484/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0035 - val_loss: 0.0042\n",
      "Epoch 1485/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0031 - val_loss: 0.0041\n",
      "Epoch 1486/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0037 - val_loss: 0.0042\n",
      "Epoch 1487/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0031 - val_loss: 0.0052\n",
      "Epoch 1488/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0035 - val_loss: 0.0041\n",
      "Epoch 1489/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 1490/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0037 - val_loss: 0.0049\n",
      "Epoch 1491/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0036 - val_loss: 0.0044\n",
      "Epoch 1492/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0032 - val_loss: 0.0043\n",
      "Epoch 1493/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0039 - val_loss: 0.0043\n",
      "Epoch 1494/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0032 - val_loss: 0.0048\n",
      "Epoch 1495/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0036 - val_loss: 0.0041\n",
      "Epoch 1496/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0035 - val_loss: 0.0040\n",
      "Epoch 1497/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0032 - val_loss: 0.0045\n",
      "Epoch 1498/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0035 - val_loss: 0.0042\n",
      "Epoch 1499/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 1500/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 1501/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0031 - val_loss: 0.0042\n",
      "Epoch 1502/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0031 - val_loss: 0.0041\n",
      "Epoch 1503/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0033 - val_loss: 0.0041\n",
      "Epoch 1504/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0033 - val_loss: 0.0044\n",
      "Epoch 1505/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0032 - val_loss: 0.0044\n",
      "Epoch 1506/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 1507/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0032 - val_loss: 0.0041\n",
      "Epoch 1508/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0032 - val_loss: 0.0043\n",
      "Epoch 1509/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0031 - val_loss: 0.0045\n",
      "Epoch 1510/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0033 - val_loss: 0.0040\n",
      "Epoch 1511/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0033 - val_loss: 0.0039\n",
      "Epoch 1512/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0034 - val_loss: 0.0044\n",
      "Epoch 1513/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0034 - val_loss: 0.0040\n",
      "Epoch 1514/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0032 - val_loss: 0.0040\n",
      "Epoch 1515/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0032 - val_loss: 0.0046\n",
      "Epoch 1516/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0032 - val_loss: 0.0043\n",
      "Epoch 1517/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0033 - val_loss: 0.0040\n",
      "Epoch 1518/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0034 - val_loss: 0.0041\n",
      "Epoch 1519/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0031 - val_loss: 0.0048\n",
      "Epoch 1520/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0035 - val_loss: 0.0041\n",
      "Epoch 1521/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0032 - val_loss: 0.0041\n",
      "Epoch 1522/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0033 - val_loss: 0.0043\n",
      "Epoch 1523/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0031 - val_loss: 0.0047\n",
      "Epoch 1524/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0033 - val_loss: 0.0041\n",
      "Epoch 1525/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0031 - val_loss: 0.0041\n",
      "Epoch 1526/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0033 - val_loss: 0.0048\n",
      "Epoch 1527/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0035 - val_loss: 0.0042\n",
      "Epoch 1528/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0030 - val_loss: 0.0041\n",
      "Epoch 1529/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0032 - val_loss: 0.0041\n",
      "Epoch 1530/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0031 - val_loss: 0.0043\n",
      "Epoch 1531/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0032 - val_loss: 0.0039\n",
      "Epoch 1532/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0031 - val_loss: 0.0039\n",
      "Epoch 1533/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0033 - val_loss: 0.0043\n",
      "Epoch 1534/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0032 - val_loss: 0.0039\n",
      "Epoch 1535/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0031 - val_loss: 0.0039\n",
      "Epoch 1536/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0032 - val_loss: 0.0041\n",
      "Epoch 1537/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0030 - val_loss: 0.0045\n",
      "Epoch 1538/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0034 - val_loss: 0.0040\n",
      "Epoch 1539/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0032 - val_loss: 0.0040\n",
      "Epoch 1540/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0031 - val_loss: 0.0043\n",
      "Epoch 1541/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0031 - val_loss: 0.0042\n",
      "Epoch 1542/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0032 - val_loss: 0.0041\n",
      "Epoch 1543/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0033 - val_loss: 0.0045\n",
      "Epoch 1544/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 1545/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0030 - val_loss: 0.0040\n",
      "Epoch 1546/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0033 - val_loss: 0.0043\n",
      "Epoch 1547/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0030 - val_loss: 0.0040\n",
      "Epoch 1548/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0032 - val_loss: 0.0041\n",
      "Epoch 1549/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0032 - val_loss: 0.0044\n",
      "Epoch 1550/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0032 - val_loss: 0.0041\n",
      "Epoch 1551/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0030 - val_loss: 0.0041\n",
      "Epoch 1552/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0032 - val_loss: 0.0047\n",
      "Epoch 1553/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0032 - val_loss: 0.0043\n",
      "Epoch 1554/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0032 - val_loss: 0.0041\n",
      "Epoch 1555/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0035 - val_loss: 0.0042\n",
      "Epoch 1556/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0031 - val_loss: 0.0049\n",
      "Epoch 1557/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0035 - val_loss: 0.0039\n",
      "Epoch 1558/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0033 - val_loss: 0.0039\n",
      "Epoch 1559/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0030 - val_loss: 0.0044\n",
      "Epoch 1560/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0031 - val_loss: 0.0043\n",
      "Epoch 1561/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0033 - val_loss: 0.0041\n",
      "Epoch 1562/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0032 - val_loss: 0.0041\n",
      "Epoch 1563/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0032 - val_loss: 0.0052\n",
      "Epoch 1564/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 1565/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0030 - val_loss: 0.0041\n",
      "Epoch 1566/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0034 - val_loss: 0.0050\n",
      "Epoch 1567/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0034 - val_loss: 0.0045\n",
      "Epoch 1568/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0032 - val_loss: 0.0045\n",
      "Epoch 1569/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0035 - val_loss: 0.0042\n",
      "Epoch 1570/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0030 - val_loss: 0.0049\n",
      "Epoch 1571/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0033 - val_loss: 0.0043\n",
      "Epoch 1572/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0032 - val_loss: 0.0041\n",
      "Epoch 1573/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 1574/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0031 - val_loss: 0.0046\n",
      "Epoch 1575/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0031 - val_loss: 0.0041\n",
      "Epoch 1576/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0030 - val_loss: 0.0040\n",
      "Epoch 1577/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0032 - val_loss: 0.0041\n",
      "Epoch 1578/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0032 - val_loss: 0.0041\n",
      "Epoch 1579/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0032 - val_loss: 0.0039\n",
      "Epoch 1580/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 1581/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0031 - val_loss: 0.0043\n",
      "Epoch 1582/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0030 - val_loss: 0.0040\n",
      "Epoch 1583/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0033 - val_loss: 0.0045\n",
      "Epoch 1584/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0031 - val_loss: 0.0042\n",
      "Epoch 1585/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0030 - val_loss: 0.0040\n",
      "Epoch 1586/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0030 - val_loss: 0.0040\n",
      "Epoch 1587/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0033 - val_loss: 0.0047\n",
      "Epoch 1588/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 1589/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0032 - val_loss: 0.0048\n",
      "Epoch 1590/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0039 - val_loss: 0.0043\n",
      "Epoch 1591/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0031 - val_loss: 0.0050\n",
      "Epoch 1592/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0037 - val_loss: 0.0040\n",
      "Epoch 1593/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 1594/2000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0030 - val_loss: 0.0047\n",
      "Epoch 1595/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0035 - val_loss: 0.0040\n",
      "Epoch 1596/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0029 - val_loss: 0.0039\n",
      "Epoch 1597/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0033 - val_loss: 0.0045\n",
      "Epoch 1598/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0034 - val_loss: 0.0050\n",
      "Epoch 1599/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0035 - val_loss: 0.0040\n",
      "Epoch 1600/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0033 - val_loss: 0.0040\n",
      "Epoch 1601/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0033 - val_loss: 0.0045\n",
      "Epoch 1602/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0033 - val_loss: 0.0043\n",
      "Epoch 1603/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0030 - val_loss: 0.0039\n",
      "Epoch 1604/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0035 - val_loss: 0.0040\n",
      "Epoch 1605/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0029 - val_loss: 0.0044\n",
      "Epoch 1606/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0031 - val_loss: 0.0044\n",
      "Epoch 1607/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 1608/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0036 - val_loss: 0.0042\n",
      "Epoch 1609/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0032 - val_loss: 0.0049\n",
      "Epoch 1610/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0033 - val_loss: 0.0041\n",
      "Epoch 1611/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0031 - val_loss: 0.0042\n",
      "Epoch 1612/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0033 - val_loss: 0.0049\n",
      "Epoch 1613/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0032 - val_loss: 0.0048\n",
      "Epoch 1614/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0034 - val_loss: 0.0043\n",
      "Epoch 1615/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0033 - val_loss: 0.0040\n",
      "Epoch 1616/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0034 - val_loss: 0.0056\n",
      "Epoch 1617/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0042 - val_loss: 0.0039\n",
      "Epoch 1618/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0030 - val_loss: 0.0042\n",
      "Epoch 1619/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0037 - val_loss: 0.0043\n",
      "Epoch 1620/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0031 - val_loss: 0.0047\n",
      "Epoch 1621/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0036 - val_loss: 0.0039\n",
      "Epoch 1622/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0032 - val_loss: 0.0039\n",
      "Epoch 1623/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0032 - val_loss: 0.0043\n",
      "Epoch 1624/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0032 - val_loss: 0.0050\n",
      "Epoch 1625/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 1626/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0032 - val_loss: 0.0044\n",
      "Epoch 1627/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0038 - val_loss: 0.0047\n",
      "Epoch 1628/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0034 - val_loss: 0.0045\n",
      "Epoch 1629/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0033 - val_loss: 0.0040\n",
      "Epoch 1630/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0033 - val_loss: 0.0038\n",
      "Epoch 1631/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0032 - val_loss: 0.0053\n",
      "Epoch 1632/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0036 - val_loss: 0.0045\n",
      "Epoch 1633/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0031 - val_loss: 0.0040\n",
      "Epoch 1634/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0034 - val_loss: 0.0040\n",
      "Epoch 1635/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0031 - val_loss: 0.0048\n",
      "Epoch 1636/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0032 - val_loss: 0.0045\n",
      "Epoch 1637/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0031 - val_loss: 0.0040\n",
      "Epoch 1638/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0033 - val_loss: 0.0040\n",
      "Epoch 1639/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0029 - val_loss: 0.0043\n",
      "Epoch 1640/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0031 - val_loss: 0.0041\n",
      "Epoch 1641/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0029 - val_loss: 0.0039\n",
      "Epoch 1642/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0030 - val_loss: 0.0039\n",
      "Epoch 1643/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0030 - val_loss: 0.0042\n",
      "Epoch 1644/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0030 - val_loss: 0.0039\n",
      "Epoch 1645/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0030 - val_loss: 0.0038\n",
      "Epoch 1646/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0029 - val_loss: 0.0038\n",
      "Epoch 1647/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0029 - val_loss: 0.0039\n",
      "Epoch 1648/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0029 - val_loss: 0.0038\n",
      "Epoch 1649/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0032 - val_loss: 0.0038\n",
      "Epoch 1650/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0034 - val_loss: 0.0043\n",
      "Epoch 1651/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0030 - val_loss: 0.0044\n",
      "Epoch 1652/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0032 - val_loss: 0.0044\n",
      "Epoch 1653/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 1654/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0030 - val_loss: 0.0046\n",
      "Epoch 1655/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0032 - val_loss: 0.0040\n",
      "Epoch 1656/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0031 - val_loss: 0.0039\n",
      "Epoch 1657/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0032 - val_loss: 0.0041\n",
      "Epoch 1658/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0032 - val_loss: 0.0043\n",
      "Epoch 1659/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0030 - val_loss: 0.0041\n",
      "Epoch 1660/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0031 - val_loss: 0.0040\n",
      "Epoch 1661/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0032 - val_loss: 0.0050\n",
      "Epoch 1662/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0034 - val_loss: 0.0043\n",
      "Epoch 1663/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0032 - val_loss: 0.0039\n",
      "Epoch 1664/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0037 - val_loss: 0.0039\n",
      "Epoch 1665/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0029 - val_loss: 0.0052\n",
      "Epoch 1666/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0039 - val_loss: 0.0038\n",
      "Epoch 1667/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0031 - val_loss: 0.0040\n",
      "Epoch 1668/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0036 - val_loss: 0.0046\n",
      "Epoch 1669/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0033 - val_loss: 0.0044\n",
      "Epoch 1670/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0033 - val_loss: 0.0038\n",
      "Epoch 1671/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0036 - val_loss: 0.0038\n",
      "Epoch 1672/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0031 - val_loss: 0.0050\n",
      "Epoch 1673/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0036 - val_loss: 0.0041\n",
      "Epoch 1674/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0030 - val_loss: 0.0038\n",
      "Epoch 1675/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0033 - val_loss: 0.0039\n",
      "Epoch 1676/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0030 - val_loss: 0.0046\n",
      "Epoch 1677/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0036 - val_loss: 0.0039\n",
      "Epoch 1678/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0031 - val_loss: 0.0039\n",
      "Epoch 1679/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0030 - val_loss: 0.0042\n",
      "Epoch 1680/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0029 - val_loss: 0.0050\n",
      "Epoch 1681/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0035 - val_loss: 0.0041\n",
      "Epoch 1682/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0031 - val_loss: 0.0043\n",
      "Epoch 1683/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0038 - val_loss: 0.0045\n",
      "Epoch 1684/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0034 - val_loss: 0.0045\n",
      "Epoch 1685/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0031 - val_loss: 0.0039\n",
      "Epoch 1686/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0034 - val_loss: 0.0040\n",
      "Epoch 1687/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0029 - val_loss: 0.0047\n",
      "Epoch 1688/2000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0034 - val_loss: 0.0040\n",
      "Epoch 1689/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0030 - val_loss: 0.0039\n",
      "Epoch 1690/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0035 - val_loss: 0.0044\n",
      "Epoch 1691/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0031 - val_loss: 0.0048\n",
      "Epoch 1692/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0036 - val_loss: 0.0038\n",
      "Epoch 1693/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0031 - val_loss: 0.0039\n",
      "Epoch 1694/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0032 - val_loss: 0.0045\n",
      "Epoch 1695/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0031 - val_loss: 0.0043\n",
      "Epoch 1696/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0032 - val_loss: 0.0040\n",
      "Epoch 1697/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0032 - val_loss: 0.0040\n",
      "Epoch 1698/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0030 - val_loss: 0.0045\n",
      "Epoch 1699/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0030 - val_loss: 0.0043\n",
      "Epoch 1700/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0030 - val_loss: 0.0041\n",
      "Epoch 1701/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0031 - val_loss: 0.0039\n",
      "Epoch 1702/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0030 - val_loss: 0.0048\n",
      "Epoch 1703/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 1704/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0030 - val_loss: 0.0040\n",
      "Epoch 1705/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 1706/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0030 - val_loss: 0.0045\n",
      "Epoch 1707/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0030 - val_loss: 0.0044\n",
      "Epoch 1708/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0030 - val_loss: 0.0040\n",
      "Epoch 1709/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0029 - val_loss: 0.0039\n",
      "Epoch 1710/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0031 - val_loss: 0.0045\n",
      "Epoch 1711/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0030 - val_loss: 0.0041\n",
      "Epoch 1712/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0029 - val_loss: 0.0038\n",
      "Epoch 1713/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0030 - val_loss: 0.0038\n",
      "Epoch 1714/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0030 - val_loss: 0.0045\n",
      "Epoch 1715/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0032 - val_loss: 0.0040\n",
      "Epoch 1716/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0029 - val_loss: 0.0038\n",
      "Epoch 1717/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0032 - val_loss: 0.0040\n",
      "Epoch 1718/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0030 - val_loss: 0.0041\n",
      "Epoch 1719/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0031 - val_loss: 0.0044\n",
      "Epoch 1720/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0032 - val_loss: 0.0038\n",
      "Epoch 1721/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0032 - val_loss: 0.0040\n",
      "Epoch 1722/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0029 - val_loss: 0.0047\n",
      "Epoch 1723/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0033 - val_loss: 0.0039\n",
      "Epoch 1724/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0029 - val_loss: 0.0038\n",
      "Epoch 1725/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0032 - val_loss: 0.0045\n",
      "Epoch 1726/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0033 - val_loss: 0.0041\n",
      "Epoch 1727/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0029 - val_loss: 0.0038\n",
      "Epoch 1728/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0035 - val_loss: 0.0038\n",
      "Epoch 1729/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0027 - val_loss: 0.0044\n",
      "Epoch 1730/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0031 - val_loss: 0.0038\n",
      "Epoch 1731/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0033 - val_loss: 0.0040\n",
      "Epoch 1732/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0028 - val_loss: 0.0039\n",
      "Epoch 1733/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0029 - val_loss: 0.0038\n",
      "Epoch 1734/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0032 - val_loss: 0.0044\n",
      "Epoch 1735/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0032 - val_loss: 0.0040\n",
      "Epoch 1736/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0028 - val_loss: 0.0039\n",
      "Epoch 1737/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0031 - val_loss: 0.0043\n",
      "Epoch 1738/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0029 - val_loss: 0.0044\n",
      "Epoch 1739/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0029 - val_loss: 0.0039\n",
      "Epoch 1740/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0029 - val_loss: 0.0039\n",
      "Epoch 1741/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0031 - val_loss: 0.0044\n",
      "Epoch 1742/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0029 - val_loss: 0.0042\n",
      "Epoch 1743/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0029 - val_loss: 0.0039\n",
      "Epoch 1744/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0030 - val_loss: 0.0040\n",
      "Epoch 1745/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0029 - val_loss: 0.0047\n",
      "Epoch 1746/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0031 - val_loss: 0.0047\n",
      "Epoch 1747/2000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0031 - val_loss: 0.0040\n",
      "Epoch 1748/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0031 - val_loss: 0.0039\n",
      "Epoch 1749/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0028 - val_loss: 0.0050\n",
      "Epoch 1750/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0031 - val_loss: 0.0044\n",
      "Epoch 1751/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0030 - val_loss: 0.0040\n",
      "Epoch 1752/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0034 - val_loss: 0.0039\n",
      "Epoch 1753/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0031 - val_loss: 0.0057\n",
      "Epoch 1754/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0036 - val_loss: 0.0040\n",
      "Epoch 1755/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0028 - val_loss: 0.0043\n",
      "Epoch 1756/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0038 - val_loss: 0.0043\n",
      "Epoch 1757/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0030 - val_loss: 0.0051\n",
      "Epoch 1758/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0037 - val_loss: 0.0040\n",
      "Epoch 1759/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0033 - val_loss: 0.0039\n",
      "Epoch 1760/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0029 - val_loss: 0.0045\n",
      "Epoch 1761/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0031 - val_loss: 0.0044\n",
      "Epoch 1762/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0030 - val_loss: 0.0040\n",
      "Epoch 1763/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0027 - val_loss: 0.0039\n",
      "Epoch 1764/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0030 - val_loss: 0.0040\n",
      "Epoch 1765/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0029 - val_loss: 0.0042\n",
      "Epoch 1766/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0031 - val_loss: 0.0040\n",
      "Epoch 1767/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0030 - val_loss: 0.0038\n",
      "Epoch 1768/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0030 - val_loss: 0.0039\n",
      "Epoch 1769/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0028 - val_loss: 0.0041\n",
      "Epoch 1770/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0029 - val_loss: 0.0039\n",
      "Epoch 1771/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0028 - val_loss: 0.0037\n",
      "Epoch 1772/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0029 - val_loss: 0.0037\n",
      "Epoch 1773/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0030 - val_loss: 0.0040\n",
      "Epoch 1774/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0029 - val_loss: 0.0039\n",
      "Epoch 1775/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0028 - val_loss: 0.0038\n",
      "Epoch 1776/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0029 - val_loss: 0.0041\n",
      "Epoch 1777/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0029 - val_loss: 0.0043\n",
      "Epoch 1778/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0029 - val_loss: 0.0039\n",
      "Epoch 1779/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0031 - val_loss: 0.0041\n",
      "Epoch 1780/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0030 - val_loss: 0.0051\n",
      "Epoch 1781/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0032 - val_loss: 0.0040\n",
      "Epoch 1782/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0028 - val_loss: 0.0039\n",
      "Epoch 1783/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0040 - val_loss: 0.0044\n",
      "Epoch 1784/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0030 - val_loss: 0.0061\n",
      "Epoch 1785/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0043 - val_loss: 0.0038\n",
      "Epoch 1786/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 1787/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0035 - val_loss: 0.0048\n",
      "Epoch 1788/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0033 - val_loss: 0.0047\n",
      "Epoch 1789/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0030 - val_loss: 0.0039\n",
      "Epoch 1790/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0033 - val_loss: 0.0038\n",
      "Epoch 1791/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0033 - val_loss: 0.0050\n",
      "Epoch 1792/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0031 - val_loss: 0.0045\n",
      "Epoch 1793/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0031 - val_loss: 0.0038\n",
      "Epoch 1794/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0033 - val_loss: 0.0038\n",
      "Epoch 1795/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0030 - val_loss: 0.0050\n",
      "Epoch 1796/2000\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0032 - val_loss: 0.0044\n",
      "Epoch 1797/2000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0030 - val_loss: 0.0037\n",
      "Epoch 1798/2000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0029 - val_loss: 0.0037\n",
      "Epoch 1799/2000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0031 - val_loss: 0.0043\n",
      "Epoch 1800/2000\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0031 - val_loss: 0.0041\n",
      "Epoch 1801/2000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0029 - val_loss: 0.0038\n",
      "Epoch 1802/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0029 - val_loss: 0.0039\n",
      "Epoch 1803/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0028 - val_loss: 0.0047\n",
      "Epoch 1804/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0032 - val_loss: 0.0041\n",
      "Epoch 1805/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0029 - val_loss: 0.0040\n",
      "Epoch 1806/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0032 - val_loss: 0.0041\n",
      "Epoch 1807/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0029 - val_loss: 0.0056\n",
      "Epoch 1808/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0038 - val_loss: 0.0039\n",
      "Epoch 1809/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0029 - val_loss: 0.0042\n",
      "Epoch 1810/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0037 - val_loss: 0.0045\n",
      "Epoch 1811/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0030 - val_loss: 0.0049\n",
      "Epoch 1812/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0034 - val_loss: 0.0040\n",
      "Epoch 1813/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0031 - val_loss: 0.0039\n",
      "Epoch 1814/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0029 - val_loss: 0.0041\n",
      "Epoch 1815/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0029 - val_loss: 0.0042\n",
      "Epoch 1816/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0031 - val_loss: 0.0037\n",
      "Epoch 1817/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0029 - val_loss: 0.0037\n",
      "Epoch 1818/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0028 - val_loss: 0.0040\n",
      "Epoch 1819/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0029 - val_loss: 0.0040\n",
      "Epoch 1820/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0028 - val_loss: 0.0040\n",
      "Epoch 1821/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0028 - val_loss: 0.0039\n",
      "Epoch 1822/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0030 - val_loss: 0.0042\n",
      "Epoch 1823/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0029 - val_loss: 0.0038\n",
      "Epoch 1824/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0027 - val_loss: 0.0038\n",
      "Epoch 1825/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0029 - val_loss: 0.0040\n",
      "Epoch 1826/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0028 - val_loss: 0.0041\n",
      "Epoch 1827/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0028 - val_loss: 0.0038\n",
      "Epoch 1828/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0028 - val_loss: 0.0040\n",
      "Epoch 1829/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0029 - val_loss: 0.0044\n",
      "Epoch 1830/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0030 - val_loss: 0.0039\n",
      "Epoch 1831/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0030 - val_loss: 0.0039\n",
      "Epoch 1832/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0029 - val_loss: 0.0042\n",
      "Epoch 1833/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0029 - val_loss: 0.0038\n",
      "Epoch 1834/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0029 - val_loss: 0.0037\n",
      "Epoch 1835/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0028 - val_loss: 0.0037\n",
      "Epoch 1836/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0029 - val_loss: 0.0046\n",
      "Epoch 1837/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0032 - val_loss: 0.0038\n",
      "Epoch 1838/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0030 - val_loss: 0.0040\n",
      "Epoch 1839/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0031 - val_loss: 0.0042\n",
      "Epoch 1840/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0030 - val_loss: 0.0044\n",
      "Epoch 1841/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0030 - val_loss: 0.0038\n",
      "Epoch 1842/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0028 - val_loss: 0.0037\n",
      "Epoch 1843/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0031 - val_loss: 0.0040\n",
      "Epoch 1844/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0030 - val_loss: 0.0046\n",
      "Epoch 1845/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0031 - val_loss: 0.0040\n",
      "Epoch 1846/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0028 - val_loss: 0.0040\n",
      "Epoch 1847/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0030 - val_loss: 0.0041\n",
      "Epoch 1848/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0029 - val_loss: 0.0042\n",
      "Epoch 1849/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0029 - val_loss: 0.0040\n",
      "Epoch 1850/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0029 - val_loss: 0.0039\n",
      "Epoch 1851/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0028 - val_loss: 0.0038\n",
      "Epoch 1852/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0031 - val_loss: 0.0041\n",
      "Epoch 1853/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0030 - val_loss: 0.0040\n",
      "Epoch 1854/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0028 - val_loss: 0.0040\n",
      "Epoch 1855/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0031 - val_loss: 0.0040\n",
      "Epoch 1856/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0028 - val_loss: 0.0041\n",
      "Epoch 1857/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0030 - val_loss: 0.0038\n",
      "Epoch 1858/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0030 - val_loss: 0.0039\n",
      "Epoch 1859/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0028 - val_loss: 0.0038\n",
      "Epoch 1860/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0027 - val_loss: 0.0038\n",
      "Epoch 1861/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0028 - val_loss: 0.0039\n",
      "Epoch 1862/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0029 - val_loss: 0.0039\n",
      "Epoch 1863/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0029 - val_loss: 0.0037\n",
      "Epoch 1864/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0027 - val_loss: 0.0036\n",
      "Epoch 1865/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0028 - val_loss: 0.0038\n",
      "Epoch 1866/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0031 - val_loss: 0.0037\n",
      "Epoch 1867/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0028 - val_loss: 0.0037\n",
      "Epoch 1868/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0029 - val_loss: 0.0039\n",
      "Epoch 1869/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0029 - val_loss: 0.0036\n",
      "Epoch 1870/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0029 - val_loss: 0.0037\n",
      "Epoch 1871/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0027 - val_loss: 0.0041\n",
      "Epoch 1872/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0028 - val_loss: 0.0040\n",
      "Epoch 1873/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0027 - val_loss: 0.0038\n",
      "Epoch 1874/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0031 - val_loss: 0.0040\n",
      "Epoch 1875/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0029 - val_loss: 0.0046\n",
      "Epoch 1876/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0032 - val_loss: 0.0039\n",
      "Epoch 1877/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0028 - val_loss: 0.0039\n",
      "Epoch 1878/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0030 - val_loss: 0.0048\n",
      "Epoch 1879/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0032 - val_loss: 0.0039\n",
      "Epoch 1880/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0033 - val_loss: 0.0039\n",
      "Epoch 1881/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0031 - val_loss: 0.0039\n",
      "Epoch 1882/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0027 - val_loss: 0.0048\n",
      "Epoch 1883/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0031 - val_loss: 0.0040\n",
      "Epoch 1884/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0028 - val_loss: 0.0038\n",
      "Epoch 1885/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0030 - val_loss: 0.0040\n",
      "Epoch 1886/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0029 - val_loss: 0.0047\n",
      "Epoch 1887/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0030 - val_loss: 0.0038\n",
      "Epoch 1888/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0028 - val_loss: 0.0037\n",
      "Epoch 1889/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0029 - val_loss: 0.0040\n",
      "Epoch 1890/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0028 - val_loss: 0.0044\n",
      "Epoch 1891/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0032 - val_loss: 0.0039\n",
      "Epoch 1892/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0029 - val_loss: 0.0039\n",
      "Epoch 1893/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0029 - val_loss: 0.0045\n",
      "Epoch 1894/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0029 - val_loss: 0.0041\n",
      "Epoch 1895/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0029 - val_loss: 0.0040\n",
      "Epoch 1896/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0031 - val_loss: 0.0042\n",
      "Epoch 1897/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 1898/2000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0034 - val_loss: 0.0041\n",
      "Epoch 1899/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0029 - val_loss: 0.0039\n",
      "Epoch 1900/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0030 - val_loss: 0.0044\n",
      "Epoch 1901/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0030 - val_loss: 0.0042\n",
      "Epoch 1902/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0029 - val_loss: 0.0037\n",
      "Epoch 1903/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0028 - val_loss: 0.0038\n",
      "Epoch 1904/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0029 - val_loss: 0.0045\n",
      "Epoch 1905/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0030 - val_loss: 0.0039\n",
      "Epoch 1906/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0028 - val_loss: 0.0039\n",
      "Epoch 1907/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0034 - val_loss: 0.0039\n",
      "Epoch 1908/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0027 - val_loss: 0.0044\n",
      "Epoch 1909/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0030 - val_loss: 0.0037\n",
      "Epoch 1910/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0029 - val_loss: 0.0038\n",
      "Epoch 1911/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0031 - val_loss: 0.0041\n",
      "Epoch 1912/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0029 - val_loss: 0.0042\n",
      "Epoch 1913/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0030 - val_loss: 0.0037\n",
      "Epoch 1914/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0032 - val_loss: 0.0036\n",
      "Epoch 1915/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0030 - val_loss: 0.0043\n",
      "Epoch 1916/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0028 - val_loss: 0.0043\n",
      "Epoch 1917/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0029 - val_loss: 0.0038\n",
      "Epoch 1918/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0030 - val_loss: 0.0040\n",
      "Epoch 1919/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0027 - val_loss: 0.0048\n",
      "Epoch 1920/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0031 - val_loss: 0.0038\n",
      "Epoch 1921/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0029 - val_loss: 0.0037\n",
      "Epoch 1922/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0031 - val_loss: 0.0047\n",
      "Epoch 1923/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0030 - val_loss: 0.0042\n",
      "Epoch 1924/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0028 - val_loss: 0.0039\n",
      "Epoch 1925/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0032 - val_loss: 0.0039\n",
      "Epoch 1926/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0027 - val_loss: 0.0046\n",
      "Epoch 1927/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0030 - val_loss: 0.0038\n",
      "Epoch 1928/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0028 - val_loss: 0.0038\n",
      "Epoch 1929/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0029 - val_loss: 0.0041\n",
      "Epoch 1930/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0028 - val_loss: 0.0042\n",
      "Epoch 1931/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0028 - val_loss: 0.0039\n",
      "Epoch 1932/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0027 - val_loss: 0.0038\n",
      "Epoch 1933/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0030 - val_loss: 0.0040\n",
      "Epoch 1934/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0028 - val_loss: 0.0049\n",
      "Epoch 1935/2000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0029 - val_loss: 0.0039\n",
      "Epoch 1936/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0029 - val_loss: 0.0038\n",
      "Epoch 1937/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0031 - val_loss: 0.0044\n",
      "Epoch 1938/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0030 - val_loss: 0.0042\n",
      "Epoch 1939/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0029 - val_loss: 0.0039\n",
      "Epoch 1940/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0027 - val_loss: 0.0038\n",
      "Epoch 1941/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0028 - val_loss: 0.0038\n",
      "Epoch 1942/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0027 - val_loss: 0.0041\n",
      "Epoch 1943/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0029 - val_loss: 0.0039\n",
      "Epoch 1944/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0027 - val_loss: 0.0038\n",
      "Epoch 1945/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0027 - val_loss: 0.0041\n",
      "Epoch 1946/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0027 - val_loss: 0.0041\n",
      "Epoch 1947/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0028 - val_loss: 0.0038\n",
      "Epoch 1948/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0030 - val_loss: 0.0038\n",
      "Epoch 1949/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0027 - val_loss: 0.0045\n",
      "Epoch 1950/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0029 - val_loss: 0.0039\n",
      "Epoch 1951/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0029 - val_loss: 0.0038\n",
      "Epoch 1952/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0034 - val_loss: 0.0043\n",
      "Epoch 1953/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0030 - val_loss: 0.0041\n",
      "Epoch 1954/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0029 - val_loss: 0.0037\n",
      "Epoch 1955/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0027 - val_loss: 0.0037\n",
      "Epoch 1956/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0027 - val_loss: 0.0041\n",
      "Epoch 1957/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0027 - val_loss: 0.0041\n",
      "Epoch 1958/2000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0029 - val_loss: 0.0037\n",
      "Epoch 1959/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0028 - val_loss: 0.0039\n",
      "Epoch 1960/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0026 - val_loss: 0.0042\n",
      "Epoch 1961/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0030 - val_loss: 0.0037\n",
      "Epoch 1962/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0030 - val_loss: 0.0037\n",
      "Epoch 1963/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0028 - val_loss: 0.0044\n",
      "Epoch 1964/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0032 - val_loss: 0.0037\n",
      "Epoch 1965/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0028 - val_loss: 0.0037\n",
      "Epoch 1966/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0030 - val_loss: 0.0040\n",
      "Epoch 1967/2000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0028 - val_loss: 0.0043\n",
      "Epoch 1968/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0029 - val_loss: 0.0036\n",
      "Epoch 1969/2000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0029 - val_loss: 0.0038\n",
      "Epoch 1970/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0027 - val_loss: 0.0048\n",
      "Epoch 1971/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0030 - val_loss: 0.0039\n",
      "Epoch 1972/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0027 - val_loss: 0.0037\n",
      "Epoch 1973/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0031 - val_loss: 0.0042\n",
      "Epoch 1974/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0028 - val_loss: 0.0041\n",
      "Epoch 1975/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0029 - val_loss: 0.0042\n",
      "Epoch 1976/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0029 - val_loss: 0.0038\n",
      "Epoch 1977/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0031 - val_loss: 0.0043\n",
      "Epoch 1978/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0027 - val_loss: 0.0041\n",
      "Epoch 1979/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0027 - val_loss: 0.0038\n",
      "Epoch 1980/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0030 - val_loss: 0.0040\n",
      "Epoch 1981/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0027 - val_loss: 0.0043\n",
      "Epoch 1982/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0027 - val_loss: 0.0039\n",
      "Epoch 1983/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0027 - val_loss: 0.0039\n",
      "Epoch 1984/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0027 - val_loss: 0.0042\n",
      "Epoch 1985/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0028 - val_loss: 0.0043\n",
      "Epoch 1986/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0028 - val_loss: 0.0042\n",
      "Epoch 1987/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0027 - val_loss: 0.0040\n",
      "Epoch 1988/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0027 - val_loss: 0.0041\n",
      "Epoch 1989/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0027 - val_loss: 0.0039\n",
      "Epoch 1990/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0028 - val_loss: 0.0039\n",
      "Epoch 1991/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0027 - val_loss: 0.0037\n",
      "Epoch 1992/2000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0028 - val_loss: 0.0037\n",
      "Epoch 1993/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0028 - val_loss: 0.0039\n",
      "Epoch 1994/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0029 - val_loss: 0.0037\n",
      "Epoch 1995/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0026 - val_loss: 0.0039\n",
      "Epoch 1996/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0028 - val_loss: 0.0039\n",
      "Epoch 1997/2000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0028 - val_loss: 0.0038\n",
      "Epoch 1998/2000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0028 - val_loss: 0.0039\n",
      "Epoch 1999/2000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0029 - val_loss: 0.0039\n",
      "Epoch 2000/2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0027 - val_loss: 0.0039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x14b396f0f10>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpnn.fit(X_train,y_train,validation_data = (X_test,y_test),epochs = 2000,callbacks = [history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a55bf62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 64ms/step\n"
     ]
    }
   ],
   "source": [
    "train_sigle_pre_scatter = model_predict(bpnn,y,X_train,y_train)\n",
    "train_real_scatter = train_sigle_pre_scatter[0]\n",
    "train_pre_scatter = train_sigle_pre_scatter[1]\n",
    "\n",
    "\n",
    "#以下为保存数据的代码\n",
    "#train_pre_scatter = pd.DataFrame(train_pre_scatter)\n",
    "#train_pre_scatter.index = train_real_scatter.index\n",
    "#train_sigle_pre_scatter = pd.concat([train_real_scatter,train_pre_scatter],axis=1)\n",
    "#train_sigle_pre_scatter.colums = [\"DFT\",\"ML_predict\"]\n",
    "#train_sigle_pre_scatter.to_excel(r\"C:\\Users\\26482\\Desktop\\Mo2C_data_eassy\\train_sigle_pre_scatter.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ed95976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n"
     ]
    }
   ],
   "source": [
    "test_sigle_pre_scatter = model_predict(bpnn,y,X_test,y_test)\n",
    "test_real_scatter = test_sigle_pre_scatter[0]\n",
    "test_pre_scatter = test_sigle_pre_scatter[1]\n",
    "\n",
    "\n",
    "\n",
    "#以下为保存数据的代码\n",
    "#test_pre_scatter = pd.DataFrame(test_pre_scatter)\n",
    "#test_pre_scatter.index = test_real_scatter.index\n",
    "#test_sigle_pre_scatter = pd.concat([test_real_scatter,test_pre_scatter],axis=1)\n",
    "#test_sigle_pre_scatter.columns = [\"DFT\",\"ML predict\"]\n",
    "#test_sigle_pre_scatter.to_excel(r\"C:\\Users\\26482\\Desktop\\Mo2C_data_eassy\\test_sigle_pre_scatter.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72a62830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkUAAAG2CAYAAABxkPLrAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAACDUklEQVR4nO3de1xUdf748dcAA8NdQBLxAoZ4N++It5QMr6WlGKtWum5Wa5vVutV3u9fX2m9tW/tbXd1KV+2i7Yr3W0l5Jy+kmXdURPCGCgjDZYBh5vz+GGcEuQ3XGZj38/GYGM75nPP+fOYQvvmcz/l8VIqiKAghhBBCODgnW1dACCGEEMIeSFIkhBBCCIEkRUIIIYQQgCRFQgghhBCAJEVCCCGEEIAkRUIIIYQQgCRFQgghhBCAJEVCCCGEEAC42LoCTYnRaOTq1at4e3ujUqlsXR0hhBBCWEFRFHJzcwkODsbJqfL+IEmKauDq1au0a9fO1tUQQgghRC1cunSJtm3bVrpfkqIa8Pb2Bkwfqru7O9u3b2fUqFGo1epGr4ter7dZfEeNbev4jhrb1vEdNbat40tsueb1SavV0q5dO8u/45WRpKgGzLfMfHx8cHd3x8PDAx8fH5v94NoqvqPGtnV8R41t6/iOGtvW8SW2XPOGUN3QFxloLYQQQgiBJEVCCCGEEIAkRUIIIYQQgCRFQgghhBCAJEVCCCGEEIAkRUIIIYQQgCRFQgghhBCAnSZFGRkZzJ07l5CQEHx9fRk1ahQnTpyw6thPPvkElUpV7uXn50dRUZGlXE5ODi+88AI9e/YkMjKSGTNmcO3atYZqkhBCCCHsnN1N3nj9+nWGDx9OUlKSZVt8fDxRUVEcO3aM1q1bV3n8ypUrK9w+efJk3NzcANDpdIwePZrExERSUlIICgoiJCSEhIQEEhISaNWqVf01SAghhBBNgt31FL3xxhu8+eabXLlyhc2bN1sSlIyMDNavX1/lsUlJSSQlJdG5c+dyryeffNJSbtGiRRw8eJCwsDDat2+Pq6srAwcOJDk5mbfeeqshmyeEEEIIO2VXPUWFhYXMnj2biIgIAIKDg5k/fz6zZ88GwNXVtcrjv/nmG9577z1eeumlKsstWbIEMC3XYdaiRQvA1NO0cOFCm0yvLoQQQgjbsaueIo1GY0mIzPz8/AAICwvjscceq/L4lStX8uqrrxIYGMiwYcN4++23uXTpUpkyN27c4MyZM0DFSVZeXh6HDh2qSzOEEEII0QTZVVJ0t5KSEhYuXIiXlxdbtmypcnXbAwcOkJycjF6vJyMjg3379vHee+/RuXNnFi9ebCl38uRJy3sXl4o7ys6ePVt/jRBCCCFEtbKzs8nKyrJpHezq9llpn3/+OYsWLeLXX38FIDo6mh9++IFOnTpVWH7t2rWo1Wr0en2Z7Tqdjjlz5uDv709sbCyZmZmWfU5OFeeEGRkZVdYtKysLjUZDQUEBmZmZeHl5WQZxNxZzO+9ur8RuvvEdNbat4ztqbFvHl9iOdc2TkpLYvHkzRUVFpKSk0KFDh3o9v7XtUSmKotRr5HqyY8cOPv/8c/7zn/9Ytg0aNIiffvqpyuNycnI4evQocXFxLF26FJ1OB0DHjh05d+4cq1atYtq0aQAMHz6cXbt2ATBz5kxWrFgBwEcffcTLL79c7txarRZfX99y22NjY5k6dWqt2imEEEI4KqPRyNWrVy2dER4eHoSGhlY7hrimCgoKmDZtGjk5OWXGE9/NbnuKHnjgAR544AHGjBnDrFmzUBSF/fv3o9Vqq2yQr68vw4cPZ/jw4cybN4+JEydy7Ngxzp8/T2ZmJv7+/tXGDggIqHJ/SkoKGo2GnTt3EhUVZbOeovj4eKKjoxt9ULijxrZ1fEeNbev4jhrb1vEldvO/5rdu3WLdunWWhGjAgAEUFxczatSoeo+t1WqtKme3SZHZzJkz2bdvH0uXLgVq1qUXGhrKli1bCA8Pp7CwEDc3N3r27GnZbzAYKjyuW7duVZ7X398fd3d3PDw8CAgIsOmTamq12mbxHTW2reM7amxbx3fU2LaOL7Fto6Hjnzx5kk2bNlFUVIS7uzuPPPIIHTp0YOvWrQ0S29rz2fVAazPz7a7Q0NBqe3Hu1rZtW6KioujVqxdeXl4EBwcTHh4OmKYAuJunpyf9+vWre6WFEEIIUUZJSQlbtmwhLi6OoqIi2rVrxzPPPFPpeOHGZldJ0blz55g/fz4XL14ss71ly5YAvPrqqwBs27aNoKAgwsLCOHLkCADJycl88MEHHD58uNx5XVxceOONNyzfm+c9Kj2g2pwgTZ8+XeYoEkIIIepZZmYmS5Ys4eeffwZg6NChzJw5s8KxurZiV0nR+++/z5tvvknfvn0tA6wVReGf//wnzz//PM8++yxgmpH6+vXrXLhwgeXLlwPwwQcf8PrrrzNgwADmzZtHQUEBAImJiURGRhITE2OJM3fuXCIjI0lNTSUtLQ2j0cjhw4cJDQ1l/vz5jdtoIYQQopk7fvw4n3/+OdevX8fDw4Pp06czcuTISp8CtxW7GlP06quvkpGRwcGDB3n88cf59NNPGTx4MI888ghjx461lJszZw6JiYl4eHgwc+ZMAN555x2Ki4v58ccfWbhwIdu3b+eJJ55g+PDhvPbaa2XiuLm5sX37dl555RVGjhyJn58fERERfPzxxwQGBjZmk4UQQohmS6/Xs23bNn755RfANAxm0qRJVc47aEt2lRR17dqVzZs3V1tu7NixpKenl9nWrl07vvrqK6tjeXt7l5nUUQghhBD15+bNm8TFxXHjxg0A7r//foYPH253vUOl2VVSJIQQQoim7+jRo2zduhW9Xo+npyeTJk3i3nvvtXW1qiVJkRBCCCHqRXFxMVu3brWsRtGhQwcmTZqEl5eXjWtmHUmKhBBCCFFnN27cYPXq1WRkZKBSqRgxYgRDhw6169tld5OkSAghhBC1pigKv/zyC9u2baOkpARvb28mTZpEaGioratWY5IUCSGEEKJWioqK2LJlC8ePHwdM64w+8sgjeHp62rhmtSNJkRBCCCFqLD09nbi4ODIzM1GpVDzwwAMMGTIElUpl66rVmiRFQgghhLCaoij8/PPPfP/99xgMBnx8fJg8eTLt27e3ddXqTJIiIYQQQlilsLCQzZs3c/LkSQA6derExIkT8fDwsHHN6ockRUIIIYSo1tWrV4mLi+PWrVs4OTnx4IMPEhkZ2aRvl91NkiIhhBBCVEpRFA4dOsT27dsxGo34+voSExND27ZtbV21eidJkRBCCCEqpNPp2LhxI2fOnAGgS5cuTJgwAXd3dxvXrGFIUiSEEEKIci5fvkxcXBw5OTk4OzsTHR1NREREs7pddjdJioQQQghhoSgKBw8eZOfOnRiNRvz8/IiJiSE4ONjWVWtwkhQJIYQQAoCCggJSUlIsa5d169aNhx9+GI1GY+OaNQ5JimohKysLjUZDQUEBmZmZeHl54ebm1qh10Ov1Zb5K7OYf31Fj2zq+o8a2dXyJ3fixL126xPr168nNzbXcLuvTpw8qlapR6tOQbbf2nCpFUZR6j95MabVafH19y22PjY1l6tSpNqiREEIIUTeKonDjxg2uXbsGgJubGyEhIc1m7iEw9YBNmzaNnJwcfHx8Ki0nPUW1kJKSgkajYefOnURFRdmspyg+Pp7o6GjUarXEdoD4jhrb1vEdNbat40vsxomdn5/Ppk2bLAlR165dcXFx4e23x/Dgg2o+/BAaa1x1Q7Zdq9VaVU6Solrw9/fH3d0dDw8PAgICbPLLykytVtssvqPGtnV8R41t6/iOGtvW8SV2w7l48SJr164lNzcXFxcXxo4dS0lJD65d24aHh5rdu9UcPw79+jVoNcppiLZbez6neo0qhBBCCLtmNBrZvXs3X375Jbm5ubRs2ZLZs2fTp09fVq40dQsFB4NOBytWgCMNspGeIiGEEMJB5OXlsXbtWlJSUgDo1asX48aNw9XVlcOH4cABiIoy3TILDIR9++DIkcbvLbIVSYqEEEIIB3DhwgXWrl1Lfn4+arWacePG0bt3b8DUG7R8ORQW3inv4wM3b5p6i/r2bbyxRbYkSZEQQgjRjJlvl+3ZsweAe+65h5iYGAIDAy1ljhyBhARo2fLOcY7YWyRJkRBCCNFM5ebmsmbNGlJTUwHo06cPY8eOLTPw2NxLpNOBt3fZ4x2tt0iSIiGEEKIZOn/+POvWraOgoABXV1ceeughevbsWa6cuZcoMLB80uNovUWSFAkhhBDNiNFoZMeOHSQkJADQqlUrpkyZQkBAQLmypXuJKlvazJF6iyQpEkIIIZqJnJwc1qxZw6VLlwDo378/o0ePxsWl4n/uq+olMnOk3iJJioQQQohm4OzZs6xfvx6dToebmxsPP/ww3bt3r7S8uZdIqwU/PygoAFdX0z6dDoqL75RVq03lmntvkSRFQgghRBNmMBj48ccf2b9/PwCtW7cmJiYGf3//Ko/LyoKTJ02Dq7OyTNvMK1bdugVFRWXLe3vDiROmshXciWsWJCkSQgghmqjs7Gzi4uK4cuUKABEREURHR1d6u6y0gAD46itTD5GZwQBJSaYeJGfn8sd4eDTfhAgkKRJCCCGapDNnzrBhwwYKCwvRaDRMnDiRLl261OgcbdqU/V6vNyVFYWGmW2aORpIiIYQQogkpKSnhhx9+4ODBgwC0adOGmJgYWrRoYduKNQN2uSBsRkYGc+fOJSQkBF9fX0aNGsWJEyesOjYxMZEHH3yQiIgI+vfvz4IFC1AqWM0uJyeHF154gZ49exIZGcmMGTO4du1afTdFCCGEqDe3bt3i3//+tyUhGjRoEL/97W8lIaondtdTdP36dYYPH05SUpJlW3x8PFFRURw7dozWrVtXemxCQgLR0dEMGTKEQ4cOsWnTJiZMmEBSUhILFy60lNPpdIwePZrExERSUlIICgoiJCSEhIQEEhISaNWqVYO2UQghhKipU6dOsXHjRoqKinB3d+eRRx6hU6dOtq5Ws2J3PUVvvPEGb775JleuXGHz5s2WBCUjI4P169dXepyiKMyZMwedTkdUVBQAw4YNA+Cf//ynZVQ+wKJFizh48CBhYWG0b98eV1dXBg4cSHJyMm+99VbDNU4IIYSooZKSErZs2cLq1aspKiqiXbt2PPPMM5IQNQC7SooKCwuZPXs206dPJzg4mPHjxzN//nzLflfzBAoVOHToEMeOHQPAx8cHoEx34vLlyy3vlyxZUqZc6bIrV65Er9fXtSlCCCFEnWVmZrJ06VJ+/vlnAIYMGcKMGTPw9fW1cc2aJ7u6fabRaIiIiCizzc/PD4CwsDAee+yxSo/dvXu35X1FydOOHTsAuHHjBmfOnKm0XF5eHocOHWLIkCE1b4AQQghRT44fP87mzZspLi7Gw8ODRx99lI4dO9q6Ws2aXSVFdyspKWHhwoV4eXmxZcsWvO9evreUkydPWt5XND9DcnIyBoOh2nJgmhVUkiIhhBC2YDQa2bp1K0ePHgUgJCSESZMmlbm7IRqG3SZFn3/+OYsWLeLXX38FIDo6mh9++KHSe6iZmZmW905O5e8KKopCVlZWteXANH6pKllZWWg0GgoKCsjMzMTLyws38zSgjcR8i88Wt/ocNbat4ztqbFvHd9TYto7vqLHT09M5e/YshYWFgOl22bBhw3BycmqU+jTXz93ac9ptUtSxY0e6dOliSYouXbrEzJkz+emnnyosb22D6+PD7tChQ5nvY2NjmTp1ap3PWxvx8fE2ievIsW0d31Fj2zq+o8a2dXxHip2VlcXly5cxGo24uLgQEhJCfn4+3333XaPWA5rf515QetruKthtUvTAAw/wwAMPMGbMGGbNmoWiKOzfvx+tVlthF2J1a7yoVCr8/PyqLQcQUM0c5ikpKWg0Gnbu3ElUVJTNeori4+OJjo5G3cjTjjpqbFvHd9TYto7vqLFtHd+RYhcXF7N9+3bS0tIA8PLy4oknnrCMqW1MzfVz12q1VpWz26TIbObMmezbt4+lS5cClff09OjRw/LeYDCU29+pUydcXFzo2bNnleUAunXrVmWd/P39cXd3x8PDg4CAAJv8sjJTq9U2i++osW0d31Fj2zq+o8a2dfzmHvvGjRusXr2ajIwMVCoVw4YNIycnBz8/P7nm9XxOa9jVI/mVmTZtGgChoaGV9uKY5yYCLPdiSxs+fDgAwcHBhIeHV1rO09OTfv361bnOQgghRGUUReHIkSN88cUXZGRk4OXlxZNPPom391BUKhW3Z5gRjcyukqJz584xf/58Ll68WGZ7y5YtAXj11VcB2LZtG0FBQYSFhXHkyBHANNV59+7dgTsDpUsnPTNmzLC8nz17dplypctOnz7dptm5EEKI5q2oqIh169axadMmSkpKCAsL49lnnyUkJJSvvzaV+fprqGCFKtHA7Copev/993nzzTfp27cv//nPfwBTNv3Pf/6T559/nmeffRYwzUh9/fp1Lly4YJmUUaVSsXTpUjQaDXv37gVMEzoCPPfccwwePNgSZ+7cuURGRpKamkpaWhpGo5HDhw8TGhpaZrJIIYQQoj6lp6fzxRdfcPz4cVQqFSNHjmT69Ol4enpy5AgcOGAqt38/3P6bXzQiuxpT9Oqrr5KRkcHBgwd5/PHH+fTTTxk8eDCPPPIIY8eOtZSbM2cOiYmJeHh4MHPmTMv2gQMHsnv3bv70pz/Ru3dvnJyc+OSTT3jxxRfLxHFzc2P79u288sorjBw5Ej8/PyIiIvj4448JDAxspNYKIYRwFIqicPjwYb777jsMBgM+Pj5MnjyZ9u3b394Py5eD+QZHYSGsWAF9+4JKZbt6Oxq7Soq6du3K5s2bqy03duxY0tPTK9wXERHBnj17qj2Ht7c3ixcvrnEdhRBCiJooKipi06ZNlsmDw8PDeeSRR/Dw8LCUOXIEEhLg9mgRWraEfftM22WYa+Oxq6RICCGEaE6uXr1KXFwct27dwsnJiZEjRzJo0CBUpbp/zL1EOh2YF27w9oYrV6S3qLFJUiSEEELUM0VROHToEPHx8RgMBnx9fYmJiaFt27blypp7iQID7yQ/KpXpe+ktalySFAkhhBD1SKfTsXHjRsvi4126dGHChAm4u7uXK1u6lyg4uOw+Hx+4eVN6ixqTJEVCCCFEPbly5QpxcXFkZ2fj5OTEqFGjiIiIKHO7rLSKeonMpLeo8UlSJIQQQtSRoigcOHCAH374AaPRiJ+fHzExMQTf3f1T5pjKe4nMpLeocUlSJIQQQlgpN/fOYGgznU7H+vXrOXv2LGBaKurhhx9Go9FUea6qeonMpLeocdnV5I1CCCGEvTpyBKZMgV9+ubPt0qVL/Otf/+Ls2bM4Ozszbtw4YmJiqk2IzL1EWi2o1VBQYHrpdKb9Ot2dbWq1qdyKFTLLdUOTniIhhBCiGooCy5bBqVOmr716Kfz0UwI7duxAURT8/f2ZMmUKQUFBVp0vKwtOnjT1OmVl3dnu5mb6eusWFBXd2e7tDSdOmMpWsgSoqAeSFAkhhBDVMN/q8veHQ4fy+de/1nPz5nkAevTowUMPPYSbOaOxQkAAfPWVqSeoNIMBkpJMvUjOzmX3eXhIQtTQJCkSQgghqlB6QHSvXqmEhKzh5s1cXFxcGDt2LH369Kn06bKqtGlTfpteb0qKwsJMt81E45KkSAghhKiCqZfIyIAB+7j33l2oVAq5uS0ZPz6Gvn1b2bp6oh5JUiSEEEJUQlFgxYo8Bg5cR1DQBQAyMnqxa9c4iotdGTFCHpNvTiQpEkIIISqxfXsKnp5r0GjyMRjUpKWNIzOzN35+8ph8cySP5AshhBB3URSF3bv3sH//l2g0+RQUBHL69GwyM3sDpkkVdTp5TL65kZ4iIYQQopTc3FySk5PJy8tDpYIrV/pw/fpYjMY7I59lUsXmSZKiWsjKykKj0VBQUEBmZiZeXl41ehSzPuj1+jJfJXbzj++osW0d31Fj2zq+rWJfuHCBjRs3UlBQgKKo+fnncajV3XF2BmfnsnUJCDBNqvj119CzZ/2MLZJr3jCxrT2nSlGk489aWq0WX1/fcttjY2OZOnWqDWokhBCiPiiKwrVr17hx4wYAGo2G0NDQamemFk1DQUEB06ZNIycnBx8fn0rLSU9RLaSkpKDRaNi5cydRUVE26ymKj48nOjoadSNPZuGosW0d31Fj2zq+o8a2dfzGjK3Valm/fr0lIerVqxeKovC//zuaVq2qj52WBqNGwYcf1r23SK55w8TWarVWlZOkqBb8/f1xd3fHw8ODgIAAm/zgmqnVapvFd9TYto7vqLFtHd9RY9s6fkPHPnfuHOvWrUOn0+Hq6sqECRNo2bITBw5sxdVVTXp69bFdXeHXX02LxdbXjNNyzes3trXnk6RICCGEwzEYDOzYsYOffvoJgNatWxMTE4O/v79l/ElFS21URpbgaB4kKRJCCOFQsrOzWbNmDZcvXwYgIiKC6OhoXFzK/pMoS204HkmKhBBCOIwzZ86wYcMGCgsLcXNzY+LEiXTt2tXW1RJ2QpIiIYQQzZ7BYCA+Pp6DBw8C0KZNGyZPnoyfn5+NaybsiSRFQgghmrVbt24RFxfH1atXARg0aBAjR47E2doBQ8JhSFIkhBCi2Tp16hQbN26kqKgId3d3Jk6cSHBwZ6sHUAvHImufCSGEaHZKSkrYsmULq1evpqioiHbt2vHMM8+Qn9+ZKVPgl19sXUNhj6SnSAghRLOSmZlJXFwc6enpAAwZMoSoqCicnJxZtgxOnYJly6B37/pZmkM0H5IUCSGEaDZOnDjBpk2bKC4uxsPDg0cffZSOHTsCcPgwJCSAv78s5CoqJkmREEKIJk+v1/Pdd99x5MgRAEJCQpg0aZJlnStFMU3GqNOZ5h9KToYVK6BvX+ktEndIUiSEEKJJy8jIYPXq1Za1y4YNG8aIESNwcrozbPbIEVMvUWCgKQkKDJTeIlGeJEVCCCGarF9//ZUtW7ag1+vx9PRk0qRJ3HvvvWXKlO4lCg42bfPxgZs3pbdIlCVJkRBCiCanuLiYbdu2cfToUQA6dOjAo48+ire3d7myd/cSgfQWiYrZ5SP5Fy5cYNq0abRu3RofHx/GjBnDL1Y+P/nJJ5+gUqnKvfz8/CgqKrKUy8nJ4YUXXqBnz55ERkYyY8YMrl271lBNEkIIUU9u3LjBkiVLOHr0KCqVihEjRvD4449XmBCV7iW6PbzIwsfHtH3FClM5Ieyupyg5OZlBgwZx8+ZNy7bvv/+evXv3cvDgQXr06FHl8StXrqxw++TJk3FzcwNAp9MxevRoEhMTSUlJISgoiJCQEBISEkhISKBVq1b11yAhhBD1QlEUjh49ytatWykpKcHLy4vJkycTGhpa6TEV9RKZSW+RuJvd9RS9+uqrvP766xw+fJjFixfj5eUFQEFBAe+++26VxyYlJZGUlETnzp3LvZ588klLuUWLFnHw4EHCwsJo3749rq6uDBw4kOTkZN56660GbZ8QQogaSEmB5GSKz5xh3ZdfsnHjRkpKSghr3Zpnx4wh1GAwPUqWnAy3l/Ewq6qXyEx6i0RpdtVTlJ+fz4wZM3j44YcB6Nu3LyUlJTz//PMAnD17tsrjv/nmG9577z1eeumlKsstWbIEwPKoJkCLFi0AU0/TwoULUavVtW2GEEKIuro98SJPPkm6RkPcgAFkenmhMhqJOnOGoRs2oPrXv8oe4+0Nq1ZZRlNX1UtkJr1FojS7Soo8PT0tCZFZVFSU5X14eHiVx69cuZK0tDQ++OADunTpwgMPPMBTTz1Fu3btLGVu3LjBmTNnAHB1dS13jry8PA4dOsSQIUPq0hQhhBB1odOhKApHOnQgvlMnDE5OeBcVEXPqFO21Wrj9h2zp8uTmmr5yp5dIqwU/PygoqDyUWm0qZ34STTguu7t9dresrCzL+yeeeKLScgcOHCA5ORm9Xk9GRgb79u3jvffeo3PnzixevNhS7uTJk5b3Li4V54TV9UgJIYRoWEV6PampqXzXpQsGJyfCs7N59vRp2hsM4OlZ/uXuXub4rCw4edLUeZSVVf3L2xtOnDC9F47LrnqKKnLgwAEAhg4dWq4XqbS1a9eiVqvR6/Vltut0OubMmYO/vz+xsbFkZmZa9pWe2Ku0jIyMKuuUlZWFRqOhoKCAzMxMvLy8LIO4G4u5nXe3V2I33/iOGtvW8R01ti3jp6ens/a778jOy8PJaGREejoDb95EpVKhr2xog6ur6WUwgF6Pj4+pp6iqHqK7eXiYxhjJNW9+bbf2nCpFsd+hZUajkW7dupGVlUViYiIhISHVHpOTk8PRo0eJi4tj6dKl6G53pXbs2JFz586xatUqpk2bBsDw4cPZtWsXADNnzmTFihUAfPTRR7z88svlzq3VavH19S23PTY2lqlTp9a2mUIIITA9XZaRkcHVq1dRFAW1Wk1oaCienp62rppo4goKCpg2bRo5OTllxhPfza57ij7//HMuXbrEDz/8YFVCBODr68vw4cMZPnw48+bNY+LEiRw7dozz58+TmZmJv79/tecICAiocn9KSgoajYadO3cSFRVls56i+Ph4oqOjG31QuKPGtnV8R41t6/iOGrux4xcWFrJlyxauXLkCQMfgYDT+/oxdvty62AUFkJ0NX34JHTrUqS5yzZtf27VarVXl7DYpOn/+PG+88Qbr1q1j0KBBlu3Z2dmWJ8WqExoaypYtWwgPD6ewsBA3Nzd69uxp2W8wGCo8rlu3blWe19/fH3d3dzw8PAgICLDpk2pqtdpm8R01tq3jO2psW8d31NiNEf/KlSvExcWRnZ2Nk5MT0dHR9PXzY9uZM6j1eqyKXFxsejk7m0ZO1wO55s2n7daezy6TosLCQmbOnMmqVauIjo62bN+0aRNZWVnMmDHD6nO1bduWqKgorl69ipeXF15eXoSHh3Pu3DkKCwvLlff09KSfPJMphBANJjfXNLBZURQOHDjADz/8gNFopEWLFsTExNCmTRv08sCLsAG7fPrsD3/4A8ePH+f555+nS5cudOnShZCQECZOnEivXr3Ytm0bQUFBhIWFceTIEcA0E/YHH3zA4cOHy53PxcWFN954w/L97NmzgbIDqs0J0vTp02WOIiGEaCBHjsCUKXDwoI5vv/2W7du3YzQa6dq1K8888wxt2rSxdRWFA7O7pGjRokUsXboUrVZrmaE6KSmJtLQ0XFxc6NatG4sWLeL69etcuHCB5cuXA/DBBx/w+uuvM2DAAObNm0fB7UcOEhMTiYyMJCYmxhJj7ty5REZGkpqaSlpaGkajkcOHDxMaGsr8+fNt0WwhhGj2FAWWLYOMjEts2fIvzp49i7OzM+PGjWPKlCloNBpbV1E4OLu6fXbgwAFefPHFSvd369YNV1dX5syZQ2JiIh4eHsycOROAd955h+LiYn788UcWLlzI9u3beeKJJxg+fDivvfZamfO4ubmxfft2XnnlFUaOHImfnx8RERF8/PHHBAYGNmALhRDCcR0+rHD9+k+MH/8jTk4KHh7+PP54DK1bt7Z11YQA7CwpioyMpLi4uNpyY8eOJd08Bfxt7dq146uvvrI6lre3d5lJHYUQQjScvLx81q1bT/fu5wFITe2Bl9dDBAVV8+SuTmcaQF2d29OvCFEXdpUUCSGEaH5SU1NZtWoNrq65GAwuXLo0huTkvuTlqSpfb8w8Q3VRkXVJEZhGb981s7UQNSFJkRBCiAahKAp79+5l165dKIqCVhtAWtoUCgtb4eMDN2/eWW+s3IKtQUGmr19+aXrM3hru7pbFYIWoDUmKhBBC1Lu8vDzWrVvHhQsXALh06T6Sk8fj5WVaiNvq1ek7dKi3eYeEqI7dPX0mhBCiaUtJSeGzzz7jwoULuLi4kJs7kYSER/H0dC1TzsfHNBRoxQrTk2lC2Jr0FAkhhKgXRqORPXv2sHv3bgACAwPp1m0Kf/xjIIGB5W+RWd1bJEQjkaRICCFEneXm5rJ27VouXrwIQO/evRk7dhzz5qnR6Sof6lPt2CIhGpEkRUIIIeokOTmZtWvXUlBQgFqt5qGHHuK+++7j8GFISKDCXiIz6S0S9kSSIiGEELViNBrZuXMn+/btA6BVq1bExMTQsmVLFAWWLwetFvz8TIvYV0atNpWT3iJha5IUCSGEqDGtVsuaNWtIS0sDoF+/fowePdqydmRWFpw8aZo6KCur+vN5e8OJE6ayAQENWXMhKidJkRBCiBo5d+4c69atQ6fT4erqysMPP0yPHj3KlAkIgK++qrqH6G4eHpIQCduSpEgIIYRVDAYDO3bs4KeffgKgdevWxMTE4O/vX2F5y4L3V69atwxHIZCMaRJGWYdS2IAkRUIIIaqVk5NDXFwcly9fBiAiIoLo6GhcXKr5Z+TqVZg6FXJzrQ/m7Q1ff12H2gpRO5IUCSGEqNLZs2fZvHkzhYWFuLm5MXHiRLp27WrdwTqdKSFyc7NuXTJzeVngVdiAJEVCCCEqZDAYuHLlCkePHgUgODiYmJgY/Pz8an4yd3fw9LSubFFRzc8vRD2QpKgWsrKy0Gg0FBQUkJmZiZeXF25ubo1aB71eX+arxG7+8R01tq3jO2rs7OxsvvlmLTk5NwHT7bKoqCicnZ1rVh+DAVxdTS9r1jC7XVZvMACO97nL/2sNE9vac6oURVacsZZWq8XX17fc9tjYWKZOnWqDGgkhRP3Lzs4mLS0No9GIs7Mz7du3r/B3nxBNRUFBAdOmTSMnJwcfH59Ky0lPUS2kpKSg0WjYuXMnUVFRNuspio+PJzo62jIviMRu3vEdNbat4ztS7JKSEn788UfLUh3Z2cEMHtyCBQvG87e/qenduxYnTUmBJ5+EFi1Mz9xXp6AAsrPRL1tG/NmzDvG520tsW8dvyNhardaqcpIU1YK/vz/u7u54eHgQEBBgkx9cM7VabbP4jhrb1vEdNbat4zf32FlZWaxevZr09HQACgoGs2vXMEaM+B6tVs1XX6np378Ws007O0NxsellTRvMZZ2dgeb/udtjbFvHb4jY1p7PqV6jCiGEaHJOnDjBZ599Rnp6Oh4eHkRETCM+PpqAAFNi0rLlnbXJhGjOJCkSQggHpdfr2bRpE2vWrKG4uJj27dvz9NPPsHVrODqdabogMH3V6Uxrk8koVNGcye0zIYRwQBkZGcTFxXH9+nUAhg0bxogRI/jlF6dyK9vLSvbCUUhSJIQQDubYsWNs3rwZvV6Pp6cnjz76KGFhYZaV7XU6CA4ue4yPD9y8KSvZi+ZNkiIhhHAQer2erVu3WiZjDA0NZdKkSXjfvk925AjleonM6txbZO0M1TKTtbAhSYqEEMIB3Lhxg7i4OG7eNE3GOHz4cO6//36cnExDS6vqJTKrVW+Ru7tpUFJurvUzVXt7W7ckiBD1TJIiIYRoxhRF4ejRo2zdupWSkhK8vLyYNGkSHTp0KFOuql4is1r1FgUHw6pVNesBcnc3BRKikUlSJIQQzVRxcTFbtmzh2LFjANx7771MmjQJz7vWIDP3Emm14Odnmj8RTCtugCmfKS42vVerTeVq1FtUWddTVWy0zIVwbJIUCSFEM3T9+nVWr15NZmYmKpWKqKgohg4diqqCLCYrC06eNN21ysq6s908Uf+tW2XvfHl7w4kTprIBAQ3cECEakSRFQgjRjCiKwpEjR9i2bRsGgwFvb28mT55MSEhIpccEBMBXX93pITIzGCApydSLdHuCaQsPD0mIRPMjSZEQQjQTRUVFbN68mRMnTgDQsWNHHn30UTysWHOsTZvy2/R6U1IUFmbdCh1CNHWSFAkhRDNw7do14uLiyMrKwsnJiQceeIDBgwdXeLtMCFExSYqEEKIJUxSFxMREtm/fjsFgwNfXl8mTJ9OuXTtbV02IJkeSIiGEaKIKCwvZuHEjp0+fBqBz585MnDgRd5njR4hascsFYS9cuMC0adNo3bo1Pj4+jBkzhl9++cWqYxMTE3nwwQeJiIigf//+LFiwAKWCFQxzcnJ44YUX6NmzJ5GRkcyYMYNr167Vd1OEEKJBXLlyhc8++4zTp0/j5OTE6NGjiY2NlYRIiDqwu56i5ORkBg0aZJl1FeD7779n7969HDx4kB49elR6bEJCAtHR0QwZMoRDhw6xadMmJkyYQFJSEgsXLrSU0+l0jB49msTERFJSUggKCiIkJISEhAQSEhJo1apVg7ZRCCFqS1EUDh48SHx8PEajkRYtWhATE0ObikZKCyFqxO56il599VVef/11Dh8+zOLFi/Hy8gKgoKCAd999t9LjFEVhzpw56HQ6oqKiANOqzwD//Oc/2b9/v6XsokWLOHjwIGFhYbRv3x5XV1cGDhxIcnIyb731VgO2Tgghak+n0/Gf//yH77//HqPRSNeuXXnmmWckIRKinthVT1F+fj4zZszg4YcfBqBv376UlJTw/PPPA3D27NlKjz106JBl1lYfHx8AWrRoYdm/fPlyBg0aBMCSJUvKlCtdduXKlSxcuBC1PH8qhLAjly5dYs2aNeTk5ODs7MyoUaMYMGCAPF0mRD2yq6TI09PTkhCZmXt9AMLDwys9dvfu3Zb3rua56UvZsWMHYFoU8cyZM5WWy8vL49ChQwwZMqRmlRdCiAagKAoHDhxg165dGI1G/Pz8mDJlCq1bt7Z11YRoduwqKapIVqk555944olKy508edLy3sWlfLOSk5MxGAzVlgNTj5QkRUIIWysoKCAlJYVff/0VgO7du/Pwww/jZl5/QwhRr+w+KTpw4AAAQ4cOLdeLVFpmZqblvZNT+aFSiqKQlZVVbTmAjIyMKuuUlZWFRqOhoKCAzMxMvLy8Gv2XlP72Yol6Gyya6KixbR3fUWPbOr6tYqelpbF+/Xry8vIst8t69+6NSqVqtLo44ufuyLFtHb8hY1t7TrtOioxGI0uXLiUwMJCvv/660iQGrG9wfXzYHTp0KPN9bGwsU6dOrfN5ayM+Pt4mcR05tq3jO2psW8dvrNiKonDjxg3LFCFubm6EhoZy7do1m00b4gifu8S2n/gNEbvg7oX9KmHXSdHnn3/OpUuX+OGHH6pczBDA39+/yv0qlQo/P79qywEEVLPKYUpKChqNhp07dxIVFWWznqL4+Hiio6MbfVC4o8a2dXxHjW3r+I0ZOz8/n40bN1qSn27duuHs7MyYMWPkc5fYzT5+Q8bWarVWlbPbpOj8+fO88cYbrFu3zvLUGEB2dnaZp8rMSs9fZDAYyu3v1KkTLi4u9OzZs8pyYPpFVBV/f3/c3d3x8PAgICDApk+qqdVqm8V31Ni2ju+osW0dv6Fjp6SksHbtWvLy8nBxcWHcuHF0796dbdu2yecusR0qfkPEtvZ8dpkUFRYWMnPmTFatWkV0dLRl+6ZNm8jKymLGjBnljin9lFphYWG5/cOHDwcgODiY8PBwzp07V2E5T09P+vXrVx/NEEKIahmNRvbs2cOePXtQFIXAwEBiYmK45557bDauRAhHZXeTNwL84Q9/4Pjx4zz//PN06dKFLl26EBISwsSJE+nVqxfbtm0jKCiIsLAwjhw5AsCgQYPo3r07cGegdOmkp3QiNXv27DLlSpedPn26zFEkhGgUubm5fP311+zevRtFUejduzezZ8/mnnvusXXVhHBIdtdTtGjRIpYuXQqUvweoVqvp1q0bb775JtevXwdMkzL27dsXlUrF0qVLGTFiBHv37gVMEzoCPPfccwwePNhynrlz57J27VoOHjxIWloabdu25fDhw4SGhjJ//vzGaKYQwsElJyezbt068vPzUavVjB8/nl69etm6WkI4NLtKig4cOMCLL75Y6f5u3brh6urKnDlzSExMxMPDg5kzZ1r2Dxw4kN27d/OnP/2J3r174+TkxCeffFLunG5ubmzfvp1XXnmFkSNH4ufnR0REBB9//DGBgYEN0zghhMB0u2zXrl2WP97uuecepkyZQsuWLW1cMyGEXSVFkZGRFBcXV1tu7NixpKenV7gvIiKCPXv2VHsOb29vFi9eXOM6CiFEbWm1WtasWUNaWhoA/fr1Y/To0XLLXgg7YVdJkRBCNFfnzp1j3bp16HQ6XF1defjhh8s8NSuEsD1JioQQogEZDAZ27NjBTz/9BEBQUBAxMTHVzocmhGh89ZYUff7552zdupUWLVrQtm1b2rZty7PPPltfpxdCiCYnJyeHuLg4Ll++DMCAAQMYNWpUpesuCiFsq97+z3z66ad5/PHH+eSTT3jvvfcwGo2SFAkhHFZSUhLr16+nsLAQNzc3JkyYUO3EsEII26rXP1c8PDx44403CAkJKfNUmBBCOAqDwcAPP/xgWcw6ODiYmJgY/Pz8bFwzIUR1GmTyxieeeIKgoKCGOLUQQtitW7dusWzZMktCNHDgQGbNmiUJkRBNRIPd2L733nsb6tRCCGF3Tp8+zYYNGygqKkKj0fDII4/QuXNnW1dLCFEDVvUUffvttzU+scy7IYRwBCUlJWzdupX//ve/FBUV0bZtW5555hlJiIRogqzqKZo/fz6/+c1vanTioqKiWlVICCGagtxc0OuziIuL49q1awAMHjyYBx54AGdnZxvXTghRG1YlRadPn+Z//ud/6Nq1KyqVqsqyiqJw+fJlfv7553qpoBBC2JsjR+BvfztBt26bKCkpxt3dnUcffZTw8HBbV00IUQdWJUWKovDXv/61oesihBB2r7hYz3//+z2dOh2mpATatWtPTMxkfHx8bF01IUQdNchAa0VRqu1REkKIpiYjI4Mvv4zD3f06igLnzg1l3LgofHwa5EFeIUQjszopUhSlIeshhBB27cSJE2zbtg29Xk9hoQepqZP45ZcwvvoK+vcH+TtQiKbPqj9vPDw8OH36NAaDAaPRWO0rNzeXhx9+uKHrLoQQDU6v15OWlsbGjRvR6/XcvBlKYuKz5OaGERgI+/aZxhgJIZo+q3qKhgwZUqPHSz09PZkyZUqtK2XvsrKy0Gg0FBQUkJmZiZeXF25ubo1aB71eX+arxG7+8R01ti3j37x5k3Xr1pGVlQWATjeUffuG0qGDEyqVnoAA0Grh66+hZ8/67y1y1M9dYss1b6hzV0elWHFf7MKFCzWejLE2x9g7rVaLr69vue2xsbFMnTrVBjUSQjSUzMxMLl++jKIouLi4EBISgre3t62rJYSohYKCAqZNm0ZOTk6VD0VY1VM0ZcoUDh8+XKMKPPbYY832sfyUlBQ0Gg07d+4kKirKZj1F8fHxREdHN/pEmY4a29bxHTV2fcb/9Vd491145x24776KyxQXF/P9999z6dIlAEJCQvDy8mL//nH8+KOaDh3K9ggpCqSkQFQUfPhh/fYWNZfPXWI3jdi2jt+QsbVarVXlrEqKjh8/ztKlS+nYsaNV8xSdO3eO48ePW1WBpsjf3x93d3c8PDwICAiw6ezdarXaZvEdNbat4ztq7LrGVxRYscKUGK1YAf/v/5VPYK5fv05cXBwZGRmoVCpGjBhBZGQk27ZtIyFBjY+PmpKS8uf28YHdu+H4cejXr1bVq1JT/twldtOLbev4DRHb2vNZlRSVlJTw9NNP16lCQghhS0eOQEIC+PvfGRxtTmAUReHIkSN89913lJSU4O3tzeTJkwkJCaG42DQWobAQWras+Nw+PnDzpinZ6ttXnkQToqmq0TxF1jyWr1KpZJ4iIYRdURRYvhx0OggLg+TkOwlMcXERmzdv5sSJEwB07NiRRx55BE9PT8DUswSmhKiyX2sqFWWeRGuI3iIhRMOr98kbZT4jIYS9MfcSBQaWTWB27rzGyZNxZGVloVKpGDlyJIMHD7b8Uaco8M03pvFCajUUFFQeQ602PYkmvUVCNF1WJUUTJkxg9+7dTJ06laeeeoqAgIBKyyqKQlJSEpMmTaq3SgohRG2V7iUKDjZt8/FR8PX9mT17vkelMuDj40NMTAzt2rUrc2xWFpw5Y0qKbt2C6ta59vaGEydMx1Xxa1IIYaesSorWr1/PtWvXWLJkCdOmTWPIkCE8//zz9O7du8LyoaGh9OzZsz7rKYQQtXJ3L5GzcyEhIZvw9z8FQKtWnZgx4xHc3d3LHRsQAJ9/Dr/8YkqsnJ2rj+fhIQmREE2V1bfPWrduzZtvvskbb7zB1q1beeutt9Bqtfz+978nJiYG57t+W/z444/1XlkhhKiJu3uJPDyuEBYWh5tbNkajE0ePPkhISCQaTeX3ulq3NiVFYWGmW2RCiOarxqsYqlQqxo8fz8aNG/n66685ffo0gwcP5n//93+5fv26pZx5kKIQQtjKnV4ihVatDtCly79xc8umqKgFSUmzuHVrEPv2qWSZDiEEUIukqLS2bdvy4osvMmnSJP7yl78QEhLC448/zsGDB+urfkIIUSvmXiKDQUffvv+hffvvcXIycutWV06deob8/Db4+Jh6kVasMJUXQji2WidFhw8f5ne/+x1t2rThtddeo6ioiOLiYlauXMngwYOZNm1afdZTCCFq5MgROHXqMmPHfoafXxJGozOpqWNJTp6CwaAByj9KL4RwbDV6JL+4uJhvv/2WRYsWkZiYCJR/BD86OpoXXniBcePG1V8thRCiBoxGhf/85yeGDt2Bk5MRnc6P06enkJfXulxZeZReCGFmVVJ08eJFFi9ezL///W/LatGlkyEPDw+eeOIJ5s6dS9euXRumpkIIYYWCggL++9/1eHqeA+Dy5e4cPfowJSWVr08oj9ILIcDKpKhjx44oimKZqdqcELVv357nnnuO2bNn06JFizLHyKzWQojGlpaWRlxcHLm5uTg7O9O37ximTu1n1e8ieZReCGFVUmQ0GkvN8KowdOhQXnjhBR599FGcnCoelhQREWG5xSaEEA1JURT27dvHzp07URSFgIAAYmJiCAoKsnXVhBBNiNVjitzc3IiNjeWFF16odNJGsx07dvCrecGgOrh48SILFixg165dHD582KpjPvnkE+bNm1due4sWLUhPT8fNzdSFnpOTw1tvvcWOHTvw9PSkc+fO/N///R+tW5cfcyCEsF/5+fmsW7eO5ORkAO677z7Gjx+Pq6urdSe4etX0CFplDAbT15QU0+yN7u53psYWQjQrNZq8MSUlhT/+8Y+WbXffTjO/P3bsGAbzL5Ja2LdvH59++ikbNmzAYDAQFhZm9bErV66scPvkyZMtCZFOp2P06NEkJiaSkpJCUFAQISEhJCQkkJCQQKtWrWpddyFE47l48SJr1qwhLy8PFxcXxo0bR+/eva2/dX/1KkydCrm5lZdxdYU//xmefBKKi00DkFatksRIiGbIqqSoVatWnDlzBrWV07lev36d8PDwWlXowIEDXL58GX9//xonVklJSSQlJdG5c+dy+5588knL+0WLFnHw4EHCw8Np3749AAMHDmTDhg289dZbfPbZZ7WquxCicRiNRvbu3cvu3btRFIWWLVsyZcoU7rnnnpqdSKczJURubqYeoIqYe5xatICcHFP5qnqWhBBNllVJ0dNPP11pQpSfn8+aNWtQqVQ88cQTgCmJ+sMf/lCrCkVGRhIZGUn//v1ZsmRJjY795ptveO+993jppZeqLGc+r4+Pj2WbeaD4ypUrWbhwodUJoBCiceXl5bF27VpSUlIA6N27N2PHjrX+dllF3N2hsln4zb8LPDxMPUXVrQorhGiyrEqK3n333Ur3paenM3PmTJycnCxJEcAHH3xQt4q51GgKJcCU0KSlpfHBBx/QpUsXHnjgAZ566qkyK1/fuHGDM2fOAFT4SzQvL49Dhw4xZMiQ2ldeCNEgUlJS2LhxI/n5+ajVasaPH0+vXr1sXS0hRDNRp2U+Srt7EsfGduDAAZKTk9Hr9WRkZLBv3z7ee+89OnfuzOLFiy3lTp48aXlfWeJ19uzZBq+vEMJ6RqORa9eusWrVKvLz87nnnnt4+umnJSESQtSrmnfH2Km1a9eiVqvR6/Vltut0OubMmYO/vz+xsbFkZmZa9lU2nUBGRkaVsbKystBoNBQUFJCZmYmXl5dlEHdjMbfz7vZK7OYbv7nHzssDL6/y23Nzc1m3bp1lwenevXsTHR1d4f/vNWYwmMYMubreuU12F/3t7Xq1+k5ZgwEa4TrIz7vEdqT4DRnb2nOqlDp28SQnJxMeHo5KparTE2d3u3jxIh06dAAgLCyM8+fPW3VcTk4OR48eJS4ujqVLl6K7PSCyY8eOnDt3jlWrVlnWZRs+fDi7du0CYObMmaxYsQKAjz76iJdffrncubVaLb6+vuW2x8bGMnXq1Bq3UQhRNa1WS2pqKgaDAScnJ9q1a4efn5+tqyWEaGIKCgqYNm0aOTk5ZcYT363Z9BSZ+fr6Mnz4cIYPH868efOYOHEix44d4/z582RmZuLv71/tOQKqmdY2JSUFjUbDzp07iYqKsllPUXx8vOUvZond/OM319iKAq+8Alu3wvjx8OGHYDQa2LNnD0ePHgXgnnvuISAggIceeqh+46ekmB61b9HCNJC6Anq1mvhp04heuRJ1Tg5kZ8OXX8LtP9oakvy8S+zG1lzbrtVqrSpX56TI39+ft99+u66naRChoaFs2bKF8PBwCgsLcXNzo2fPnpb9lfVsdevWrcrz+vv74+7ujoeHBwEBATZ9Uk2tVtssvqPGtnX85hb78GHYs8eUk+zeDYcO5XD27BouXboEwIABA4iKimL79u31H9/Z2fREWXFxpbfPzNR6PWpzWWfnasvXJ/l5l9iOFL8hYlt7vjonRX5+fnabFAG0bduWqKgorl69ipeXF15eXoSHh3Pu3DkKCwvLlff09KRfv342qKkQjkdRYPly07Q/YWGg0yURH78BJycdbm5uTJgwgW7dutlsfIUQwrHU29Nn9a30L8G7e3S2bdtGUFAQYWFhHDlyBDCNbfrggw8qXA7ExcWFN954w/L97NmzgbIDqs0J0vTp02WOIiEayZEjkJAA99xjoF2777n//m9xctLh6xvMM888U22vrRBC1Ce7TYp+/vlny/v09HSuXbtm+X7RokVcv36dCxcusHz5csA0L9Lrr7/OgAEDmDdvHgUFBQAkJiYSGRlJTEyM5fi5c+cSGRlJamoqaWlpGI1GDh8+TGhoKPPnz2+cBgrh4My9RE5Ot4iIWEZQ0AEAkpIGcuHCb2nRohEHVOt0kJ9f8ev27xIKCmQmayGaObtLirZs2UJERAQzZ860bCssLKRHjx5MmjQJgDlz5tCqVSs6dOhgKffOO+/w+OOPExQUxMKFCxk4cCAfffQRRqOR1157rUwMNzc3tm/fzjPPPMPIkSOJjIwkIiKCn376icDAwMZqqhAO7cgRuHDhNGPGfIaX1xVKSjScPx/L5ctj2LfPhdudwA3L3d20lllRkWkAdWUvMH0tKjKVr2xJECFEk2Z3T5+NHz+e8ePHV1lm7NixpKenl9nWrl07vvrqK6vjeHt7l5nUUQjRePT6Elavjici4hAAeXltuXBhMsXFLfDxgZs3YcUK6NsXrF3btVaCg02Lu1bVA2QwwOnTpifOnJ1NCZEsBitEs2R3SZEQwsauXi2fJJjH9aWkmBKD0mqYJGRlZfHll3G4u5tuiaenD+bKlQdQFNN5VSoIDIR9+0y9SQ3+3EN1ddfrTUlRhw6N+sSZEKLxSVIkhLjj6lWYOtW0Enxprq7w5z+b5vQpLi67z9vb1NtiRWJ08uRJNm7cSHFxMUVF7qSmPoJW26lcubt7i4QQojFIUiSEuEOnMyVEbm5lx82YF09u0aJsUmQuX80AZL1ez/fff295OjQzsz2nT09Go6l4Ztm7e4vuu68ujRJCCOtIUiSEKM/dHTw973xvvm3k4VH+FlJRUZWnysjIIC4uzrJ2WUHBULZti6J9eyfLg10VUatBqzX1Fn38cW0aIYQQNSNJkRCiwRw7dozNmzej1+vx8PBg5MhH+dOfOuLlBVlZ1R/v7Q0nTsCtWw1fVyGEkKRICFHv9Ho927Zt45dffgFMS+5MmjQJb29vvvqKKnuI7ubhAVYsWSiEEHUmSZEQol7dvHmT1atXc/PmTQDuv/9+hg8fjpOTaVq0Nm1qfk5Z5UMI0RgkKRJC1JujR4+ydetW9Ho9np6eTJ48mQ6NsJq8EELUB0mKhBB1VqzXs3X9en799VcA7r33Xh599FG8vLxsXDMhhLCeJEVCiDq57u1N3LZtZGi1qFQqRowYwdChQy23y4QQoqmQpEgIUYbBAM7VF0MBfgkKYlvHjpRotXh7ezN58mRCQkIauopCCNEgJCkSQlicOAGaFGjlCt6elZcrcnJic0gIJwICAOgYHMwj06bh6VnFQUIIYeckKRJCAKAosGYNTCqEjEs6vLxBBZQYAPfbM1oXFHDN1ZW4Ll3I8vBAZTTywOnTDJk+HZUkREKIJk6SIiEEYFpOI+EXd8a7euOizaXgahEqTMuh3dPBFUVROOzjww9dumBwdsanoICYn3+mXUmJaTIhIYRo4iQpEkKgKLB8OVwqCeYfkatIT9ExJMK074cfYNQAHd4Xf+DX7t0B6NSmDRMHD8bj6adNS4JYsRisEELYO0mKaiErKwuNRkNBQQGZmZl4eXnh5ubWqHXQ357NTm+DWe0cNbat4zdk7KNHITHRNLFink8gxg6w1vR0Pa16XCMvYBPGnBycnJyIiooiIiIClUqFpSYN/Hk018/dnmPbOr7ElmveEOeujkpRFKXeozdTWq0WX1/fcttjY2OZOnWqDWokRMNRFIWMjAyuXr2Koiio1WpCQ0NlMLUQoskpKChg2rRp5OTk4OPjU2k56SmqhZSUFDQaDTt37iQqKspmPUXx8fFER0ejvnvVcondLOM3VOyjR+H5503vr1yBzp3B2VlHq1ZbCAu7AkBWVjj33+9G27Zj6dtXPndHiG3r+BJbrnl90mq1VpWTpKgW/P39cXd3x8PDg4CAAJv84Jqp1WqbxXfU2LaOX5+xFQW+/BKys6GwEDIyICjoMsOHx+HtnUNJiTM//xyNRtMHF5dtfPutmogINSpVvYSvsebyuTel2LaOL7Fto7m13drzSVIkhAM7cgQSEkxjpa9dU4iM3M+wYT/i7GwkO9uPbdtiuHYtmAEDTPfj9+83HdOvn40rLoQQDUCSIiEclPmJs4ICUJQCJk5cT1jYOQBOnuxOQsJDlJRoMBjg8mXTMYWFsGIF9O2LzXqLhBCioUhSJISDMvcStWmTRp8+a/D21lJS4sz334/h8OF++PqqUKvBzQ1u3TId07Il7NsnvUVCiOZJkiIhHJCpl0ihXbt99O69EycnhVu3Ali3LoZLl4JQqUCnM/UGqVSm9dAAvL1Ng7Glt0gI0RxJUiSEA7p8OR8np3X07ZsMwKlTPdm+fTxarRvmxe2Li8HZ2fTSaEzb9HoIDJTeIiFE8yRJkRAO5uLFi6xZswZ//zyMRhcOHBhLcXEfPD1VFBaaEiBFgaIi8PKC0FDTLTQw9Rip1aDVSm+REKL5kaRICFu6etV0n8oa5ntYtWQ0Gtm7dy+7d+9GURS8vFqyceMUFOUe3N1Ng6hdXcFoNJV3doacHNN4Ij8/07Zbt0zJkrc3nDgBWVkQEFCnagkhhN2QpEgIW7l6FaZOhdxc68q7usKf/wzp6dCuXY1C5eXlsXbtWlJSUgDo1as3P/44lhs3XAkLM/X29OlTNu9SFEhLg4EDTWHPnjU9rebsbNrv4SEJkRCieZGkSAhb0elMCZGbm2mioOqYMxZre5Zuu3DhAmvXriU/Px+1Ws348eMpKelFQoJpfJD59pd53FBpbdqYeoTy8kzfh4WZbp8JIURzJEmRELbm7g7WrCdWw0USjUYju3fvZs+ePQDcc889xMTE0LJlIHPnmsYF+fmZ5imqjHn80MqVMGJEjcILIUSTI0mREM1Qbm4ua9asITU1FYC+ffsyZswY1Go1mZlw8qRpXFBWVvXn8vaG06clKRJCNH+SFAnRzJw/f55169ZRUFCAq6srDz30ED179rTsDwiAr76quofobm5u8MsvDVBZIYSwI5IUCdFMGAwGdu7cSUJCAgBBQUHExMQQUMFo6DZtanZuvV6SIiFE8+dk6wpU5eLFi8ybN49+NZghLjExkQcffJCIiAj69+/PggULUBSlXLmcnBxeeOEFevbsSWRkJDNmzODatWv1WX0h6lXe7YfUzpwuvy8nJ4cVK1ZYEqL+/fvzu9/9rsKESAghRMXssqdo3759fPrpp2zYsAGDwUBYWJhVxyUkJBAdHc2QIUM4dOgQmzZtYsKECSQlJbFw4UJLOZ1Ox+jRo0lMTCQlJYWgoCBCQkJISEggISGBVq1aNVTThKgVBbh23fR+w0boMeHOU2Nnz55l/fr16HQ63NzcePjhh+nevbvN6iqEEE2V3fUUHThwgMuXL+Pv74+hBpPVKYrCnDlz0Ol0REVFATBs2DAA/vnPf7J//35L2UWLFnHw4EHCwsJo3749rq6uDBw4kOTkZN566636bZAQ9SAnx/QUGJhuYx05Yrpd9v3337Nq1Sp0Oh2tW7fm6aefloRICCFqye6SosjISH7zm9/w6quv1ui4Q4cOcezYMQB8fHwAaNGihWX/8uXLLe+XLFlSplzpsitXrkRfw0efhWhICnDp0p1pigoL4auvslm2bBkHDhwAYODAgcyaNQt/f3/bVVQIIZo4u0uKzFxcanZnb/fu3Zb3rq6u5fbv2LEDgBs3bnDmzJlKy+Xl5XHo0KEaxRaiIeXkwK0scL09aWLbTpfx9PyMK1euoNFoiI2NZcyYMTX+f0YIIURZzea36MmTJy3vK/rHITk5GYPBUG05MI3RGDJkSP1XUoiKVDFDtQJcvwBqPbh6Gbl8+TLBfTIA0OvbMHduDH5+LRqnnkII0cw1m6QoMzPT8t7JqXwHmKIoZGVlVVsOICMjo8pYWVlZaDQaCgoKyMzMxMvLCzfzMuKNxHyLzxa3+hw1dr3Hd3U1TRqUm1vppEEFOtDrwKW1B7vH9ePW7Z/Ny5cHcuLECCZOdKZ374b/LJrV5y6xm0R8iS3XvCHOXZ1mkxRZ2+D6+LA7dOhQ5vvY2FimTp1a5/PWRnx8vE3iOnLseo3/hz9UWyQ7O5u0tDSMRiPOzs60b9+e3r2LeOih77l61bSubGNpNp+7xG4y8SW248VviNgFVs5W22ySouoGmKpUKvz8/KwaiFrd3C4pKSloNBp27txJVFSUzXqK4uPjiY6ORt3IK3Q6auzGjn/4cAmff/4DISEXASgoaEP//r7ExY1Hr1ej1ZoWal2wAHr3btCqONTnLrHtI77Elmten7Tmx3er0WySoh49eljeV/Qof6dOnXBxcSmz3EFlj/x369atylj+/v64u7vj4eFBQECATX5wzdRqtc3iO2rsxoifkZHJli2rCQkxTU507dpQbtwYwuDB36PXq9Hr1Wg0cOWKacmO/v3vzFvUkJr75y6x7S++xLaN5tZ2a89nt0+f1ZR5biKAwsLCcvuHDx8OQHBwMOHh4ZWW8/T0rNEM2kLUt+PHj/Ovf32Oi8t1ios9OHt2OleujAScy5RTqSAwEPbtM81bJIQQom7sNikqPfbn7h6dbdu2ERQURFhYGEdu/2swaNAgy6R15oHSpZOeGTNmWN7Pnj27TLnSZadPn27T7Fw4Lr1ez8aNG1m7di0GQzHp6SEcOfIs6ekdKSi485CaTmcal11QAGq1aVLHFSuggtVshBBC1IDdJkU///yz5X16enqZdckWLVrE9evXuXDhgmVSRpVKxdKlS9FoNOzduxfAMt/Qc889x+DBgy3Hz507l8jISFJTUy0DWA8fPkxoaCjz589vhNYJUdbNmzdZsmQJv9xedfXixfs5ePBJ0tO9ycqCrCy4dctU9tYtLNuyssDbG06cML0XQghRe3Y3pmjLli28++67/Prrr5ZthYWF9OjRg+HDh7N27VrmzJlDYmIiHh4ezJw501Ju4MCB7N69mz/96U/07t0bJycnPvnkE1588cUyMdzc3Ni+fTuvvPIKI0eOxM/Pj4iICD7++GMCAwMbqaVCmBw9epStW7ei1+vx9PRk0qRJuLndW+4pfYMBkpJg+XJwLnsnDQ8P09P9Qgghas/ukqLx48czfvz4KsuMHTuW9PT0CvdFRESwZ8+eauN4e3uzePHiWtVRiHKuXq1yEsZy3N0pbtmSrVu3Wv4A6NChA5MmTcLLy6vCQ/R6U1IUFma6bSaEEKJ+2V1SJESTc/UqTJ1qmoTRSjeCglg9ejQZ2dmoVCqGDx/OsGHDKp1QVAghRMOTpEiIutLpTAmRmxu4u1dZVAF+CQhgW9eulGRn4+3tzaRJkwgNDW2UqgohhKicJEVC1Bd3d/D0rHR3kZMTW0JCOH578E9Y69Y8On06nlUcI4QQovFIUiREI0h3dycuLIxMjQaVovDA6dMMmT4dlSREQghhNyQpEqIBKcDPgYF8364dBicnfIqLmXzyJO3T0hpnCmohhBBWk6RIiAZS6OzMppAQTt1eb69TdjYTU1LwsHINHiGEEI1LkiIhGsBVDw/i7r2XWxoNTkYjD165QuT160jfkBBC2C9JioSoRwpw6J572N62LUYnJ3yLioi5cIG2+fm2rpoQQohqSFIkRD3RubiwMSyMM35+AHS5dYsJFy/iftfafUIIIeyTJEVC1IPLfn7E9etHzu3bZaMuXybixg25XSaEEE2IJEVC1IGiKOw/dYofhw7F6OSEX2EhMRcuEHz3wmVCCCHsniRFQtRSQUEBGzZs4OzZs+DkRLdr13g4ORlNdbfLarJGmhBCiEYjSZEQtXDp0iU2bNiAVqvF2dmZ0adO0f/MGYwGwLnaw8Hbu9olQYQQQjQuSYqEqAFFUbh+/Tq//voriqLg7+/PlClTCDIaOZGo429/gz/9Cbp3r+ZE7u4QHNwodRZCCGEdSYpqISsrC41GQ0FBAZmZmXh5eeHm5taoddDr9WW+SuyGl5+fz8aNG7l27RoA3bt3Z8yYMbi5uVGswJKdkJAOLXbCh2OsmLC6hm1w1M/d1vEdNbat40tsueYNce7qqBRFUeo9ejOl1Wrx9fUttz02NpapU6faoEaiseTl5ZGamoper0elUtG2bVv8/f1RyVIdQghh9woKCpg2bRo5OTn4+PhUWk56imohJSUFjUbDzp07iYqKsllPUXx8PNHR0ajVaondQIxGIz/99FOZ22WBgYFMmDDBEl9R4JVXYNcu6NABUlIgKgo+/LB+lzdzpM/dnuI7amxbx5fYcs3rk9bK5ZUkKaoFf39/3N3d8fDwICAgwCY/uGZqtdpm8Zt77Ly8PNauXUtKSgoAvXr1Ijo6mh9++KFM/MOHYc8e8PGBkhLT19274fhx6Nev/uvV3D93e43vqLFtHV9i20Zza7u155OkSIgKXLhwgbVr15Kfn49arWbcuHH07t273H1pRYHly01P2ZvHTfv4wM2bsGIF9O1bv71FQgghGo4kRUKUYjQa2b17N3v27AHgnnvuISYmhsDAwArLHzkCCQkQGHgn+VGpTN/v22fa3xC9RUIIIeqfJEVC3Jabm8uaNWtITU0FoE+fPowdO7bSbteKeonMpLdICCGaHidbV0AIe3D+/Hn+9a9/kZqaiqurK5MmTSozmLoiFfUSmd3dWySEEM3JN998g6+vL3PnzrV1VeqVJEXCoRmNRn744Qe++eYbCgoKaNWqFU8//TQ9e/as8rjSvUSVPd3p42Pav2KFqbwQQtRUYGAgKpUKlUqFh4cHLVq0oEWLFri4uKBSqXBycrJs8/b2tpRVqVScPn26wer17bffotVqWbZsWYPFsAW5fSYcVk5ODmvWrOHSpUsA9O/fn9GjR+PiUsX/FunpAJzelkLqDmd6eYF3FWu/qn3c2bcvWMYWCSFqJTc3l1mzZvHhhx/SsmVLy/YRI0awe/du2rdvz8WLFy3bs7Ky+Mc//sG7775Lbm5ug9Xr5Zdf5saNGzz++OMNFsMWJCkSTdfVqzVbXLXU0hpnz55l/fr16HQ6XF1dmTBhAt2rW5vj6lWYNQv+8Ad8//Akf7lRjKtr1YcUuHjze99VrFgRLGOLhBA1otfrCQsL44svvsDJybobO/7+/rzzzjukpKSQl5fXYHW7//77OXjwYIOd31YkKRJNU3o6PP441OQvIW9vDN98w4+nTrF//34AWrduTUxMDP7+/tUfr9NZ4mWUtMDgWlxlcTejDreiXAI8dJw4AVlZEBBgfXWFEI5Nq9XyyCOPWJ0QlRYbG9ugPUXNlSRFomkyJyhubhWuNp+vA0/3suWzS0qI27CBKxkZAERERBAdHV317bJKdO7jgaqw6snAVDpw1hax4B+g7iIJkRCNITcXvL1tXYv64ePjw0svvVSrY6OioigqKqrnGjV/khSJps3dHTw9y2w6ew7OnYVOnSG8o2nbmYAANnTqRGFGBhqNhgkTJtC1a9dah9VoQO1sRcEiCA0F2tQ6lBDCSkeOwGuvwV/+An362Lo2dadWq8uMI6oJd3d33Cv4g1FUTZ4+E82KosCFC2BU4EIy6FHxXbt2/KdHDwpdXWkTEMAzzzxTp4RICGF/FAWWLYNTp0xf5YlP0yDtRYsW0bNnT9555x1KSkp46aWX8PPzY8iQIWRmZlrKbty4kQceeICOHTvy2GOPER4ezlNPPWV5EKU0vV7P6tWrefDBB3nggQfK7T98+DDTp08nPDwcMN0GnDdvHq1bt6ZFixbMmjULXU3GgzYiSYpEs3LuHJhX4sj1cuVfHbpwsFUrAAadP89vR42iRYsWtqugEKJBmOcN8/eX+cEA/vKXv9C1a1eee+45Tpw4AcCcOXNYtmwZ2dnZ/PTTT2zYsAGAP/7xj0ycOJGoqCiSkpL497//zaBBg1i6dCn3338/BQV3HrH97rvvePDBB4mNjeXHH3/EaDRa9h07dozHHnuMAQMGsHLlSvR6PVeuXGHQoEGsWLGC4uJicnJyWLZsGc8//3zjfiBWkqRINHklJaavigIXTGu3kt3Nj9PPdCMrwBP3khJ+c+IEo06exNnZmnteQoimpPS8YW3byvxgAH/+8585d+4c99xzDwDbt2+nc+fOXLt2jVdffZVRo0YxZswYUlJS+PTTTwH4n//5H5ydnfHy8uL//b//h0ql4uLFi+zdu9dy3jFjxrB7925iYmLKxezUqRP//e9/eeGFFwDQ6XT8/ve/529/+xs3b94kMzOTOXPmALBixQry8/Mb+mOoMUmKRJNy9xOmubnw82HIyTH1EhUpKi6Na8+Fx8IwalzwTMsjevNJOpfqJhZCNC93zy4vs8mbuLu7c++99wLg4uLCvHnzcHd35//+7//4/vvvCQ4OtiQmLVq0KDODv5+fnyWhyrj9cEppHTp0KLdNo9EApuQIoLi4mC+++IIxY8aguj0fiTlhKikp4dy5c/XV1Hpjl0lRUVER7777Lj169GDw4ME8+uijnD171qpjP/nkkzIzeppffn5+ZUbi5+Tk8MILL9CzZ08iIyOZMWMG165da6gmiXpw5Ag8+eSd7xXg6jXIy4W0S3Aq242k33XlZoTpf+SgfdfotDyJGyf1Dv0XoxDNWUWzy8ts8ne43p5MraKxPwA9evRg9erVbN26tcz2X3/9lZLb3fDmr6WZE6CKuLm5AeDr60ur28MXzNq1a2d5b49TBtjd02eKohAbG8uGDRvYt28fQ4YMoX///gwZMoS9e/fSpUuXKo9fuXJlhdsnT55suVA6nY7Ro0eTmJhISkoKQUFBhISEkJCQQEJCQrmLKGzPPIgyKenO9wUFph4itSsc9/MneWoIRjdnXPL1dFiXgm+yFkWBYiNcvAgd/GzaBCFEA6hoDcK7e4sceTZ5lRUzxppvhRUWFrJixQr+8Y9/MGDAAMt+pYLMsqrzVjWvUukn4gwGQ7V1a2x211O0du1aNmzYgKurK4MHDwZg6NChZGRkWLrdKpOUlERSUhKdO3cu93qyVBfDokWLOHjwIGFhYbRv3x5XV1cGDhxIcnIyb731VoO2T1iv9B8R5l98frcTm9OnISsTilVOnB8TwrmJ92J0c8brYi7dPjuFb7IWuPNL8tJl+YtRiOamqjUIpbeoZpYuXUq3bt24fv06r7/+Ol988QVeXl62rlajs7ueoiVLlgBYFrYDLE8Lbd++nWvXrtG6desKj/3mm2947733qp3syhzDp9T/ReYYK1euZOHChVWuji4a3pEj8Mor8Ne/Qu/ed37xhYSY9q9YAbEaLw79pg+5Lb1AUQjac43g3VdxuusXoArQl0D6daj4J6cWCgqguOoZrWu0BIkQosYq6iUyk94i6xiNRqZPn86mTZvYs2cPPXv2LHcrzZHYVU+RwWAgISEBuHMf9G47duyo9PiVK1fy6quvEhgYyLBhw3j77bfLzbFw48YNzpw5U2mMvLw8Dh06VNsmiHqgKPDhh6Zfdh99BIcPl//Fl5aVQvyTw8lt6YU6r5geXx2j485zeBrz8VDKvjzJxx0dN29AnXpr3d3vTJWbnV39q6jIVF4mUBOi3lXVS2QmvUXVW7BgAd9++y3Tp0+nb9++tq6OzdlVT1FaWppl4FVlSy9UNuD6wIEDJCcnA6aR8vv27WPfvn389a9/5W9/+xu///3vATh58qTlmKpiDBkypNbtEHVz+DDs2GFKYOLjwWg0/WILDgYnp2LS0tLo+UAWBlwIvHiT3nGH0eTdGURf0Z1uBbhl8OYfX7jz0l9rWbHgYPj3v01/dn75JVjzeH+pRWiFEPWnql4is+bcW2Qe51PReJ+a2L59O0CV05WUnououbOrpKj07JqVDdSq6NFAMI1FUqvV6M0z992m0+mYM2cO/v7+xMbG1imGWVZWFhqNhoKCAjIzM/Hy8rIM4m4s5nbe3d6mHltR4NNPTXMPtWhh6mzZswe6dwcfnxuEhq4jKysLo1HF6cQ+nNkXTonBCe7qjHFSlf9FmW90p3hVIL//X71V+UxF9LcXMNO3bQvW3mKtp8+puV5ze4/vqLFtHb+q2IoC//63Ezk5Tvj6KuWm6ijN2RlyclQsW2akZ09jpQmUtbEbmrWxs7KyAMjOzq6yrHlfRkZGheV8fX0B0/CT3/3ud3Tt2pWbN2/y1FNPceXKFQAuX77M5cuX2bFjB1OnTgXuPDlWUFBQ7ry3bt0CTE+SV1W33NzcMvsb8nO39px2lRTV5YP46KOP+Oijj8jJyeHo0aPExcWxdOlSy1Tib7zxBrGxsfXyYd89P0NsbKzlB6WxxcfH2yRuQ8Z+7DHTy0xRFLKysrh8+TKKouDi4kJISAh9+xrhmaQan//77+tex+b4udt7bFvHd9TYto5fUWytVs3evQNQFDWpqdadZ88ePf/5TyI+Ptb/G2Bv7QZTr83PP//MqVOngDtLaIwYMaJcb8+FCxc4cnuyppUrVxIeHk5oaGiZMuZ/z7RaLREREfj7+5OXl8ecOXNo164dKSkpzJ8/n88++4x3332XrVu3UlBQQFxcHGCaxfqzzz6zPGpfXFzMihUrALh+/Tp//etf6d69uyXeTz/9ZHm/YMEC9Hp9uXo3xOdeelbuqthVUuTv719tmYBqlhr39fVl+PDhDB8+nHnz5jFx4kSOHTvG+fPnyczMrJcYKSkpaDQadu7cSVRUlM16iuLj44mOjm70QeH1Gjs9HWbNgtxcFCAtFXK0YO7EK3J25vio7lztblpRNTAtk3vG3s+n/9OflII2FBdD69bg4WEqryhw8yZ07QpTppTvLQoMhFGjal/dZvO5N6HYto7vqLFtHb+62MOGmZ53sJaHB7RpE10vsRtSVbFXr17NrFmzysy5B6bk4l//+hd/+ctfmDt3LgBTpkyxLOMBpt6bF198kd/85jd8+eWXlu3jxo0jKCiITz75hBs3btCnTx8mTJjArFmzCA8P5+WXX6Zfv34sXryYjh078tVXX/H0009bHqcvKiri+eef57e//S3Tp09n9OjRln2KovD666/ToUMHkpKSaNmyJVqt1hI7Pj6eXbt2cfToUcLDwxv0cy8dtyp2lRSFhYXh7u6OTqerdP6Cbt26WX2+0NBQtmzZQnh4OIWFhbi5udGzZ0/L/trG8Pf3x93dHQ8PDwICAmz6pJparbZZ/HqJXVwMmZng5oZW787VDDAawMkZclp6cjqmG7qWHmBU6LQvhZCDF7k5YSQqXTElJWoKC02zXIeE3EmANBpTrnXffQ03hqDJf+5NMLat4ztqbFvHryz2XR0ejRq7MVQUe9q0aUybNs2q49evX291rOeff96yFpler2fr1q2o1WqeffZZnn322TJlZ82axaxZsyo9V0UTPZrl5ORYVZ+G+NytPZ9dPX3m4uLC0KFDAdMkUndTqVTcf//9NTpn27ZtiYqKolevXnh5eREcHGxZubeiGJ6envRrTqPxmgjF3Z1zVz3JNXqic/EkpV8ovzzVF11LD9Q5xXRanoTvriyKFNPgIaPR9HJxgYwMuH7d9FdjQYFpqI9WK0+cCCGEqBm7SooAZs+eDZi6+sw9OebkJTo6muDgYLZt20ZQUBBhYWGW+6XJycl88MEHHD58uNw5XVxceOONN8rFKD2g2hxj+vTpMkeRDeRqTcmNwc2Z84/ey8WxISguTviezabbZ6fwSsvDUOoBCL3eNAjbYDB1Nl28aOpwysoyvby94cQJ03shhBDCGnaXFMXExDBp0iQMBgP79+8H4NChQ/j7+7NgwQLANCP19evXuXDhAsuXLwfggw8+4PXXX2fAgAHMmzfPMqgqMTGRyMjIMiv6zp07l8jISFJTU0lLS8NoNHL48GFCQ0OZP39+4zZYoACpaZBzjwenZnclq5s/KoORNtsvEbbqPC4FJZbH7FW3f2JbtYJ27UyvsDDo2BE+/xz++987r6++gmqGhwkhhBAWdjWmCEy3yL799lveeecdfvvb3xIQEEDLli3Zv3+/ZeXdOXPmkJiYiIeHBzNnzgTgnXfeobi4mB9//JGFCxeyfft2nnjiCYYPH85rr71WJoabmxvbt2/nlVdeYeTIkfj5+REREcHHH39MYGBgYzfZ4eUXwPGwYM4/GIbi4oRbdhEd113A62p+mZ9QgwHLJET/7/+BMeTOPtMgykatthBCiGbG7pIiMA2Iev/993n//fcr3D927FjS09PLbGvXrh1fffWV1TG8vb1ZvHhxneop6q6gsIjVAwaQ1tE0waFf0i3CNl/EpbD8IHhnJzDeHsPXrh24hjdmTYUQQjR3dpkUCcdw5coV/rtlK9qOwagMRtr9cJlWh26gAqqbPzU7G+5phDoKIYRwHJIUiUanKAoHDhzghx9+wGg04pmdT4+NZ3C7oZSbmfpurm6QS+XT+gshhBC1JUmRaFQ6nY7169db1rDr1r49o9cvxvkeL5QQz2qPNxhgF6YlQIQQQoj6JEmRaDSXLl0iLi4OrVaLs7Mzo0ePpr+fH6oFC0w9RNXnRPW1jJgQQghRjiRFosEpikJCQgI7duxAURT8/f2ZMmUKQUFBkJxsKnR7jbpqVTILuRBCCFFXkhSJBpWfn8/69es5f/48AD169OChhx66s1acu7tppsXcXNNsjNVxdb1znBBCCFGPJCkSDSY1NZU1a9aQm5uLi4sLY8aMoW/fvqhKj5IODoZVq2rWU3T6NAQFNUylhRBCOCxJikS9MxqN7Nu3j127dqEoCi1btiQmJoZWrVpVfEBwsPUn1+tNSZEQQghRzyQpEjX266+mr8eOlV+FPi8vj3Xr1nHhwgUAevXqxbhx43A13/YSQggh7JQkRaJKubmmIT9migJffw1RUaavffvemTMoJSWFNWvWkJ+fj1qtZty4cfTu3dsm9RZCCCFqyu4WhBX248gRmDIFfvml7LYDB0zv9+83fW80Gtm5cydffvkl+fn5BAYGMnv2bEmIhBBCNCnSUyQqpCiwbBmcOmX6as5vli+HwkLT+8JC+PLLXI4fX0tq6kUA+vTpw9ixY1Gr1baothBCCFFrkhSJcq5ehWvXICEBfH1h3z5TjxCYtrVsaXrfseMFPD03kppagFqt5qGHHuK+++6zXcWFEEKIOpCkSJTxzTcwdy707w+3bkFeHnh4wIoVYDSanpz39jZy9epVevY8CkBJSSuee24KLVsG2LbyQgghRB1IUlQLWVlZaDQaCgoKyMzMxMvL685khI1Ef3u9C309rnthNMLf/26aQzEx0bS+mF4POTnw/feg0UCHDlo6dVrHjRs3ALh2rQ/Hjj3IhAlqfH0bfg2Ohmh3U4nvqLFtHd9RY9s6vsSWa94Q566OSlEUpd6jN1NarRZfX99y22NjY5k6daoNatS4cnJySEtLw2Aw4OTkRLt27fDz87N1tYQQotl68skn0Wq1ALi6uuLiYurLKCwsxGg0olKpcL89w7/RaKTQPOgTWLBgAe3atWv8StuhgoICpk2bRk5ODj4+PpWWk56iWkhJSUGj0bBz506ioqJs1lMUHx9PdHR0vQxqNhpNcw5duABOTqbvVSrTezAwbNguBg5MAUCna0Xfvv6sWfMQer0ardZ0m23BgjsDshtKfbe7KcV31Ni2ju+osW0dX2KbYhcVFTFz5kw++OADWpoHdAIPPvgge/bsoX379pw7d86yPSsri4ULFzJ//nz69u3LgAED6hS/MTVkbHNiWR1JimrB398fd3d3PDw8CAgIsOmTVmq1ul7if/WV6Umzu/sN/fyymTx5DW3bXgbg1KkI9PoRDBq0Hb1ejV6vRqOBK1dM5+jf/868RQ2pvtrdFOM7amxbx3fU2LaO78ixAcLCwli6dClOTmVn0Cm9XFLpOrZq1Yr//d//JS0tjcLCwlrX35q2Z2dn88Ybb7Bw4cJaxahL7Nqc0xqSFAmMRnjvvfIJUefOZ3jkkQ24uxdSWOjGhg0TSUvrysCBZe/NqlQQGHjnKbW7Z7kWQghRc1qtlkceeaRcQmSN2NhYcnNzG6BWd/zjH/8gLy+vQWM0NkmKBN98A8nJd753djYQHR1PZORBAK5cCSYuLoZbt/xwdobUVFM5nQ6Ki03v1WrQak1PqZWe5VoIIRrE1avWLyQN4O5es3UW7YCPjw8vvfRSrY6NioqiqKionmt0x6+//spf/vIXYmNjGyyGLUhS5ODu7iXy87tFTEwcbdpcBeCnnyL58ccHMRicAdMi9RkZprK3bpmeVDPz9oYTJyArCwLk6XwhREO5ehWmTjWtQ2Qtb29YtapJJUZqtbrMOKKacHd3twzArm+//vorY8aMKTOou7mQpMjBle4l6tbtFBMmbESjKUKn07Bu3SOcPdsZMA24VqlMSdHthx9YtuzOezMPD0mIhBANTKczJURubqYeIGvL16RnqZnIysriww8/ZNOmTaSmpqLRaBgxYgRvvfUWvXr1KlNWp9PxxhtvsGLFCvLz88skPb/88gu9e/dm1apV/PGPfyQzMxOAlStXsn79egBeeukl3n777UZrW0OQtc8cmLmXyNm5hHHjtvDYY6vRaIq4kRbI1n+NpeSsC/eSzL0kE2pM5l4lmTCSaV1oegqtZW4K4Z5XCQ/H8mrTxsaNEkI4Dnd38PSs/tVAPSb27uTJkwwcOJB7772Xn3/+mZs3b/Lyyy+zbt06IiMj2b59e5nykydP5uDBg/zf//0fWq2WpKQkHnrooTJlpk6dyrVr15g2bRoA06ZNIzs7m+zs7CafEIH0FDm0b76B7OxMfve7OFq3Tgeg3b5Uxu/YyO+MS8qVd7r9H4PRlcv8Gd+5T5r+UmtiXdJCCNHc5efnM3HiRF566SWeeeYZy/b/+Z//4fTp03z55Zc8+eSTpKSk4O7uzsGDB9m2bRtr1661DOzu1KkTa9asoWfPnrZqRqOTniI71MAPDACmXqIVK04we/bntG6dTn6+Bz98HUXvH45SbHTlFi3KvbKMLchxakGOqgUABSVuDtslLYQQ9uzf//43qampzJgxo9y+QYMGAXD9+nVLb1FKiukOwN69e8uUdXV15fnnn2/g2toP6SmyM0eOwGuvwV/+An36NEwMvV7Pf/7zHcOGmVZ5TU1tT1zcZAJzbwKgw50CPCs81tkIOJnme7ic5Y5vi1zkQTMhhLAv69atw2Aw0LZt23L7DAaDZcLh5NuDSjt3No0f/fTTT0lKSiIyMpJWrVoB8Ic//KGRam17khTZEUUxDV4+dcr0tXfv+n+0PSMjg9WrV3Pjxg0UBRIShnH48AjACWenm2AEFVgSHeX2e1dXcHYGJ2dQ3EEPFOrA4CU/REIIYW+OHz/OPffcQ3p6ulXl+/Tpw4wZM1ixYgVbt24lPDyc3//+9/zxj3+0JEeOQG6f2ZEjRyAhAfz970yEWJ9+/fVXPv/8c27cuEFJiSerVj1OYuIDODk54eoKXl7g7GRKflxcbr+cTYmZbwsYGAkDBpjmIQLo0qX802dCCCFsLzs7m8zMzBo9Nr9s2TIWLVqEr68vubm5fPTRR4SHh7NgwYIGrKl9kaTITigKLF9uGp7Ttq3p64oV5WeZro3i4mI2bNjA+vXr0ev1tGkTyrffPsPFi2EYDKa5hoqKoKTE1DOEYopbOrZWC65q8PIETw/TtkZe7k0IIYSVPDw8KCkpISEhocpyBoPB8l6lUvHUU0/x2Wef8f7779OiRQtyc3OZO3cuf/3rXxu6ynZBkiI7Ye4lCgwsv2xGXdy4cYMlS5Zw9OhRVCoVI0aMYOLEJ7jnHm/CwqBduzuv1q1NiY+rBjTud14ejvtEqxBCNEnmMUJ/+9vfKi2zf/9+/v3vfwOwfv16tmzZAoBGo+Hll1/mzJkzDB8+HID33nuvTALVXMnNDztQupfI/GS7jw/cvFn7ZTMUReHo0aNs3bqVkpISvLy8mDx5MqGhoQD8979QUFD2GHUaBM0Fg49p3FBpzs6mcUVCCCFsT7ndla9Ucjvh4YcfJjExkW3btvGXv/yFP//5z2X263Q65s2bx7fffmvZFhcXx6hRoyzft2rViri4OIKCgsjLyyMjI8Myvsj82H5zm9XaLnuKioqKePfdd+nRoweDBw/m0Ucf5ezZs1Ydm5iYyIMPPkhERAT9+/dnwYIFFf7Q5OTk8MILL9CzZ08iIyOZMWMG165dq++mWOXuXiKoW29RcXEx69atY+PGjZSUlBAWFsazzz5rSYjANMli6UkXw8MhNBQ0buDpbrpNVvrlrqm35gohRP3Q6SA/v/pXM5w25NatW4Bp7FBFnnvuOYKCggB47bXXmDhxIlu2bOHo0aP897//ZdiwYfTp04f27dtbjvnqq6/Yv39/mfMEBATg5eVF69atCQwMtGz39vYG4NChQ+h0Om7cuMG8efPqs4k2YXc9RYqiEBsby4YNG9i3bx9Dhgyhf//+DBkyhL1799KlS5dKj01ISCA6OpohQ4Zw6NAhNm3axIQJE0hKSmLhwoWWcjqdjtGjR5OYmEhKSgpBQUGEhISQkJBAQkJCo460r6iXyKw2vUXp6enExcWRmZmJSqUiKiqKoUOHopIVWoUQzYW7u2kts9zcsgswVsXbu1mMAzAajWzevJlTp04BoNVqWbZsGU8++STOzs6Wcv7+/qxbt45x48Zx69YtNm7cyMaNGy37hwwZwscff1zm3AaDgcmTJ/Pb3/6WUaNGYTAYePfdd8nLy2Pp0qWW3iGAyMhI/vGPf5CSkkLbtm1RFKXcDNlNkd31FK1du5YNGzbg6urK4MGDARg6dCgZGRm88MILlR6nKApz5sxBp9MRFRUFwLBhwwD45z//WSb7XbRoEQcPHiQsLIz27dvj6urKwIEDSU5O5q233mrA1pVXUS+RWU16ixRF4eeff2bJkiVkZmbi7e3NzJkzGTZsmCREQojmJTjYNJP+6tXWv5rBzPv//e9/8fDwYOLEiWXG98yaNQt3d3f+/ve/lykfGRnJr7/+ylNPPUVwcDCurq6EhYXx9ttvEx8fX+GCsRkZGfz1r3/F19eXe++9l5MnT3Lo0CEmT55cplxsbCwvvvgi3t7etG7dmlWrVtG/f/8GaXdjsrueoiVLTMtLeHt7W/4xb9GiBQDbt2/n2rVrtG7dutxxhw4d4tixYwD4+PiUOQ5g+fLlllk8zTHM5UqXXblyJQsXLkStVtdfoypRVS+RmTW9RUVFRWzatImTJ08CEB4eziOPPIKHh0ftKlZdV7N5cFEz7JIWQjQRTTzBqY3HHnuMxx57rEbHtGvXji+++KLaco888giKoqDX69m6dSvjxo2r8t9BJycnPv30Uz799NMa1cfe2VVPkcFgsDw+6FrJqN4dO3ZUuH337t2W9xUdaz7uxo0bnDlzptJyeXl5HDp0qGYVr6WqeonMqustSk9P57PPPuPkyZM4OTkRHR3N1KlTa5cQmbuki4ogO7vqF5jKNZMuaSGEEMKueorS0tLIvb3wl0slswJWNuDa3EtS2bHJyckYDIZqy5ljDBkyxOp614a5l0irBT+/8k+ClaZWm8qV7i1SFIWbN2+yYsUKDAYDvr6+TJ48mXbt2tW+UuYu6ep6gAwGOH0avvzSNOOjA/7FJoQQovmxq6QoMzPT8r70gK7SMjIyanWsoihkZWXVKYZZVlYWGo2GgoICMjMz8fLysqwjY62sLDh3Dlq2ND0cUZ2WLeHsWbhxAzw8Ctm0aRNXrlwBTLfLHnroIdzd3dHr9TWqRzmlni6ojF6vh9On0bdta8rY6hqzBsztq3M7m2B8R41t6/iOGtvW8SW2XPOGOHd17CopqssHYe2x9fFhd+jQocz3sbGxTJ06tcbn+f3vax77xx/zSU1Npbi4GJVKRXBwMB4eHuzcubPmJ6uj+Pj4Ro9pD7FtHd9RY9s6vqPGtnV8ie148RsidkFVt2NKsaukyN/fv9oyAQEBtTpWpVLh5+dXpxhmKSkpaDQadu7cSVRUVK16impKURQSExM5duwYRqMRX19fWrVqxcSJExtlUHhper2e+Ph4oqOjHSq2reM7amxbx3fU2LaOL7HlmtcnrVZrVTm7SorCwsJwd3dHp9NVOp14t27dKtzeo0cPy/uKju3UqRMuLi707NmzynJVxTDz9/fH3d0dDw8PAgICGvwHR6fTsX79est4qq5duzJ27Fh27NiBWq22yf84gMPGtnV8R41t6/iOGtvW8SW2bTS3tlt7Prt6+szFxYWhQ4cCFU8drlKpuP/++ys81jw3UWXHmtdvCQ4OJjw8vNJynp6e9OvXr+aVbyCXLl3iX//6F2fPnsXZ2ZmxY8cyZcoUNBqZYloIIYSoT3aVFAHMnj0bME1hbu7JMScv0dHRBAcHs23bNoKCgggLC+PI7efUBw0aRPfu3YE7A6VLJz0zZswoF6P0gGpz2enTp9s0OzdTFIWEhASWLVuGVqvF39+f3/3ud0RERMhkjEIIIUQDsLukKCYmhkmTJmEwGCyzUB86dAh/f38WLFgAmGakvn79OhcuXGD58uWAqRdp6dKlaDQa9u7dazkOTGvAmGfHBpg7dy6RkZGkpqaSlpaG0Wjk8OHDhIaGMn/+/EZsbcXy8/NZuXIlP/zwA4qi0KNHD55++ukKJ60UQgghRP2wqzFFYEpuvv32W9555x1++9vfEhAQQMuWLdm/fz+dOnUCYM6cOSQmJuLh4cHMmTMtxw4cOJDdu3fzpz/9id69e+Pk5MQnn3zCiy++WCaGm5sb27dv55VXXmHkyJH4+fkRERHBxx9/XGbBO1tITU1lzZo15Obm4uLiwpgxY+jbt6/0DgkhhBANzO6SIjANiHr//fd5//33K9w/duxY0tPTK9wXERHBnj17qo3h7e3N4sWL61TP+qQoCnv37mXXrl0oikJAQABTpkxp1MVphRBCCEdml0mRo8nLy2PdunVcuHABgPvuu4/x48dXutSJEEIIIeqfJEU2ptPp+Oyzz8jLy8PFxYVx48bRu3dvuV0mhBBCNDJJimzM3d2d7t27c+HCBaZMmWLzMU1CCCGEo5KkyA5ER0djMBjkdpkQQghhQ5IU2QFnZ2ecnZ1tXQ0hhBDCodndPEVCCCGEELYgSZEQQgghBJIUCSGEEEIAkhQJIYQQQgCSFAkhhBBCAJIUCSGEEEIAkhQJIYQQQgCSFAkhhBBCAJIUCSGEEEIAkhTVWlFREatWraKoqMjh4jtqbFvHd9TYto7vqLFtHV9iyzW3BZWiKIrNojcxWq0WX19fUlJSKCkpoWfPnhw/fpygoCDc3NwatS5ZWVl06NCBlJQU/P39JbYDxHfU2LaO76ixbR1fYss1r09arZaWLVuSk5ODj49PpeUkKaoBc1J0t9jYWKZOnWqDGgkhhBCiOgUFBUybNq3apEgWhK0F6SlyzNi2ju+osW0d31Fj2zq+xJZrXp+0Wq1V5SQpqgFzp5qLi+ljKywsxMXFBaPRiE6na9S66HS6Mi+J3fzjO2psW8d31Ni2ji+x5ZrX97nhzr/jlZHbZzVw+fJl2rVrZ+tqCCGEEKIWLl26RNu2bSvdL0lRDRiNRq5evYq3tzcqlcrW1RFCCCGEFRRFITc3l+DgYJycKn/wXpIiIYQQQghkniIhhBBCCECSIpsqKiri3XffpUePHgwePJhHH32Us2fPWnVsYmIiDz74IBEREfTv358FCxZUOIAsJyeHF154gZ49exIZGcmMGTO4du1afTelxurS9k8++QSVSlXu5efnV2bSL3ttO8DFixeZN28e/fr1s/qYpn7NzWrT9qZ+zS9cuMC0adNo3bo1Pj4+jBkzhl9++cWqY5vyda9Lu5v6Nc/IyGDu3LmEhITg6+vLqFGjOHHihFXHNuVrXpd228U1V4RNGI1GZeLEiQqg7Nu3T1EURenXr5/SsmVL5fTp01Ueu2/fPsXd3V158MEHFUVRlI0bNyqA8txzz5UpV1BQoAwcOFBxcnJSUlNTlaKiIiUoKEgJCwtT0tPTG6ZhVqhL281lgXKv3/3ud5Yy9tr2vXv3KpMmTVKcnZ0VQAkLC7PquKZ+zRWl9m1XlKZ9zc+fP68EBgaWq7uHh4dy/PjxKo9tyte9Lu1WlKZ9zdPT05XOnTuXq3vLli2Vq1evVnlsU77mdWm3otjHNZekyEbi4uIUQHF1dVWMRqOiKIrywgsvKIAyatSoSo8zGo3KfffdpwDK+++/ryiKoty6dcvyw/PTTz9Zyn788ccKoISHh1u2mZORp59+uoFaVr3atl1RFOXMmTOKl5eX0rlz53Kv3bt3W8rZY9v379+vrFq1Snnqqacs18uaxKA5XPPatl1RmvY1VxRFmTx5svL3v/9dOXz4sLJ48WLFy8vL8hnExMRUelxTv+61bbeiNP1r/tRTTylff/21cuXKFWXz5s1Kq1atLG1ftGhRpcc19Wte23Yriv1cc0mKbGTMmDEKoAQEBFi2vf3225YfoMqy6gMHDljKLFiwwLLdvK30D0WXLl0UQOnXr59l24wZMxRA8fLyUoqLixugZdWrbdsVRVHefPNN5ZNPPqk2hr22XVEU5dy5czVKDJrDNTeradsVpWlf87y8PGXjxo1lti1YsMDyGdx3332VHtuUr3td2q0oTfua63Q65eDBg2W2ffHFF5a2L1mypNJjm/I1r0u7FcV+rrmMKbIBg8FAQkICAK6urhWW2bFjR4Xbd+/ebXlf0bHm427cuMGZM2cqLZeXl8ehQ4dqVvF6UJe2A6xcuZJXX32VwMBAhg0bxttvv82lS5fKlLHXtpuZJ/+0VlO/5qXVtO3QtK+5p6cnDz/8cJltUVFRlvfh4eGVHtuUr3td2g1N+5prNBoiIiLKbPPz8wMgLCyMxx57rNJjm/I1r0u7wX6uuSRFNpCWlkZubi5Q+T8SlQ06PnnypOV9RccmJydjMBiqLVdVjIZUl7YfOHCA5ORk9Ho9GRkZ7Nu3j/fee4/OnTuzePFiSzl7bXttNfVrXhfN8ZpnZWVZ3j/xxBOVlmtu193adje3a15SUsLChQvx8vJiy5YteHt7V1q2OV3zmrTbnq65JEU2kJmZaXlf2SRSGRkZtTpWURSysrLqFKMh1aVea9euRa1Wl9uu0+mYM2cO//nPf+ocwx419WteF83xmh84cACAoUOHlutNKa25XXdr292crvnnn39O//792bVrF3l5eURHR1f5j3ZzueY1bbc9XXNJimxAr9c3+LF1idGQ6lKvjz76iOLiYrKzs9m1axd/+MMfcHd3t+x/44036hzDHjX1a14Xze2aG41Gli5dSmBgIF9//XWVM+s2p+tek3Y3p2vesWNHunTpYvn+0qVLzJw5s9LyzeWa17Td9nTNJSmyAWtW/w0ICKjVseY5HeoSoyHVR718fX0ZPnw4CxYs4NSpU9x3330AnD9/nszMTLtte2019WteH5rLNf/888+5dOkSGzZsICQkpMqyzem616TdZs3hmj/wwAN8++23LFu2zLI01P79+ytdsb25XPOattvMHq65JEU2EBYWZsmCDQZDhWW6detW4fYePXpY3ld0bKdOnXBxcaFnz55VlqsqRkOqS9srEhoaypYtW9BoNAC4ubnZbdtrq6lf8/rWVK/5+fPneeONN1i3bh2DBg2ybM/Ozq6wfHO57jVtd0Wa6jU3mzlzJrNmzbJ8X1mPR3O55mbWtrsitrrmkhTZgIuLC0OHDgWgsLCw3H6VSsX9999f4bGln+Co6Njhw4cDEBwcbHnCo6Jynp6eNZpRuL7Upe2Vadu2LVFRUfTq1QsvLy+7bXttNfVr3hCa2jUvLCxk5syZrFq1ilGjRlm2b9q0iQ0bNlR4THO47rVpd2Wa2jW/27Rp0wDTP/aV9WY0h2t+N2vaXRlbXHNJimxk9uzZANy6dcuS8ZovcnR0NMHBwWzbto2goCDCwsI4cuQIAIMGDaJ79+7AnQFlpX84ZsyYUS5G6YFn5rLTp0+vcGBbY6ht25OTk/nggw84fPhwuXO6uLhY7juXjmFvbYeyfy3d/ddOc73mZjVte3O55n/4wx84fvw4zz//PF26dKFLly6EhIQwceJEevXq1Wyve23a3dSv+blz55g/fz4XL14ss71ly5YAvPrqq0Dz+3+9Lu22q2tep1mORK0ZjUZl0qRJCqDs3btXURRFiYqKUvz9/ZWkpCRFURTloYceskx89fzzz1uOPXDggKLRaJSRI0cqiqIou3fvrnAa+MLCQiUyMlJRqVRKamqqYjAYlI4dOyqhoaHKjRs3Gqml5dW27bNmzVIARaVSKX/84x+V/Px8RVEU5dChQ5bZX83ste2KoigrV660tE2j0ZSZrLK5XnOzmra9OVzzf/7znxUuXQAoarVaKSoqapbXvbbtburX3DyRoJ+fn/Ltt98qimL6nff000+XubbN7ZrXpd32dM0lKbKh4uJi5bXXXlM6duyoDBw4UBk/frwlKVAURdm6davSqlUrpUOHDsrhw4fLHHvw4EFl2LBhSq9evZQ+ffoon3zyiWXJjNK0Wq3y7LPPKh07dlQGDBigTJs2zao1aBpabdqelpamPP7440rr1q0VV1dXpUePHsqHH36oHDhwoMIY9tb2zZs3KwMGDFBcXV3L/APh7++vPProo4qiNN9rXtu2N/Vrvn//fkWtVleaHPTq1UtRlOZ33evS7qZ+zU+dOqWMHz9eadmypeLi4qIMHDhQeemll5StW7eWKdfcrnld2m1P11ylKBUsvSuEEEII4WBkTJEQQgghBJIUCSGEEEIAkhQJIYQQQgCSFAkhhBBCAJIUCSGEEEIAkhQJIYQQQgCSFAkhhBBCAJIUCSGEEEIAkhQJIYQQQgCSFAkhhBBCAJIUCSGagdWrV/Ob3/wGlUplebVu3Zru3bvTvn17+vbty8svv8yJEyfKHbt9+3Z+//vf4+TkZDnW39+f3r1707NnT9q1a2fZ3qJFCwBGjBhhKTdw4EB69+5dJvaIESMYPHgwPj4+qFQqZs6c2bgfiBCiViQpEkI0eVOmTOHbb78lICDAsu2ZZ57h5MmTnDlzhqioKD7++GP69OnDm2++SeklH0eNGsXixYvp16+fZduECRM4evQox48f59KlSyQlJfHAAw+UiTl16lSuXLnCwYMH+fvf/15m365du/jpp5+4ePEiw4YNa5hGCyHqnSRFQohmw8vLq9w2Dw8P/va3vzFlyhRKSkqYP38+77//frlynp6elZ63U6dObNiwgZYtWwLg7e3N559/jru7e5X18ff354svvkClUtWwJUIIW5CkSAjhEN5+++0y78+cOVOj4728vBgzZgwAs2bNqjABq0jnzp2ZOHFijWIJIWxDkiIhhEPo3r077dq1A8BoNPL5559bddyxY8dYv349AHPnzgXg0UcfrVHsRx55pEblhRC2IUmREMJhdO3a1fL+xx9/rLb85cuXefbZZy3fd+rUqUHqJYSwDy62roAQQjQWf39/y/u0tLRKy3333XeEhYWRkpJSZlC2EKJ5k54iIUSTkZWVxfnz58u9cnNzrTreyenOr7zi4uJKy40ZM4azZ8+yePFiXFzkb0chHIX83y6EaDL+8Y9/8O6775bbvmzZMqvmAtJqtZb3bdu2rbKss7MzzzzzDKdOnapxPYUQTZP0FAkhHEZycrLlfVRUlFXH1HRQtRCi6ZKkSAjRZLzzzjsoilLuZU0v0fXr10lKSrJ8X3oAdVVKD84WQjRvkhQJIRzCokWLMBqNAPzxj3+kd+/eZfab9939vlWrVtU+Ul+6PCCDs4VooiQpEkI0G/n5+Zb3pccPrVmzhg8//BAnJydeeeUVPv744zLHFRYWkpqaavn+/Pnz5RKdqtw9EWTpHikhRNOhUuRPGiFEE7dhwwY2bNjAsmXLLNvUajW9e/emoKAAFxcXRo4cyaxZs+jevXuZY19//XW++OILbt68WWZ7SEgIr776Kr///e8rjXv+/HmmTJnC8ePHMRgMlu2enp7cd999/PTTT/XUQiFEY5CkSAghhBACuX0mhBBCCAFIUiSEEEIIAUhSJIQQQggBSFIkhBBCCAFIUiSEEEIIAUhSJIQQQggBSFIkhBBCCAFIUiSEEEIIAUhSJIQQQggBSFIkhBBCCAFIUiSEEEIIAUhSJIQQQggBSFIkhBBCCAFIUiSEEEIIAcD/B2Pmpy6qfNa7AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(-train_real_scatter,-train_pre_scatter,color = \"blue\",marker = \"^\",alpha = 0.7,s=75,label = \"Train\")\n",
    "plt.scatter(-test_real_scatter,-test_pre_scatter,color = \"red\",marker = \"s\",alpha = 0.7,s = 75,label = \"Test\")\n",
    "#这里预测结果每次运行都会上下浮动一点，但整体趋势和交叉验证的指标一致\n",
    "plt.xlabel(\"-DFT\",fontsize=15,fontweight = \"bold\")\n",
    "plt.ylabel(\"-ML\",fontsize=15,fontweight = \"bold\")\n",
    "plt.tick_params(direction = \"in\")\n",
    "plt.locator_params(axis = \"x\",nbins=16)\n",
    "plt.locator_params(axis = \"y\",nbins=16)\n",
    "plt.gca().set_xticklabels([\"0\", \"\", \"0.00\", \"\", \"0.50\", \"\", \"1.00\", \"\", \"1.50\", \"\", \"2.00\", \"\", \"2.50\", \"\", \"3.00\", \"\", \"3.50\"],\n",
    "                          fontsize=15, fontweight=\"bold\")\n",
    "plt.gca().set_yticklabels([\"0\", \"\", \"0.00\", \"\", \"0.50\", \"\", \"1.00\", \"\", \"1.50\", \"\", \"2.00\", \"\", \"2.50\", \"\", \"3.00\", \"\", \"3.50\"],\n",
    "                          fontsize=15, fontweight=\"bold\")\n",
    "\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=20,frameon=False,loc=\"lower right\")\n",
    "#plt.savefig(r'C:\\Users\\26482\\Desktop\\Mo2C_data_eassy\\scatter_hd.png', dpi=2000,facecolor= \"white\")\n",
    "plt.plot([-0.1,3.5],[-0.1,3.5],color=\"grey\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eac1209a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义预测函数.稍微改变一下\n",
    "def model_predict_all(model,y,X_data):\n",
    "    result = model.predict(X_data)\n",
    "    y_pred = result*(y.max()-y.min())+y.min()\n",
    "    y_pred = y_pred.reshape(-1)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f185e9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "df1_pre = model_predict_all(bpnn,y,df1)\n",
    "df2_pre = model_predict_all(bpnn,y,df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e63d5ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#将所有预测值合并\n",
    "all_pre = []\n",
    "\n",
    "for i in range(26):\n",
    "    if i in [2,4,5,8,11,13,16,20,23,25]:\n",
    "        all_pre.append(list(df1_pre.reshape(-1,4)[[2,4,5,8,11,13,16,20,23,25].index(i)]))\n",
    "    else:\n",
    "        all_pre.append(list(df2_pre.reshape(-1,4)[[0,1,3,6,7,9,10,12,14,15,17,18,19,21,22,24].index(i)]))\n",
    "all_pre  = pd.DataFrame(all_pre)\n",
    "all_pre.index = pri_feature.iloc[[i for i in range(26)],:].index\n",
    "all_pre.columns = [\"CO\",\"H2S\",\"CH4\",\"C2H6\"]\n",
    "#这里的预测结果每次运行都会上下浮动一点\n",
    "all_pre.to_excel(r\"C:\\Users\\Administrator\\Desktop\\Mo2C_data_eassy\\all_pre.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb903cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#将40DFT+60pre合并用来制作热图\n",
    "#将所有预测值合并\n",
    "DFT_pre = []\n",
    "\n",
    "for i in range(26):\n",
    "    if i in [2,4,5,8,11,13,16,20,23,25]:\n",
    "        DFT_pre.append(list(y.reshape(-1,4)[[2,4,5,8,11,13,16,20,23,25].index(i)]))\n",
    "    else:\n",
    "        DFT_pre.append(list(df2_pre.reshape(-1,4)[[0,1,3,6,7,9,10,12,14,15,17,18,19,21,22,24].index(i)]))\n",
    "DFT_pre  = pd.DataFrame(DFT_pre)\n",
    "DFT_pre.index = pri_feature.iloc[[i for i in range(26)],:].index\n",
    "DFT_pre.columns = [\"CO\",\"H2S\",\"CH4\",\"C2H6\"]\n",
    "#这里的预测结果每次运行都会上下浮动一点，本文随机取了一种\n",
    "DFT_pre.to_excel(r\"C:\\Users\\Administrator\\Desktop\\Mo2C_data_eassy\\DFT_pre.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c4fca7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFT_pre = pd.read_excel(r\"C:\\Users\\Administrator\\Desktop\\Mo2C_data_eassy\\DFT_pre.xlsx\",index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "39200131",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(DFT_pre,cmap=\"coolwarm\",interpolation = \"nearest\",extent=[0,4,0,4])\n",
    "plt.tick_params(axis=\"x\",which=\"both\",bottom=False,top=False,labelbottom = False)\n",
    "plt.tick_params(axis=\"y\",which=\"both\",left=False,right=False,labelleft = False)\n",
    "#plt.gca().set_xticklabels([\"\",\"CO\",\"\",\"$H_2$S\",\"\",\"C$H_4$\",\"\",\"$C_2$$H_6$\"],fontsize=15,fontweight = \"bold\")\n",
    "#plt.gca().set_yticklabels([\"\",\"Sc\",\"\",\"Ti\",\"\",\"V\",\"\",\"Cr\",\"\",\"Mn\",\"\",\"Fe\",\"\",\"Co\",\"\",\"Ni\",\"\",\"Cu\",\"\",\"Zn\",\"\",\"Y\",\"\",\"Zr\",\"\",\"Nb\",\"\",\"Mo\",\"\",\"Tc\",\"\",\"Ru\",\"\",\"Rh\",\"\",\"Pd\",\"\",\"Ag\",\"\",\"Hf\",\"\",\"Ta\",\"\",\"W\",\"\",\"Re\",\"\",\"Os\",\"\",\"Ir\",\"\",\"Pt\"]\n",
    "                          #,fontsize=15,fontweight = \"bold\")\n",
    "plt.colorbar(shrink = 0.7)\n",
    "#plt.savefig(r'C:\\Users\\Administrator\\Desktop\\Mo2C_data_eassy\\heatmap.png', dpi=2000,facecolor= \"white\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c9ad38b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CO</th>\n",
       "      <th>H2S</th>\n",
       "      <th>CH4</th>\n",
       "      <th>C2H6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sc</th>\n",
       "      <td>-1.668248</td>\n",
       "      <td>-1.004386</td>\n",
       "      <td>-0.541651</td>\n",
       "      <td>-0.618191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ti</th>\n",
       "      <td>-2.205563</td>\n",
       "      <td>-1.224349</td>\n",
       "      <td>-0.431637</td>\n",
       "      <td>-0.538580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V</th>\n",
       "      <td>-2.440000</td>\n",
       "      <td>-1.230000</td>\n",
       "      <td>-0.450000</td>\n",
       "      <td>-0.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cr</th>\n",
       "      <td>-2.550478</td>\n",
       "      <td>-1.377117</td>\n",
       "      <td>-0.598539</td>\n",
       "      <td>-0.683421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mn</th>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-1.730000</td>\n",
       "      <td>-0.790000</td>\n",
       "      <td>-0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fe</th>\n",
       "      <td>-2.550000</td>\n",
       "      <td>-0.950000</td>\n",
       "      <td>-0.220000</td>\n",
       "      <td>0.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Co</th>\n",
       "      <td>-2.387134</td>\n",
       "      <td>-0.967664</td>\n",
       "      <td>-0.154730</td>\n",
       "      <td>-0.130363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ni</th>\n",
       "      <td>-1.887271</td>\n",
       "      <td>-0.731077</td>\n",
       "      <td>-0.187407</td>\n",
       "      <td>-0.164564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cu</th>\n",
       "      <td>-1.200000</td>\n",
       "      <td>-0.530000</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>-0.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zn</th>\n",
       "      <td>-0.662265</td>\n",
       "      <td>-0.148286</td>\n",
       "      <td>0.070650</td>\n",
       "      <td>0.076509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y</th>\n",
       "      <td>-1.412704</td>\n",
       "      <td>-0.857838</td>\n",
       "      <td>-0.460443</td>\n",
       "      <td>-0.529892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zr</th>\n",
       "      <td>-1.650000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.490000</td>\n",
       "      <td>-0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nb</th>\n",
       "      <td>-2.000413</td>\n",
       "      <td>-1.159923</td>\n",
       "      <td>-0.578874</td>\n",
       "      <td>-0.687291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mo</th>\n",
       "      <td>-2.800000</td>\n",
       "      <td>-1.790000</td>\n",
       "      <td>-0.450000</td>\n",
       "      <td>-0.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tc</th>\n",
       "      <td>-2.689029</td>\n",
       "      <td>-1.563539</td>\n",
       "      <td>-0.400500</td>\n",
       "      <td>-0.523135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ru</th>\n",
       "      <td>-2.839235</td>\n",
       "      <td>-1.475865</td>\n",
       "      <td>-0.290394</td>\n",
       "      <td>-0.344286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rh</th>\n",
       "      <td>-2.680000</td>\n",
       "      <td>-1.190000</td>\n",
       "      <td>-0.180000</td>\n",
       "      <td>-0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pd</th>\n",
       "      <td>-2.108904</td>\n",
       "      <td>-0.730825</td>\n",
       "      <td>0.002995</td>\n",
       "      <td>0.016774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ag</th>\n",
       "      <td>-1.301091</td>\n",
       "      <td>-0.757221</td>\n",
       "      <td>-0.411709</td>\n",
       "      <td>-0.419019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hf</th>\n",
       "      <td>-1.936369</td>\n",
       "      <td>-1.088860</td>\n",
       "      <td>-0.486268</td>\n",
       "      <td>-0.640367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ta</th>\n",
       "      <td>-2.420000</td>\n",
       "      <td>-1.250000</td>\n",
       "      <td>-0.450000</td>\n",
       "      <td>-0.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W</th>\n",
       "      <td>-3.259129</td>\n",
       "      <td>-1.860475</td>\n",
       "      <td>-0.343269</td>\n",
       "      <td>-0.519996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Re</th>\n",
       "      <td>-2.844675</td>\n",
       "      <td>-1.614869</td>\n",
       "      <td>-0.204844</td>\n",
       "      <td>-0.384725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Os</th>\n",
       "      <td>-3.260000</td>\n",
       "      <td>-1.440000</td>\n",
       "      <td>-0.180000</td>\n",
       "      <td>-0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ir</th>\n",
       "      <td>-2.863266</td>\n",
       "      <td>-1.218003</td>\n",
       "      <td>-0.191666</td>\n",
       "      <td>-0.187028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pt</th>\n",
       "      <td>-2.250000</td>\n",
       "      <td>-0.840000</td>\n",
       "      <td>-0.170000</td>\n",
       "      <td>-0.170000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           CO       H2S       CH4      C2H6\n",
       "Sc  -1.668248 -1.004386 -0.541651 -0.618191\n",
       "Ti  -2.205563 -1.224349 -0.431637 -0.538580\n",
       "V   -2.440000 -1.230000 -0.450000 -0.570000\n",
       "Cr  -2.550478 -1.377117 -0.598539 -0.683421\n",
       "Mn  -3.000000 -1.730000 -0.790000 -0.810000\n",
       "Fe  -2.550000 -0.950000 -0.220000  0.080000\n",
       "Co  -2.387134 -0.967664 -0.154730 -0.130363\n",
       "Ni  -1.887271 -0.731077 -0.187407 -0.164564\n",
       "Cu  -1.200000 -0.530000 -0.200000 -0.210000\n",
       "Zn  -0.662265 -0.148286  0.070650  0.076509\n",
       "Y   -1.412704 -0.857838 -0.460443 -0.529892\n",
       "Zr  -1.650000 -1.000000 -0.490000 -0.560000\n",
       "Nb  -2.000413 -1.159923 -0.578874 -0.687291\n",
       "Mo  -2.800000 -1.790000 -0.450000 -0.570000\n",
       "Tc  -2.689029 -1.563539 -0.400500 -0.523135\n",
       "Ru  -2.839235 -1.475865 -0.290394 -0.344286\n",
       "Rh  -2.680000 -1.190000 -0.180000 -0.200000\n",
       "Pd  -2.108904 -0.730825  0.002995  0.016774\n",
       "Ag  -1.301091 -0.757221 -0.411709 -0.419019\n",
       "Hf  -1.936369 -1.088860 -0.486268 -0.640367\n",
       "Ta  -2.420000 -1.250000 -0.450000 -0.570000\n",
       "W   -3.259129 -1.860475 -0.343269 -0.519996\n",
       "Re  -2.844675 -1.614869 -0.204844 -0.384725\n",
       "Os  -3.260000 -1.440000 -0.180000 -0.250000\n",
       "Ir  -2.863266 -1.218003 -0.191666 -0.187028\n",
       "Pt  -2.250000 -0.840000 -0.170000 -0.170000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DFT_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "91cd84b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "# 创建 SHAP 的 DeepExplainer 对象\n",
    "explainer = shap.DeepExplainer(bpnn, X_train.values)\n",
    "# 计算 SHAP 值\n",
    "shap_values = explainer.shap_values(df1.values)\n",
    "# shap_values 包含了每个特征对预测结果的贡献，可以进一步分析和可视化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b3fffec",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_names = list(df1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b5a518df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shap.plots.violin(\n",
    "    shap_values[0], features=df1, feature_names=feat_names, plot_type=\"violin\",plot_size=(4,4),max_display = 15\n",
    ")\n",
    "#不知道为什么保存失败，直接截的图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9b802f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x900 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shap_exp = shap.Explanation(shap_values[0],feature_names=df1.columns)\n",
    "shap.plots.bar(shap_exp,max_display = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9c6a821c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rM 0.05459013394798551\n",
      "Nm 0.022377592907122094\n",
      "Im 0.030562138829845997\n",
      "dc 0.04268885016591213\n",
      "Am 0.014337015127127298\n",
      "Im3 0.027300402739877033\n",
      "bp 0.042987408776675146\n",
      "Lmin 0.020862112693827258\n",
      "de 0.04542620145577262\n",
      "Mg 0.0387931757171117\n",
      "Ig 0.11987567841643787\n",
      "rNg 0.11577786333989763\n",
      "MNg 0.0021278489276851486\n",
      "ve 0.00608773193241278\n",
      "ge 0.025259171383909606\n"
     ]
    }
   ],
   "source": [
    "#由于SHAP的默认设置只输出两位小数，后续使用上述精度不够，调整一下精度，最终使用了如下的SHAP值结果\n",
    "a =np.mean(np.abs(shap_values[0]), axis=0)\n",
    "b =df1.columns\n",
    "for i in range(15):\n",
    "    print(b[i],a[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25513098",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
